{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "q2qzGO5dtvXB"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "from tqdm import tqdm\n",
        "from glob import glob\n",
        "import os\n",
        "import json \n",
        "import random\n",
        "from xgboost import XGBClassifier, XGBRegressor"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "noor9-jFuE6O",
        "outputId": "3bf125ac-b00d-43f5-875c-08366ce6fee9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "W-KZZ3DstvXD"
      },
      "outputs": [],
      "source": [
        "path = '/content/drive/MyDrive/projects/dacon_img_competition/sample'  #코랩사용할경우만"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TtSLNaLxtvXE",
        "outputId": "22df1c38-4eb6-4fad-b913-f7df847074ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 498/498 [01:21<00:00,  6.12it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'내부 습도 1 최고': [36.5, 100.0],\n",
              " '내부 습도 1 최저': [32.4, 100.0],\n",
              " '내부 습도 1 평균': [34.1, 100.0],\n",
              " '내부 온도 1 최고': [14.5, 47.6],\n",
              " '내부 온도 1 최저': [14.4, 47.0],\n",
              " '내부 온도 1 평균': [14.4, 47.3],\n",
              " '내부 이슬점 최고': [12.8, 31.9],\n",
              " '내부 이슬점 최저': [12.1, 29.1],\n",
              " '내부 이슬점 평균': [12.4, 29.9]}"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# 분석에 사용할 feature 선택\n",
        "csv_features = ['내부 온도 1 평균', '내부 온도 1 최고', '내부 온도 1 최저', '내부 습도 1 평균', '내부 습도 1 최고', \n",
        "                '내부 습도 1 최저', '내부 이슬점 평균', '내부 이슬점 최고', '내부 이슬점 최저']\n",
        "\n",
        "# csv_files = sorted(glob('sample_data/*/*.csv'))\n",
        "allfile = glob(path + '/sample_data/sample_data/*/*.csv')\n",
        "csv_files = sorted(allfile)\n",
        "\n",
        "temp_csv = pd.read_csv(csv_files[0])[csv_features]\n",
        "max_arr, min_arr = temp_csv.max().to_numpy(), temp_csv.min().to_numpy()\n",
        "\n",
        "# feature 별 최대값, 최솟값 계산\n",
        "for csv in tqdm(csv_files[1:]):\n",
        "    temp_csv = pd.read_csv(csv)[csv_features]\n",
        "    temp_max, temp_min = temp_csv.max().to_numpy(), temp_csv.min().to_numpy()\n",
        "    max_arr = np.max([max_arr,temp_max], axis=0)\n",
        "    min_arr = np.min([min_arr,temp_min], axis=0)\n",
        "\n",
        "# feature 별 최대값, 최솟값 dictionary 생성\n",
        "csv_feature_dict = {csv_features[i]:[min_arr[i], max_arr[i]] for i in range(len(csv_features))}\n",
        "csv_feature_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "jZOO842ttvXF"
      },
      "outputs": [],
      "source": [
        "# 제공된 sample data는 파프리카와 시설포도 2종류의 작물만 존재\n",
        "label_description = {\n",
        " '3_00_0': '파프리카_정상',\n",
        " '3_a9_1': '파프리카흰가루병_초기',\n",
        " '3_a9_2': '파프리카흰가루병_중기',\n",
        " '3_a9_3': '파프리카흰가루병_말기',\n",
        " '3_a10_1': '파프리카잘록병_초기',\n",
        " '3_a10_2': '파프리카잘록병_중기',\n",
        " '3_a10_3': '파프리카잘록병_말기',\n",
        " '3_b3_1': '칼슘결핍_초기',\n",
        " '3_b3_2': '칼슘결핍_중기',\n",
        " '3_b3_3': '칼슘결핍_말기',\n",
        " '3_b6_1': '다량원소결핍 (N)_초기',\n",
        " '3_b6_2': '다량원소결핍 (N)_중기',\n",
        " '3_b6_3': '다량원소결핍 (N)_말기',\n",
        " '3_b7_1': '다량원소결핍 (P)_초기',\n",
        " '3_b7_2': '다량원소결핍 (P)_중기',\n",
        " '3_b7_3': '다량원소결핍 (P)_말기',\n",
        " '3_b8_1': '다량원소결핍 (K)_초기',\n",
        " '3_b8_2': '다량원소결핍 (K)_중기',\n",
        " '3_b8_3': '다량원소결핍 (K)_말기',\n",
        " '6_00_0': '시설포도_정상',\n",
        " '6_a11_1': '시설포도탄저병_초기',\n",
        " '6_a11_2': '시설포도탄저병_중기',\n",
        " '6_a11_3': '시설포도탄저병_말기',\n",
        " '6_a12_1': '시설포도노균병_초기',\n",
        " '6_a12_2': '시설포도노균병_중기',\n",
        " '6_a12_3': '시설포도노균병_말기',\n",
        " '6_b4_1': '일소피해_초기',\n",
        " '6_b4_2': '일소피해_중기',\n",
        " '6_b4_3': '일소피해_말기',\n",
        " '6_b5_1': '축과병_초기',\n",
        " '6_b5_2': '축과병_중기',\n",
        " '6_b5_3': '축과병_말기',\n",
        "}\n",
        "\n",
        "label_encoder = {key:idx for idx, key in enumerate(label_description)}\n",
        "label_decoder = {val:key for key, val in label_encoder.items()}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "label_decoder[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "zXeK6Pc_SIKS",
        "outputId": "03c05029-363c-4f14-cd0e-a2d185d47b7c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'3_a9_1'"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "MzYWOokPtvXG"
      },
      "outputs": [],
      "source": [
        "class DataController():\n",
        "    def __init__(self,csvfeatures,csvfeaturedict):\n",
        "        self.csv_features = csvfeatures\n",
        "        self.csv_feature_dict = csvfeaturedict\n",
        "    \n",
        "    def road_csv(self,foldnam,timenum):\n",
        "        df = pd.read_csv(foldnam)\n",
        "        return df.loc[:timenum-1,self.csv_features]      #csv파일 제일 짧은게 291개임 오류인지는 모르겠으나 일단 최소길이로 통일하여 처리\n",
        "    \n",
        "    def scaling(self,minmaxdic,df):\n",
        "        for col in minmaxdic.keys():\n",
        "            df.loc[:,col] = df.loc[:,col] - csv_feature_dict[col][0]\n",
        "            df.loc[:,col] = df.loc[:,col] / (csv_feature_dict[col][1]-csv_feature_dict[col][0])\n",
        "        return df\n",
        "    \n",
        "    \n",
        "    def getimage(self,imgpath):\n",
        "        img = cv2.imread(imgpath)\n",
        "        img = cv2.resize(img, dsize=(256, 256), interpolation=cv2.INTER_AREA)\n",
        "        img = img.astype(np.float32)/255  ##픽셀값을 0~1사이로 정규화\n",
        "        # img = np.transpose(img, (2,0,1))\n",
        "        return img\n",
        "    \n",
        "    def getlable(self,jsonpath):\n",
        "        with open(jsonpath, 'r') as f:\n",
        "            json_file = json.load(f)\n",
        "\n",
        "        crop = json_file['annotations']['crop']\n",
        "        disease = json_file['annotations']['disease']\n",
        "        risk = json_file['annotations']['risk']\n",
        "        label = f'{crop}_{disease}_{risk}'\n",
        "        return label\n",
        "    \n",
        "    def getdata(self,datapath,timenum,featnum):\n",
        "\n",
        "        csvarr = np.empty((0,timenum,featnum), float)\n",
        "        imgarr = np.empty((0,256,256,3), float)\n",
        "        lablearr = np.array([])\n",
        "        \n",
        "        # predictor = np.append(predictor,ndf.reshape(-1,timenum,featnum),axis = 0)\n",
        "        \n",
        "        for ind,i in enumerate(datapath):\n",
        "            \n",
        "            if glob(i + '/*.csv') == []:  #10462폴더 비어있음. 다음에 확인해보기\n",
        "                pass\n",
        "            else:\n",
        "                csvpath = glob(i + '/*.csv')[0]\n",
        "                imgpath = glob(i + '/*.jpg')[0]\n",
        "                jsonpath = glob(i + '/*.json')[0]\n",
        "                # con = DataController()\n",
        "                df = self.road_csv(csvpath,timenum)\n",
        "                df2 = self.scaling(csv_feature_dict,df).to_numpy().reshape(-1,timenum,featnum)\n",
        "                imgdata = self.getimage(imgpath).reshape(-1,256,256,3)\n",
        "                label = label_encoder[self.getlable(jsonpath)]\n",
        "                # label = self.getlable(jsonpath)\n",
        "                \n",
        "                csvarr = np.append(csvarr,df2, axis = 0)\n",
        "                imgarr = np.append(imgarr,imgdata, axis = 0)\n",
        "                lablearr = np.append(lablearr,label)\n",
        "            \n",
        "        return [csvarr,imgarr],lablearr\n",
        "            \n",
        "        \n",
        "        \n",
        "        \n",
        "          "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dacon = DataController(csv_features,csv_feature_dict)"
      ],
      "metadata": {
        "id": "l3BxB-6NSwht"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dacon.road_csv()"
      ],
      "metadata": {
        "id": "PNR815CWSzwQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "outputId": "3d427431-12b3-482c-e00f-8735cb55e0f7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-35e84e1f0c7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdacon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroad_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: road_csv() missing 2 required positional arguments: 'foldnam' and 'timenum'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "talU-dJBtvXH"
      },
      "outputs": [],
      "source": [
        "#프로젝트에 있는 모든 데이터폴터 불러오기\n",
        "data_files = glob(path + '/sample_data/sample_data/*')\n",
        "#셔플\n",
        "random.shuffle(data_files)\n",
        "#앞에서 300번째까지 트레인셋으로\n",
        "trainfiles = data_files[:400]\n",
        "#나머지는 테스트셋으로\n",
        "testfiles = data_files[400:]\n",
        "\n",
        "# 데이터 컨트롤러\n",
        "dacon = DataController(csv_features,csv_feature_dict)\n",
        "# 배치화된 데이터셋 만들기\n",
        "# test셋용으로는 train = False로 하여 배치안생성하게됨\n",
        "x_train,y_train = dacon.getdata(trainfiles,260,9)\n",
        "x_test,y_test = dacon.getdata(testfiles,260,9)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **전처리: 오토인코더 시계열 차원 압축 + ResNet50 -> XGBoost**"
      ],
      "metadata": {
        "id": "eu-ThEQELS-2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 데이터 불러오기"
      ],
      "metadata": {
        "id": "C6vzJyNDKSMJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications.resnet import ResNet50\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ],
      "metadata": {
        "id": "a8bPjFm9KQLr"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ff7e4fd-cad1-4d31-ad2d-fa473739da1d",
        "id": "D4rl29aMKQLt"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 498/498 [00:07<00:00, 70.24it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'내부 습도 1 최고': [36.5, 100.0],\n",
              " '내부 습도 1 최저': [32.4, 100.0],\n",
              " '내부 습도 1 평균': [34.1, 100.0],\n",
              " '내부 온도 1 최고': [14.5, 47.6],\n",
              " '내부 온도 1 최저': [14.4, 47.0],\n",
              " '내부 온도 1 평균': [14.4, 47.3],\n",
              " '내부 이슬점 최고': [12.8, 31.9],\n",
              " '내부 이슬점 최저': [12.1, 29.1],\n",
              " '내부 이슬점 평균': [12.4, 29.9]}"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "# 분석에 사용할 feature 선택\n",
        "csv_features = ['내부 온도 1 평균', '내부 온도 1 최고', '내부 온도 1 최저', '내부 습도 1 평균', '내부 습도 1 최고', \n",
        "                '내부 습도 1 최저', '내부 이슬점 평균', '내부 이슬점 최고', '내부 이슬점 최저']\n",
        "\n",
        "# csv_files = sorted(glob('sample_data/*/*.csv'))\n",
        "allfile = glob(path + '/sample_data/sample_data/*/*.csv')\n",
        "csv_files = sorted(allfile)\n",
        "\n",
        "temp_csv = pd.read_csv(csv_files[0])[csv_features]\n",
        "max_arr, min_arr = temp_csv.max().to_numpy(), temp_csv.min().to_numpy()\n",
        "\n",
        "# feature 별 최대값, 최솟값 계산\n",
        "for csv in tqdm(csv_files[1:]):\n",
        "    temp_csv = pd.read_csv(csv)[csv_features]\n",
        "    temp_max, temp_min = temp_csv.max().to_numpy(), temp_csv.min().to_numpy()\n",
        "    max_arr = np.max([max_arr,temp_max], axis=0)\n",
        "    min_arr = np.min([min_arr,temp_min], axis=0)\n",
        "\n",
        "# feature 별 최대값, 최솟값 dictionary 생성\n",
        "csv_feature_dict = {csv_features[i]:[min_arr[i], max_arr[i]] for i in range(len(csv_features))}\n",
        "csv_feature_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "tiCVdEpAKQLt"
      },
      "outputs": [],
      "source": [
        "# 제공된 sample data는 파프리카와 시설포도 2종류의 작물만 존재\n",
        "label_description = {\n",
        " '3_00_0': '파프리카_정상',\n",
        " '3_a9_1': '파프리카흰가루병_초기',\n",
        " '3_a9_2': '파프리카흰가루병_중기',\n",
        " '3_a9_3': '파프리카흰가루병_말기',\n",
        " '3_a10_1': '파프리카잘록병_초기',\n",
        " '3_a10_2': '파프리카잘록병_중기',\n",
        " '3_a10_3': '파프리카잘록병_말기',\n",
        " '3_b3_1': '칼슘결핍_초기',\n",
        " '3_b3_2': '칼슘결핍_중기',\n",
        " '3_b3_3': '칼슘결핍_말기',\n",
        " '3_b6_1': '다량원소결핍 (N)_초기',\n",
        " '3_b6_2': '다량원소결핍 (N)_중기',\n",
        " '3_b6_3': '다량원소결핍 (N)_말기',\n",
        " '3_b7_1': '다량원소결핍 (P)_초기',\n",
        " '3_b7_2': '다량원소결핍 (P)_중기',\n",
        " '3_b7_3': '다량원소결핍 (P)_말기',\n",
        " '3_b8_1': '다량원소결핍 (K)_초기',\n",
        " '3_b8_2': '다량원소결핍 (K)_중기',\n",
        " '3_b8_3': '다량원소결핍 (K)_말기',\n",
        " '6_00_0': '시설포도_정상',\n",
        " '6_a11_1': '시설포도탄저병_초기',\n",
        " '6_a11_2': '시설포도탄저병_중기',\n",
        " '6_a11_3': '시설포도탄저병_말기',\n",
        " '6_a12_1': '시설포도노균병_초기',\n",
        " '6_a12_2': '시설포도노균병_중기',\n",
        " '6_a12_3': '시설포도노균병_말기',\n",
        " '6_b4_1': '일소피해_초기',\n",
        " '6_b4_2': '일소피해_중기',\n",
        " '6_b4_3': '일소피해_말기',\n",
        " '6_b5_1': '축과병_초기',\n",
        " '6_b5_2': '축과병_중기',\n",
        " '6_b5_3': '축과병_말기',\n",
        "}\n",
        "\n",
        "label_encoder = {key:idx for idx, key in enumerate(label_description)}\n",
        "label_decoder = {val:key for key, val in label_encoder.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "W4RBgfu6KQLt"
      },
      "outputs": [],
      "source": [
        "class DataController():\n",
        "    def __init__(self,csvfeatures,csvfeaturedict):\n",
        "        self.csv_features = csvfeatures\n",
        "        self.csv_feature_dict = csvfeaturedict\n",
        "    \n",
        "    def road_csv(self,foldnam,timenum):\n",
        "        df = pd.read_csv(foldnam)\n",
        "        return df.loc[:timenum-1,self.csv_features]      #csv파일 제일 짧은게 291개임 오류인지는 모르겠으나 일단 최소길이로 통일하여 처리\n",
        "    \n",
        "    def scaling(self,minmaxdic,df):\n",
        "        for col in minmaxdic.keys():\n",
        "            df.loc[:,col] = df.loc[:,col] - csv_feature_dict[col][0]\n",
        "            df.loc[:,col] = df.loc[:,col] / (csv_feature_dict[col][1]-csv_feature_dict[col][0])\n",
        "        return df\n",
        "    \n",
        "    \n",
        "    def getimage(self,imgpath):\n",
        "        img = cv2.imread(imgpath)\n",
        "        img = cv2.resize(img, dsize=(224, 224), interpolation=cv2.INTER_AREA)\n",
        "        img = img.astype(np.float32)/255  ##픽셀값을 0~1사이로 정규화\n",
        "        # img = np.transpose(img, (2,0,1))\n",
        "        return img\n",
        "    \n",
        "    def getlable(self,jsonpath):\n",
        "        with open(jsonpath, 'r') as f:\n",
        "            json_file = json.load(f)\n",
        "\n",
        "        crop = json_file['annotations']['crop']\n",
        "        disease = json_file['annotations']['disease']\n",
        "        risk = json_file['annotations']['risk']\n",
        "        label = f'{crop}_{disease}_{risk}'\n",
        "        return label\n",
        "    \n",
        "    def getdata(self,datapath,timenum,featnum):\n",
        "\n",
        "        csvarr = np.empty((0,timenum,featnum), float)\n",
        "        imgarr = np.empty((0,224,224,3), float)\n",
        "        lablearr = np.array([])\n",
        "        \n",
        "        # predictor = np.append(predictor,ndf.reshape(-1,timenum,featnum),axis = 0)\n",
        "        \n",
        "        for ind,i in enumerate(datapath):\n",
        "            \n",
        "            if glob(i + '/*.csv') == []:  #10462폴더 비어있음. 다음에 확인해보기\n",
        "                pass\n",
        "            else:\n",
        "                csvpath = glob(i + '/*.csv')[0]\n",
        "                imgpath = glob(i + '/*.jpg')[0]\n",
        "                jsonpath = glob(i + '/*.json')[0]\n",
        "                # con = DataController()\n",
        "                df = self.road_csv(csvpath,timenum)\n",
        "                df2 = self.scaling(csv_feature_dict,df).to_numpy().reshape(-1,timenum,featnum)\n",
        "                imgdata = self.getimage(imgpath).reshape(-1,224,224,3)\n",
        "                label = label_encoder[self.getlable(jsonpath)]\n",
        "                # label = self.getlable(jsonpath)\n",
        "                \n",
        "                csvarr = np.append(csvarr,df2, axis = 0)\n",
        "                imgarr = np.append(imgarr,imgdata, axis = 0)\n",
        "                lablearr = np.append(lablearr,label)\n",
        "            \n",
        "        return [csvarr,imgarr],lablearr\n",
        "            \n",
        "        \n",
        "        \n",
        "        \n",
        "          "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "TxGw1_eBKQLu"
      },
      "outputs": [],
      "source": [
        "#프로젝트에 있는 모든 데이터폴터 불러오기\n",
        "data_files = glob(path + '/sample_data/sample_data/*')\n",
        "#셔플\n",
        "random.shuffle(data_files)\n",
        "#앞에서 300번째까지 트레인셋으로\n",
        "trainfiles = data_files[:400]\n",
        "#나머지는 테스트셋으로\n",
        "testfiles = data_files[400:]\n",
        "\n",
        "# 데이터 컨트롤러\n",
        "dacon = DataController(csv_features,csv_feature_dict)\n",
        "# 배치화된 데이터셋 만들기\n",
        "# test셋용으로는 train = False로 하여 배치안생성하게됨\n",
        "x_train,y_train = dacon.getdata(trainfiles,260,9)\n",
        "x_test,y_test = dacon.getdata(testfiles,260,9)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 오토인코더 학습"
      ],
      "metadata": {
        "id": "zKGxfqdyKV8p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Eencoder(keras.layers.Layer):\n",
        "    def __init__(self):\n",
        "        super(Eencoder, self).__init__()\n",
        "        self.block1_layer1 = tf.keras.layers.Conv1D(9, 81, activation='relu',)#input_shape=input_shape[1:])\n",
        "        self.block1_layer2 = tf.keras.layers.Conv1D(18, 81, activation='relu',)#input_shape=input_shape[1:])\n",
        "        self.block1_layer3 = tf.keras.layers.Conv1D(36, 100, activation='relu',)#input_shape=input_shape[1:])\n",
        "         \n",
        "    def call(self, inputs):\n",
        "        #LSTM파트\n",
        "        lstm_x = self.block1_layer1(inputs)\n",
        "        lstm_x = tf.nn.relu(lstm_x)\n",
        "        lstm_x = self.block1_layer2(lstm_x)\n",
        "        lstm_x = tf.nn.relu(lstm_x)\n",
        "        lstm_x = self.block1_layer3(lstm_x)\n",
        "        lstm_x = tf.nn.relu(lstm_x)\n",
        "        \n",
        "        return lstm_x"
      ],
      "metadata": {
        "id": "eWWpQA7YlcAI"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Ddecoder(keras.layers.Layer):\n",
        "    def __init__(self):\n",
        "        super(Ddecoder, self).__init__()\n",
        "        self.block1_layer1 = tf.keras.layers.Conv1DTranspose(3, 81, activation='relu',)#input_shape=input_shape[1:])\n",
        "        self.block1_layer2 = tf.keras.layers.Conv1DTranspose(6, 81, activation='relu',)#input_shape=input_shape[1:])\n",
        "        self.block1_layer3 = tf.keras.layers.Conv1DTranspose(9, 100, activation='relu',)#input_shape=input_shape[1:])\n",
        "        \n",
        "         \n",
        "    def call(self, inputs):\n",
        "        #LSTM파트\n",
        "        x = self.block1_layer1(inputs)\n",
        "        \n",
        "        x = tf.nn.relu(x)\n",
        "        x = self.block1_layer2(x)\n",
        "        \n",
        "        x = tf.nn.relu(x)\n",
        "        x = self.block1_layer3(x)\n",
        "       \n",
        "        return x\n",
        "\n",
        "# Eencoder()(x_train[0][:2,:,:]).shape\n",
        "# Ddecoder()(Eencoder()(x_train[0][:2,:,:]))\n"
      ],
      "metadata": {
        "id": "wFc4X54cl98v"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Autoencoder(tf.keras.Model): \n",
        "  def __init__(self,): \n",
        "    super(Autoencoder, self).__init__() \n",
        "    self.encoder = Eencoder() \n",
        "    self.decoder = Ddecoder() \n",
        "  \n",
        "  def call(self, input): \n",
        "    code = self.encoder(input) \n",
        "    reconstructed = self.decoder(code) \n",
        "    return reconstructed\n",
        "\n",
        "def loss(model, original): \n",
        "  reconstruction_error = tf.reduce_mean(tf.square(tf.subtract(model(original), original))) \n",
        "  return reconstruction_error\n",
        "\n",
        "def train(loss, model, opt, original): \n",
        "  with tf.GradientTape() as tape: \n",
        "    gradients = tape.gradient(loss(model, original), model.trainable_variables) \n",
        "    gradient_variables = zip(gradients, model.trainable_variables) \n",
        "    opt.apply_gradients(gradient_variables)\n",
        "\n"
      ],
      "metadata": {
        "id": "U6PCeaNLqpww"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "automodel = Autoencoder()\n",
        "opt = tf.optimizers.Adam()\n",
        "loss_fn = keras.losses.MeanSquaredError()\n",
        "automodel.compile(optimizer=opt, loss=loss_fn)"
      ],
      "metadata": {
        "id": "IJX0RI4k_2ca"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "automodel.fit(x_train[0], x_train[0], \n",
        "                 batch_size=100, \n",
        "                 epochs=20,\n",
        "                )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5KVNfPBDJEZJ",
        "outputId": "dd557254-3b6e-4d17-8e67-2fe50f255c69"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "Check_1\n",
            "Check_2\n",
            "Check_3\n",
            "Check_1\n",
            "Check_2\n",
            "Check_3\n",
            "4/4 [==============================] - 2s 244ms/step - loss: 0.2629\n",
            "Epoch 2/20\n",
            "4/4 [==============================] - 1s 239ms/step - loss: 0.2157\n",
            "Epoch 3/20\n",
            "4/4 [==============================] - 1s 243ms/step - loss: 0.1821\n",
            "Epoch 4/20\n",
            "4/4 [==============================] - 1s 244ms/step - loss: 0.1384\n",
            "Epoch 5/20\n",
            "4/4 [==============================] - 1s 234ms/step - loss: 0.1119\n",
            "Epoch 6/20\n",
            "4/4 [==============================] - 1s 232ms/step - loss: 0.0987\n",
            "Epoch 7/20\n",
            "4/4 [==============================] - 1s 234ms/step - loss: 0.0897\n",
            "Epoch 8/20\n",
            "4/4 [==============================] - 1s 232ms/step - loss: 0.0798\n",
            "Epoch 9/20\n",
            "4/4 [==============================] - 1s 236ms/step - loss: 0.0704\n",
            "Epoch 10/20\n",
            "4/4 [==============================] - 1s 234ms/step - loss: 0.0626\n",
            "Epoch 11/20\n",
            "4/4 [==============================] - 1s 241ms/step - loss: 0.0566\n",
            "Epoch 12/20\n",
            "4/4 [==============================] - 1s 238ms/step - loss: 0.0517\n",
            "Epoch 13/20\n",
            "4/4 [==============================] - 1s 236ms/step - loss: 0.0480\n",
            "Epoch 14/20\n",
            "4/4 [==============================] - 1s 246ms/step - loss: 0.0453\n",
            "Epoch 15/20\n",
            "4/4 [==============================] - 1s 239ms/step - loss: 0.0433\n",
            "Epoch 16/20\n",
            "4/4 [==============================] - 1s 235ms/step - loss: 0.0417\n",
            "Epoch 17/20\n",
            "4/4 [==============================] - 1s 240ms/step - loss: 0.0403\n",
            "Epoch 18/20\n",
            "4/4 [==============================] - 1s 233ms/step - loss: 0.0391\n",
            "Epoch 19/20\n",
            "4/4 [==============================] - 1s 231ms/step - loss: 0.0382\n",
            "Epoch 20/20\n",
            "4/4 [==============================] - 1s 229ms/step - loss: 0.0373\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f5a6bbe02d0>"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = automodel(x_test[0])\n",
        "print(\"testset에 오토인코더 손실함수 테스트: \",loss_fn(x_test[0],y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XIad-XctjZpT",
        "outputId": "c1ab6e75-1eb2-49cc-dc1f-cd5cfdb0eb16"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Check_1\n",
            "Check_2\n",
            "Check_3\n",
            "testset에 오토인코더 손실함수 테스트 tf.Tensor(0.034157023, shape=(), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* XGBoost 분류모델 ㄱㄱ"
      ],
      "metadata": {
        "id": "5Ajf5v88Kf8o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class preprmodel(keras.Model):\n",
        "    def __init__(self, imgmodel,autoencoder,name = None):\n",
        "        super(preprmodel, self).__init__()\n",
        "        self.imgmodel = imgmodel\n",
        "        self.autoencoder = autoencoder\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x1 = self.imgmodel(inputs[1])\n",
        "        x2 = self.autoencoder.encoder(inputs[0])\n",
        "\n",
        "\n",
        "        x = tf.concat([x1,x2],axis=1)\n",
        "        return(x)"
      ],
      "metadata": {
        "id": "WaX_Met5Kwpb"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_RESNET50 = ResNet50(weights='imagenet')\n",
        "prepromodel = preprmodel(model_RESNET50,automodel)"
      ],
      "metadata": {
        "id": "ySD-3Xa1Kwpc"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_prime_train = prepromodel(x_train)\n",
        "# x_prime_test = prepromodel(x_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "id": "YlSjMBPcKwpc",
        "outputId": "c94e7b64-c3c7-4792-a486-a1427e8a7af4"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-56431730fb42>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx_prime_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepromodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# x_prime_test = prepromodel(x_test)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-34-648b7afbdc37>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: Exception encountered when calling layer \"preprmodel\" (type preprmodel).\n\nConcatOp : Ranks of all input tensors should match: shape[0] = [399,1000] vs. shape[1] = [399,1,36] [Op:ConcatV2] name: concat\n\nCall arguments received:\n  • inputs=['tf.Tensor(shape=(399, 260, 9), dtype=float32)', 'tf.Tensor(shape=(399, 224, 224, 3), dtype=float32)']"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "automodel.encoder(x_train[0]).reshape(-1,0,36)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "EcrL71z3NE1J",
        "outputId": "a2185445-643b-40a8-e571-12877277a25b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-8b448d2baeaa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mautomodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m36\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'automodel' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_RESNET50(x_train[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VlNLA5rfmeMr",
        "outputId": "1a7af749-e049-4fbb-ada9-314d60515086"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(399, 1000), dtype=float32, numpy=\n",
              "array([[1.43102268e-04, 2.85931601e-04, 5.92746037e-05, ...,\n",
              "        1.25858805e-05, 1.29511536e-04, 7.30982400e-04],\n",
              "       [1.52261913e-04, 2.80277891e-04, 5.82303510e-05, ...,\n",
              "        1.27725243e-05, 1.32593428e-04, 6.94298360e-04],\n",
              "       [1.24815866e-04, 2.20880582e-04, 5.20545182e-05, ...,\n",
              "        1.06018024e-05, 1.12133210e-04, 6.34574273e-04],\n",
              "       ...,\n",
              "       [1.30669869e-04, 2.40631649e-04, 4.49259969e-05, ...,\n",
              "        1.10554829e-05, 1.13653827e-04, 6.74244773e-04],\n",
              "       [1.24628568e-04, 2.45447271e-04, 4.60812516e-05, ...,\n",
              "        1.11650279e-05, 1.13452763e-04, 6.92347588e-04],\n",
              "       [1.42073914e-04, 2.90111988e-04, 6.42268060e-05, ...,\n",
              "        1.30433973e-05, 1.32931746e-04, 7.72589061e-04]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBClassifier, XGBRegressor\n",
        "\n",
        "xgb = XGBClassifier(random_state=100)\n",
        "xgb.fit(x_prime_train,y_train)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8470cd27-a4b9-4076-8fcb-e5a30d7eeaca",
        "id": "7UPj6SNJKwpc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "XGBClassifier(objective='multi:softprob', random_state=100)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **간단한 LSTM, CNN 모델 앙상블 예측**"
      ],
      "metadata": {
        "id": "wgSN6gshPG3z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iJUkBcfltvXH"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DQWtrrqWtvXI"
      },
      "outputs": [],
      "source": [
        "class paraBlock(keras.layers.Layer):\n",
        "    def __init__(self):\n",
        "        super(paraBlock, self).__init__()\n",
        "        self.block1_layer1 = tf.keras.layers.LSTM(128)\n",
        "        self.block1_layer2 = tf.keras.layers.Dense(64)\n",
        "        self.block1_layer3 = tf.keras.layers.Dense(32)\n",
        "        self.block1_layer4 = tf.keras.layers.Dense(16)\n",
        "        \n",
        "        self.block2_layer1 = tf.keras.layers.Conv2D( filters=16, kernel_size=10, activation='relu')\n",
        "        self.block2_layer2 = tf.keras.layers.Conv2D( filters=32, kernel_size=10, activation='relu')\n",
        "        self.block2_layer3 = tf.keras.layers.Conv2D( filters=64, kernel_size=10, activation='relu')\n",
        "        self.block2_layer4 = tf.keras.layers.Conv2D( filters=128, kernel_size=10, activation='relu')\n",
        "        \n",
        "        self.pooling_layer = tf.keras.layers.MaxPooling2D((2, 2))\n",
        "        self.flat_layer = tf.keras.layers.Flatten()\n",
        "        \n",
        "        self.out_layer1 = tf.keras.layers.Dense(64,activation='relu')\n",
        "        self.out_layer2 = tf.keras.layers.Dense(32,activation='softmax')\n",
        "        \n",
        "    def call(self, inputs):\n",
        "        #LSTM파트\n",
        "        lstm_x = self.block1_layer1(inputs[0])\n",
        "        lstm_x = tf.nn.relu(lstm_x)\n",
        "        lstm_x = self.block1_layer2(lstm_x)\n",
        "        lstm_x = tf.nn.relu(lstm_x)\n",
        "        lstm_x = self.block1_layer3(lstm_x)\n",
        "        lstm_x = tf.nn.relu(lstm_x)\n",
        "        lstm_x = self.block1_layer4(lstm_x)\n",
        "        # x = tf.nn.relu(x)\n",
        "        # x = self.layer5(x)\n",
        "        \n",
        "        #CNN파트\n",
        "        cnn_x = self.block2_layer1(inputs[1])\n",
        "        cnn_x = self.pooling_layer(cnn_x)\n",
        "        \n",
        "        cnn_x = self.block2_layer2(cnn_x)\n",
        "        cnn_x = self.pooling_layer(cnn_x)\n",
        "        \n",
        "        cnn_x = self.block2_layer3(cnn_x)\n",
        "        cnn_x = self.pooling_layer(cnn_x)\n",
        "        \n",
        "        cnn_x = self.block2_layer4(cnn_x)\n",
        "        cnn_x = self.flat_layer(cnn_x)\n",
        "        \n",
        "        #합치기\n",
        "        out = tf.concat([lstm_x,cnn_x],axis=1)\n",
        "        out = self.out_layer1(out)\n",
        "        out = self.out_layer2(out)\n",
        "        \n",
        "        return out\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6bAMyzGJtvXI"
      },
      "outputs": [],
      "source": [
        "class Paramodel(keras.Model):\n",
        "    def __init__(self, name = None):\n",
        "        super(Paramodel, self).__init__()\n",
        "    \n",
        "        self.module1 = paraBlock()\n",
        "        \n",
        "    def call(self, inputs):\n",
        "        x1 = self.module1(inputs)\n",
        "    \n",
        "        # x = tf.concat([x1,x2,x3],axis=1)\n",
        "        \n",
        "        \n",
        "        return(x1)\n",
        "    \n",
        "# inputs = tf.random.normal([3,2,10,8])\n",
        "# model1 = Paramodel(name='model_01')\n",
        "# print(model1(inputs))\n",
        "# model1.variables,model1.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "syip1ueetvXJ"
      },
      "outputs": [],
      "source": [
        "# Instantiate an optimizer.\n",
        "optimizer = keras.optimizers.Adam(learning_rate=1e-3)\n",
        "# Instantiate a loss function.\n",
        "loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
        "#모델\n",
        "model1 = Paramodel(name='model_01')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o9Uo-cQutvXJ",
        "outputId": "779bfcf4-7e09-4d58-a8f8-c99b28c8c9a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Start of epoch 0\n",
            "Training loss (for one batch) at step 0: 3.4706\n",
            "Seen so far: 1 samples\n"
          ]
        }
      ],
      "source": [
        "epochs = 1\n",
        "for epoch in range(epochs):\n",
        "    print(\"\\nStart of epoch %d\" % (epoch,))\n",
        "\n",
        "    # Iterate over the batches of the dataset.\n",
        "    for step in range(1):\n",
        "\n",
        "        # Open a GradientTape to record the operations run\n",
        "        # during the forward pass, which enables auto-differentiation.\n",
        "        with tf.GradientTape() as tape:\n",
        "\n",
        "            # Run the forward pass of the layer.\n",
        "            # The operations that the layer applies\n",
        "            # to its inputs are going to be recorded\n",
        "            # on the GradientTape.\n",
        "            logits = model1(x_train, training=True)  # Logits for this minibatch\n",
        "\n",
        "            # Compute the loss value for this minibatch.\n",
        "            loss_value = loss_fn(y_train, logits)\n",
        "            #label_encoder\n",
        "        # Use the gradient tape to automatically retrieve\n",
        "        # the gradients of the trainable variables with respect to the loss.\n",
        "        grads = tape.gradient(loss_value, model1.trainable_weights)\n",
        "\n",
        "        # Run one step of gradient descent by updating\n",
        "        # the value of the variables to minimize the loss.\n",
        "        optimizer.apply_gradients(zip(grads, model1.trainable_weights))\n",
        "\n",
        "        # Log every 200 batches.\n",
        "        if step % 200 == 0:\n",
        "            print(\n",
        "                \"Training loss (for one batch) at step %d: %.4f\"\n",
        "                % (step, float(loss_value))\n",
        "            )\n",
        "            print(\"Seen so far: %s samples\" % ((step + 1)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tMOwrfGqtvXK",
        "outputId": "c110cd82-8200-4b5c-c141-d92370037c6a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "모델성능:  tf.Tensor(8.732621, shape=(), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "y_pred = model1(x_test)\n",
        "score = loss_fn(y_test,y_pred)\n",
        "print(\"모델성능: \",score)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# y_pred\n",
        "pred = np.array([])\n",
        "for i in y_pred:\n",
        "  maxi = max(i)\n",
        "  for ind,j in enumerate(i):\n",
        "    if j ==maxi:\n",
        "      pred = np.append(pred,ind)"
      ],
      "metadata": {
        "id": "PGt12zktzTVT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "QnrcmQ1WtvXK",
        "outputId": "e4c67d63-1e25-4b18-f215-ccd9a88b596b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-886e693e-f132-4edd-9d81-fc6e6ddb82ce\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>preds</th>\n",
              "      <th>시설포도_정상</th>\n",
              "      <th>파프리카_정상</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>answer</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>시설포도_정상</th>\n",
              "      <td>17</td>\n",
              "      <td>21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>시설포도노균병_중기</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>시설포도탄저병_초기</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>일소피해_말기</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>일소피해_초기</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>파프리카_정상</th>\n",
              "      <td>8</td>\n",
              "      <td>23</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>파프리카흰가루병_말기</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>파프리카흰가루병_중기</th>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>파프리카흰가루병_초기</th>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-886e693e-f132-4edd-9d81-fc6e6ddb82ce')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-886e693e-f132-4edd-9d81-fc6e6ddb82ce button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-886e693e-f132-4edd-9d81-fc6e6ddb82ce');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "preds        시설포도_정상  파프리카_정상\n",
              "answer                       \n",
              "시설포도_정상           17       21\n",
              "시설포도노균병_중기         2        1\n",
              "시설포도탄저병_초기         0        1\n",
              "일소피해_말기            1        0\n",
              "일소피해_초기            0        1\n",
              "파프리카_정상            8       23\n",
              "파프리카흰가루병_말기        3        3\n",
              "파프리카흰가루병_중기        2        4\n",
              "파프리카흰가루병_초기        5        7"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ],
      "source": [
        "answer = np.array([label_description[label_decoder[int(val)]] for val in y_test])\n",
        "predss = np.array([label_description[label_decoder[int(val)]] for val in pred])\n",
        "\n",
        "new_crosstab = pd.crosstab(answer, predss, rownames=['answer'], colnames=['preds'])\n",
        "new_crosstab"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **LSTM, VGG16으로 전처리 후 XGB로 처리**"
      ],
      "metadata": {
        "id": "IDaXilWZPQGR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications.resnet import ResNet50\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ],
      "metadata": {
        "id": "qJFk1DDqPqmp"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30fc31d7-d220-4ff8-b0fe-33b334b73a71",
        "id": "QdegX4H1o7Ha"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 498/498 [01:20<00:00,  6.22it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'내부 습도 1 최고': [36.5, 100.0],\n",
              " '내부 습도 1 최저': [32.4, 100.0],\n",
              " '내부 습도 1 평균': [34.1, 100.0],\n",
              " '내부 온도 1 최고': [14.5, 47.6],\n",
              " '내부 온도 1 최저': [14.4, 47.0],\n",
              " '내부 온도 1 평균': [14.4, 47.3],\n",
              " '내부 이슬점 최고': [12.8, 31.9],\n",
              " '내부 이슬점 최저': [12.1, 29.1],\n",
              " '내부 이슬점 평균': [12.4, 29.9]}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "# 분석에 사용할 feature 선택\n",
        "csv_features = ['내부 온도 1 평균', '내부 온도 1 최고', '내부 온도 1 최저', '내부 습도 1 평균', '내부 습도 1 최고', \n",
        "                '내부 습도 1 최저', '내부 이슬점 평균', '내부 이슬점 최고', '내부 이슬점 최저']\n",
        "\n",
        "# csv_files = sorted(glob('sample_data/*/*.csv'))\n",
        "allfile = glob(path + '/sample_data/sample_data/*/*.csv')\n",
        "csv_files = sorted(allfile)\n",
        "\n",
        "temp_csv = pd.read_csv(csv_files[0])[csv_features]\n",
        "max_arr, min_arr = temp_csv.max().to_numpy(), temp_csv.min().to_numpy()\n",
        "\n",
        "# feature 별 최대값, 최솟값 계산\n",
        "for csv in tqdm(csv_files[1:]):\n",
        "    temp_csv = pd.read_csv(csv)[csv_features]\n",
        "    temp_max, temp_min = temp_csv.max().to_numpy(), temp_csv.min().to_numpy()\n",
        "    max_arr = np.max([max_arr,temp_max], axis=0)\n",
        "    min_arr = np.min([min_arr,temp_min], axis=0)\n",
        "\n",
        "# feature 별 최대값, 최솟값 dictionary 생성\n",
        "csv_feature_dict = {csv_features[i]:[min_arr[i], max_arr[i]] for i in range(len(csv_features))}\n",
        "csv_feature_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "_F1p9Hsao7He"
      },
      "outputs": [],
      "source": [
        "# 제공된 sample data는 파프리카와 시설포도 2종류의 작물만 존재\n",
        "label_description = {\n",
        " '3_00_0': '파프리카_정상',\n",
        " '3_a9_1': '파프리카흰가루병_초기',\n",
        " '3_a9_2': '파프리카흰가루병_중기',\n",
        " '3_a9_3': '파프리카흰가루병_말기',\n",
        " '3_a10_1': '파프리카잘록병_초기',\n",
        " '3_a10_2': '파프리카잘록병_중기',\n",
        " '3_a10_3': '파프리카잘록병_말기',\n",
        " '3_b3_1': '칼슘결핍_초기',\n",
        " '3_b3_2': '칼슘결핍_중기',\n",
        " '3_b3_3': '칼슘결핍_말기',\n",
        " '3_b6_1': '다량원소결핍 (N)_초기',\n",
        " '3_b6_2': '다량원소결핍 (N)_중기',\n",
        " '3_b6_3': '다량원소결핍 (N)_말기',\n",
        " '3_b7_1': '다량원소결핍 (P)_초기',\n",
        " '3_b7_2': '다량원소결핍 (P)_중기',\n",
        " '3_b7_3': '다량원소결핍 (P)_말기',\n",
        " '3_b8_1': '다량원소결핍 (K)_초기',\n",
        " '3_b8_2': '다량원소결핍 (K)_중기',\n",
        " '3_b8_3': '다량원소결핍 (K)_말기',\n",
        " '6_00_0': '시설포도_정상',\n",
        " '6_a11_1': '시설포도탄저병_초기',\n",
        " '6_a11_2': '시설포도탄저병_중기',\n",
        " '6_a11_3': '시설포도탄저병_말기',\n",
        " '6_a12_1': '시설포도노균병_초기',\n",
        " '6_a12_2': '시설포도노균병_중기',\n",
        " '6_a12_3': '시설포도노균병_말기',\n",
        " '6_b4_1': '일소피해_초기',\n",
        " '6_b4_2': '일소피해_중기',\n",
        " '6_b4_3': '일소피해_말기',\n",
        " '6_b5_1': '축과병_초기',\n",
        " '6_b5_2': '축과병_중기',\n",
        " '6_b5_3': '축과병_말기',\n",
        "}\n",
        "\n",
        "label_encoder = {key:idx for idx, key in enumerate(label_description)}\n",
        "label_decoder = {val:key for key, val in label_encoder.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 305,
      "metadata": {
        "id": "hKG93lIQQ8jE"
      },
      "outputs": [],
      "source": [
        "class DataController():\n",
        "    def __init__(self,csvfeatures,csvfeaturedict):\n",
        "        self.csv_features = csvfeatures\n",
        "        self.csv_feature_dict = csvfeaturedict\n",
        "    \n",
        "    def road_csv(self,foldnam,timenum):\n",
        "        df = pd.read_csv(foldnam)\n",
        "        return df.loc[:timenum-1,self.csv_features]      #csv파일 제일 짧은게 291개임 오류인지는 모르겠으나 일단 최소길이로 통일하여 처리\n",
        "    \n",
        "    def scaling(self,minmaxdic,df):\n",
        "        for col in minmaxdic.keys():\n",
        "            df.loc[:,col] = df.loc[:,col] - csv_feature_dict[col][0]\n",
        "            df.loc[:,col] = df.loc[:,col] / (csv_feature_dict[col][1]-csv_feature_dict[col][0])\n",
        "        return df\n",
        "    \n",
        "    \n",
        "    def getimage(self,imgpath):\n",
        "        img = cv2.imread(imgpath)\n",
        "        img = cv2.resize(img, dsize=(224, 224), interpolation=cv2.INTER_AREA)\n",
        "        img = img.astype(np.float32)/255  ##픽셀값을 0~1사이로 정규화\n",
        "        # img = np.transpose(img, (2,0,1))\n",
        "        return img\n",
        "    \n",
        "    def getlable(self,jsonpath):\n",
        "        with open(jsonpath, 'r') as f:\n",
        "            json_file = json.load(f)\n",
        "\n",
        "        crop = json_file['annotations']['crop']\n",
        "        disease = json_file['annotations']['disease']\n",
        "        risk = json_file['annotations']['risk']\n",
        "        label = f'{crop}_{disease}_{risk}'\n",
        "        return label\n",
        "    \n",
        "    def getdata(self,datapath,timenum,featnum):\n",
        "\n",
        "        csvarr = np.empty((0,timenum,featnum), float)\n",
        "        imgarr = np.empty((0,224,224,3), float)\n",
        "        lablearr = np.array([])\n",
        "        \n",
        "        # predictor = np.append(predictor,ndf.reshape(-1,timenum,featnum),axis = 0)\n",
        "        \n",
        "        for ind,i in enumerate(datapath):\n",
        "            \n",
        "            if glob(i + '/*.csv') == []:  #10462폴더 비어있음. 다음에 확인해보기\n",
        "                pass\n",
        "            else:\n",
        "                csvpath = glob(i + '/*.csv')[0]\n",
        "                imgpath = glob(i + '/*.jpg')[0]\n",
        "                jsonpath = glob(i + '/*.json')[0]\n",
        "                # con = DataController()\n",
        "                df = self.road_csv(csvpath,timenum)\n",
        "                df2 = self.scaling(csv_feature_dict,df).to_numpy().reshape(-1,timenum,featnum)\n",
        "                imgdata = self.getimage(imgpath).reshape(-1,224,224,3)\n",
        "                label = label_encoder[self.getlable(jsonpath)]\n",
        "                # label = self.getlable(jsonpath)\n",
        "                \n",
        "                csvarr = np.append(csvarr,df2, axis = 0)\n",
        "                imgarr = np.append(imgarr,imgdata, axis = 0)\n",
        "                lablearr = np.append(lablearr,label)\n",
        "            \n",
        "        return [csvarr,imgarr],lablearr\n",
        "            \n",
        "        \n",
        "        \n",
        "        \n",
        "          "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 306,
      "metadata": {
        "id": "HhiLYlj2Q8jF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        },
        "outputId": "04a9ba66-d694-4c76-c5e8-e74de253cc8b"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-306-983a43ec45fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# 배치화된 데이터셋 만들기\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# test셋용으로는 train = False로 하여 배치안생성하게됨\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdacon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetdata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainfiles\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m260\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdacon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetdata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestfiles\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m260\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-305-68f150874d6a>\u001b[0m in \u001b[0;36mgetdata\u001b[0;34m(self, datapath, timenum, featnum)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgetdata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdatapath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtimenum\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeatnum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mcsvarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtimenum\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeatnum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0mimgarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mlablearr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'float32' is not defined"
          ]
        }
      ],
      "source": [
        "#프로젝트에 있는 모든 데이터폴터 불러오기\n",
        "data_files = glob(path + '/sample_data/sample_data/*')\n",
        "#셔플\n",
        "random.shuffle(data_files)\n",
        "#앞에서 300번째까지 트레인셋으로\n",
        "trainfiles = data_files[:400]\n",
        "#나머지는 테스트셋으로\n",
        "testfiles = data_files[400:]\n",
        "\n",
        "# 데이터 컨트롤러\n",
        "dacon = DataController(csv_features,csv_feature_dict)\n",
        "# 배치화된 데이터셋 만들기\n",
        "# test셋용으로는 train = False로 하여 배치안생성하게됨\n",
        "x_train,y_train = dacon.getdata(trainfiles,260,9)\n",
        "x_test,y_test = dacon.getdata(testfiles,260,9)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train[1].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zJZyjBqZXKC6",
        "outputId": "d8284ff2-6a98-4e33-bb17-67bf38c1adcd"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(399, 224, 224, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_RESNET50 = ResNet50(weights='imagenet')\n",
        "class preprmodel(keras.Model):\n",
        "    def __init__(self, imgmodel,name = None):\n",
        "        super(preprmodel, self).__init__()\n",
        "        self.imgmodel = imgmodel\n",
        "        \n",
        "        self.block1_layer1 = tf.keras.layers.LSTM(128)\n",
        "        self.block1_layer2 = tf.keras.layers.Dense(64)\n",
        "        self.block1_layer3 = tf.keras.layers.Dense(32)\n",
        "        \n",
        "\n",
        "\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x1 = self.imgmodel(inputs[1])\n",
        "\n",
        "        x2 = self.block1_layer1(inputs[0])\n",
        "        x2 = self.block1_layer2(x2)\n",
        "        x2 = self.block1_layer3(x2)\n",
        "    \n",
        "        x = tf.concat([x1,x2],axis=1)\n",
        "        return(x)"
      ],
      "metadata": {
        "id": "omTL5fUqR5hy"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_RESNET50 = ResNet50(weights='imagenet')\n",
        "prepromodel = preprmodel(model_RESNET50)"
      ],
      "metadata": {
        "id": "5s3zvRmZQQOx"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_prime_train = prepromodel(x_train)\n",
        "x_prime_test = prepromodel(x_test)"
      ],
      "metadata": {
        "id": "UmN_rwypTIfv"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBClassifier, XGBRegressor\n",
        "\n",
        "xgb = XGBClassifier(random_state=100)\n",
        "xgb.fit(x_prime_train,y_train)\n",
        "\n"
      ],
      "metadata": {
        "id": "zx3J3Iz4TSlz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8470cd27-a4b9-4076-8fcb-e5a30d7eeaca"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "XGBClassifier(objective='multi:softprob', random_state=100)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "y_pred=xgb.predict(x_prime_test)\n",
        "answer = np.array([label_description[label_decoder[int(val)]] for val in y_test])\n",
        "predss = np.array([label_description[label_decoder[int(val)]] for val in y_pred])\n",
        "\n",
        "new_crosstab = pd.crosstab(answer, predss, rownames=['answer'], colnames=['preds'])\n",
        "new_crosstab"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "Ie7znX8rUKcC",
        "outputId": "e2b3abf1-ee15-4107-c5c0-771c5d41d68e"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-d6ae5e29-f57c-46a9-8d66-bd063cc2a502\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>preds</th>\n",
              "      <th>시설포도_정상</th>\n",
              "      <th>시설포도노균병_중기</th>\n",
              "      <th>시설포도탄저병_초기</th>\n",
              "      <th>파프리카_정상</th>\n",
              "      <th>파프리카잘록병_말기</th>\n",
              "      <th>파프리카흰가루병_중기</th>\n",
              "      <th>파프리카흰가루병_초기</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>answer</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>시설포도_정상</th>\n",
              "      <td>37</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>시설포도노균병_중기</th>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>시설포도노균병_초기</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>시설포도탄저병_초기</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>일소피해_말기</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>일소피해_초기</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>축과병_초기</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>파프리카_정상</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>35</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>파프리카잘록병_중기</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>파프리카잘록병_초기</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>파프리카흰가루병_말기</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>파프리카흰가루병_중기</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>파프리카흰가루병_초기</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d6ae5e29-f57c-46a9-8d66-bd063cc2a502')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d6ae5e29-f57c-46a9-8d66-bd063cc2a502 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d6ae5e29-f57c-46a9-8d66-bd063cc2a502');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "preds        시설포도_정상  시설포도노균병_중기  ...  파프리카흰가루병_중기  파프리카흰가루병_초기\n",
              "answer                            ...                          \n",
              "시설포도_정상           37           0  ...            0            0\n",
              "시설포도노균병_중기         0           4  ...            0            0\n",
              "시설포도노균병_초기         1           0  ...            1            0\n",
              "시설포도탄저병_초기         1           0  ...            0            0\n",
              "일소피해_말기            1           0  ...            0            0\n",
              "일소피해_초기            0           0  ...            0            0\n",
              "축과병_초기             2           0  ...            0            0\n",
              "파프리카_정상            0           0  ...            0            0\n",
              "파프리카잘록병_중기         0           0  ...            0            0\n",
              "파프리카잘록병_초기         0           0  ...            1            0\n",
              "파프리카흰가루병_말기        0           0  ...            0            0\n",
              "파프리카흰가루병_중기        0           0  ...            1            3\n",
              "파프리카흰가루병_초기        0           0  ...            2            6\n",
              "\n",
              "[13 rows x 7 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "# Print the confusion matrix\n",
        "print(metrics.confusion_matrix(y_test, y_pred))\n",
        "\n",
        "# Print the precision and recall, among other metrics\n",
        "print(metrics.classification_report(y_test, y_pred, digits=3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BAz1V5voWOez",
        "outputId": "ebe67f08-2725-4e85-d4bf-881a241880ac"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[35  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 2  6  2  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 0  3  1  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 1  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 0  0  1  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  1  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0 37  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  1  0  0  0  0  0  0]\n",
            " [ 0  0  1  0  0  0  0  1  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  4  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  1  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  1  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  2  0  0  0  0  0  0]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0      0.921     1.000     0.959        35\n",
            "         1.0      0.667     0.600     0.632        10\n",
            "         2.0      0.200     0.250     0.222         4\n",
            "         3.0      0.000     0.000     0.000         1\n",
            "         4.0      0.000     0.000     0.000         1\n",
            "         5.0      0.000     0.000     0.000         1\n",
            "         6.0      0.000     0.000     0.000         0\n",
            "        19.0      0.881     1.000     0.937        37\n",
            "        20.0      0.000     0.000     0.000         1\n",
            "        23.0      0.000     0.000     0.000         2\n",
            "        24.0      1.000     1.000     1.000         4\n",
            "        26.0      0.000     0.000     0.000         1\n",
            "        28.0      0.000     0.000     0.000         1\n",
            "        29.0      0.000     0.000     0.000         2\n",
            "\n",
            "    accuracy                          0.830       100\n",
            "   macro avg      0.262     0.275     0.268       100\n",
            "weighted avg      0.763     0.830     0.794       100\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.metrics import f1_score\n",
        "# # f1_score(y_true, y_pred, average=[‘micro’, ‘macro’, ‘samples’,’weighted’ 중 하나 선택])\n",
        "# f1_score(y_test, y_pred, average='weighted')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PRjcy28nWwfa",
        "outputId": "d2230542-98c9-424d-cd98-76d7fab17363"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.3809290236470763"
            ]
          },
          "metadata": {},
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **LSTM, VGG16으로 전처리 후 XGB로 처리 + 베이지안 최적화**"
      ],
      "metadata": {
        "id": "JMsrbqTaUlob"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications.resnet import ResNet50"
      ],
      "metadata": {
        "id": "_zlCUS_pU3cy"
      },
      "execution_count": 307,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 312,
      "metadata": {
        "id": "IhNVKQf-U3cz"
      },
      "outputs": [],
      "source": [
        "class DataController():\n",
        "    def __init__(self,csvfeatures,csvfeaturedict):\n",
        "        self.csv_features = csvfeatures\n",
        "        self.csv_feature_dict = csvfeaturedict\n",
        "    \n",
        "    def road_csv(self,foldnam,timenum):\n",
        "        df = pd.read_csv(foldnam)\n",
        "        return df.loc[:timenum-1,self.csv_features]      #csv파일 제일 짧은게 291개임 오류인지는 모르겠으나 일단 최소길이로 통일하여 처리\n",
        "    \n",
        "    def scaling(self,minmaxdic,df):\n",
        "        for col in minmaxdic.keys():\n",
        "            df.loc[:,col] = df.loc[:,col] - csv_feature_dict[col][0]\n",
        "            df.loc[:,col] = df.loc[:,col] / (csv_feature_dict[col][1]-csv_feature_dict[col][0])\n",
        "        return df\n",
        "    \n",
        "    \n",
        "    def getimage(self,imgpath):\n",
        "        img = cv2.imread(imgpath)\n",
        "        img = cv2.resize(img, dsize=(224, 224), interpolation=cv2.INTER_AREA)\n",
        "        img = img.astype(np.float32)/255  ##픽셀값을 0~1사이로 정규화\n",
        "        # img = np.transpose(img, (2,0,1))\n",
        "        return img\n",
        "    \n",
        "    def getlable(self,jsonpath):\n",
        "        with open(jsonpath, 'r') as f:\n",
        "            json_file = json.load(f)\n",
        "\n",
        "        crop = json_file['annotations']['crop']\n",
        "        disease = json_file['annotations']['disease']\n",
        "        risk = json_file['annotations']['risk']\n",
        "        label = f'{crop}_{disease}_{risk}'\n",
        "        return label\n",
        "    \n",
        "    def getdata(self,datapath,timenum,featnum):\n",
        "\n",
        "        csvarr = np.empty((0,timenum,featnum), dtype = float)\n",
        "        imgarr = np.empty((0,224,224,3), float)\n",
        "        lablearr = np.array([])\n",
        "        \n",
        "        # predictor = np.append(predictor,ndf.reshape(-1,timenum,featnum),axis = 0)\n",
        "        \n",
        "        for ind,i in enumerate(datapath):\n",
        "            \n",
        "            if glob(i + '/*.csv') == []:  #10462폴더 비어있음. 다음에 확인해보기\n",
        "                pass\n",
        "            else:\n",
        "                csvpath = glob(i + '/*.csv')[0]\n",
        "                imgpath = glob(i + '/*.jpg')[0]\n",
        "                jsonpath = glob(i + '/*.json')[0]\n",
        "                # con = DataController()\n",
        "                df = self.road_csv(csvpath,timenum)\n",
        "                df2 = self.scaling(csv_feature_dict,df).to_numpy().reshape(-1,timenum,featnum)\n",
        "                imgdata = self.getimage(imgpath).reshape(-1,224,224,3)\n",
        "                label = label_encoder[self.getlable(jsonpath)]\n",
        "                # label = self.getlable(jsonpath)\n",
        "                \n",
        "                csvarr = np.append(csvarr,df2, axis = 0)\n",
        "                imgarr = np.append(imgarr,imgdata, axis = 0)\n",
        "                lablearr = np.append(lablearr,label)\n",
        "            \n",
        "        return [csvarr,imgarr],lablearr\n",
        "            \n",
        "        \n",
        "        \n",
        "        \n",
        "          "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 313,
      "metadata": {
        "id": "AAXfFYGZU3cz"
      },
      "outputs": [],
      "source": [
        "#프로젝트에 있는 모든 데이터폴터 불러오기\n",
        "data_files = glob(path + '/sample_data/sample_data/*')\n",
        "#셔플\n",
        "random.shuffle(data_files)\n",
        "#앞에서 300번째까지 트레인셋으로\n",
        "trainfiles = data_files[:400]\n",
        "#나머지는 테스트셋으로\n",
        "testfiles = data_files[400:]\n",
        "\n",
        "# 데이터 컨트롤러\n",
        "dacon = DataController(csv_features,csv_feature_dict)\n",
        "# 배치화된 데이터셋 만들기\n",
        "# test셋용으로는 train = False로 하여 배치안생성하게됨\n",
        "x_train,y_train = dacon.getdata(trainfiles,260,9)\n",
        "x_test,y_test = dacon.getdata(testfiles,260,9)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class preprmodel(keras.Model):\n",
        "    def __init__(self, imgmodel,name = None):\n",
        "        super(preprmodel, self).__init__()\n",
        "        self.imgmodel = imgmodel\n",
        "        \n",
        "        self.block1_layer1 = tf.keras.layers.LSTM(128)\n",
        "        self.block1_layer2 = tf.keras.layers.Dense(64)\n",
        "        self.block1_layer3 = tf.keras.layers.Dense(32)\n",
        "        \n",
        "\n",
        "\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x1 = self.imgmodel(inputs[1])\n",
        "\n",
        "        x2 = self.block1_layer1(inputs[0])\n",
        "        x2 = self.block1_layer2(x2)\n",
        "        x2 = self.block1_layer3(x2)\n",
        "    \n",
        "        x = tf.concat([x1,x2],axis=1)\n",
        "        return(x)"
      ],
      "metadata": {
        "id": "hj9O96GHU3cz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_RESNET50 = ResNet50(weights='imagenet')\n",
        "prepromodel = preprmodel(model_RESNET50)"
      ],
      "metadata": {
        "id": "xspgu8QLU3cz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_prime_train = prepromodel(x_train)\n",
        "x_prime_test = prepromodel(x_test)"
      ],
      "metadata": {
        "id": "yccu2FyGU3c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**최적화파트**"
      ],
      "metadata": {
        "id": "oMg5vAjiVFti"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "from hyperopt import fmin, tpe, hp, Trials, space_eval, STATUS_OK\n",
        "from sklearn.metrics import f1_score\n",
        "import time\n",
        "# # f1_score(y_true, y_pred, average=[‘micro’, ‘macro’, ‘samples’,’weighted’ 중 하나 선택])\n",
        "# f1_score(y_test, y_pred, average='macro')"
      ],
      "metadata": {
        "id": "nQctv9zpVfE6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###서치공간\n",
        "space_xgb = [hp.uniform('learn',0.01,0.8),            #학습률\n",
        "         hp.quniform('nesti',100,350,q=10),          #\n",
        "         hp.quniform('maxd',3,8,q=1),                #\n",
        "         hp.quniform('minc',  1,10,q=1),             #과적합을 방지할 목적으로사용한다. 기본값은 1이지만 너무 높은값은 오히려 과소적합을 일으키기 때문에 적절한 값을 찾아야함 \n",
        "         hp.uniform('gamm',  0.1,3),              #\n",
        "         hp.uniform('subsa',  0.5,1),\n",
        "         hp.uniform('colsample_b',  0.6,0.9),\n",
        "         #hp.quniform('scalepos',  0.8,1.2,q=0.1)\n",
        "         #hp.quniform('times',  0,6,q=1)\n",
        "         ]\n",
        "XGBClassifier()\n",
        "###함수\n",
        "def XGB(args):\n",
        "    learn,nesti,maxd,minc,gamm,subsa,colsample_b = args\n",
        "    nesti,maxd,minc=int(nesti),int(maxd),int(minc)#,int(times)\n",
        "    xgb=XGBClassifier(\n",
        "        learning_rate =learn,\n",
        "        n_estimators=nesti,\n",
        "        max_depth=maxd,\n",
        "        min_child_weight=minc,\n",
        "        gamma=gamm,\n",
        "        subsample=subsa,\n",
        "        colsample_bytree=colsample_b,\n",
        "        objective= 'binary:logistic',\n",
        "        # scale_pos_weight=scalepos,\n",
        "        #seed=seed_value,\n",
        "        n_jobs = -1,\n",
        "        seed = 100\n",
        "        )\n",
        "    \n",
        "    for t in range(len(Train_set_X)):\n",
        "        x_train, y_train = Train_set_X[t], Train_set_y[t]\n",
        "        x_val, y_val = Val_set_X[t], Val_set_y[t]\n",
        "        xgb.fit(x_train, y_train.ravel()) #validation_data=(x_val, y_val))\n",
        "        y_pred=xgb.predict(x_val)\n",
        "        y_pred=y_pred.reshape((len(y_pred),1),)\n",
        "\n",
        "\n",
        "        result = -f1_score(y_val, y_pred, average='macro')\n",
        "        resultdf.loc[t,['result']]=result\n",
        "        \n",
        "        del x_train, y_train\n",
        "        del x_val, y_val\n",
        "    #print(resultdf['auc'])\n",
        "    result = resultdf['result'].mean()\n",
        "    return(result)"
      ],
      "metadata": {
        "id": "eiut5DDyVA4R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import StratifiedKFold, KFold, RepeatedKFold\n",
        "kf = RepeatedKFold(n_splits=10, n_repeats=10,random_state = 100)\n",
        "# kf = StratifiedKFold()\n",
        "Train_set_X = []\n",
        "Train_set_y = []\n",
        "Val_set_X = []\n",
        "Val_set_y = []\n",
        "#\n",
        "x_prime_train = np.array(x_prime_train)\n",
        "x_prime_test = np.array(x_prime_test)\n",
        "\n",
        "#\n",
        "for train_index, val_index in kf.split(x_prime_train,y_train):\n",
        "    print(\"TRAIN:\", train_index, \"Val:\", val_index)\n",
        "    X_tra, X_val = x_prime_train[train_index,:], x_prime_train[val_index,:]\n",
        "    y_tra, y_val = y_train[train_index], y_train[val_index]\n",
        "\n",
        "    Train_set_X.append(X_tra)\n",
        "    Train_set_y.append(y_tra)\n",
        "    Val_set_X.append(X_val)\n",
        "    Val_set_y.append(y_val)\n",
        "\n",
        "Train_set_X = np.array(Train_set_X)\n",
        "Train_set_y = np.array(Train_set_y)\n",
        "Val_set_X = np.array(Val_set_X)\n",
        "Val_set_y = np.array(Val_set_y)\n",
        "#\n",
        "result = {\"result\":[np.nan]}\n",
        "resultdf = pd.DataFrame(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "slAy1b0bVZ6i",
        "outputId": "cc873d84-241e-4b2a-fbb5-479d38e42636"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAIN: [  0   1   2   3   4   5   6   8   9  10  11  12  13  14  15  16  17  19\n",
            "  20  21  22  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38\n",
            "  39  40  41  42  44  45  46  47  48  49  50  51  52  53  54  55  57  58\n",
            "  59  60  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76\n",
            "  77  79  80  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95\n",
            "  99 100 101 102 104 105 106 107 108 109 110 111 112 113 114 115 116 117\n",
            " 119 120 121 122 123 126 128 130 131 132 133 134 135 136 137 138 139 141\n",
            " 142 143 144 145 146 147 148 149 150 152 154 155 156 157 158 159 160 161\n",
            " 162 163 164 165 166 167 168 170 171 172 174 175 176 177 178 179 180 181\n",
            " 182 183 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200\n",
            " 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219\n",
            " 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 237 238 239\n",
            " 240 241 243 244 245 246 247 249 250 251 252 253 254 255 256 257 258 259\n",
            " 260 261 262 264 265 266 267 268 269 270 273 274 276 277 279 280 281 283\n",
            " 284 286 287 288 289 290 291 293 294 296 297 298 299 300 301 302 303 304\n",
            " 305 306 307 308 309 310 311 312 315 316 317 318 319 320 321 322 323 324\n",
            " 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 342 343\n",
            " 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361\n",
            " 362 363 364 365 366 367 368 370 371 372 373 374 375 376 377 378 379 380\n",
            " 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398] Val: [  7  18  23  43  56  78  96  97  98 103 118 124 125 127 129 140 151 153\n",
            " 169 173 184 201 235 236 242 248 263 271 272 275 278 282 285 292 295 313\n",
            " 314 341 369 381]\n",
            "TRAIN: [  0   2   3   4   6   7   8   9  10  11  12  13  14  15  16  17  18  19\n",
            "  20  22  23  24  25  26  27  29  30  31  32  34  35  36  37  38  39  40\n",
            "  41  42  43  44  45  46  47  48  49  50  51  52  53  55  56  57  58  59\n",
            "  60  61  62  63  65  66  67  68  70  71  72  73  74  76  77  78  79  80\n",
            "  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98\n",
            "  99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116\n",
            " 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 135\n",
            " 137 138 139 140 141 142 143 144 145 146 147 148 150 151 152 153 154 155\n",
            " 156 157 159 160 161 162 163 164 165 167 168 169 170 171 172 173 175 176\n",
            " 177 179 180 181 182 183 184 185 186 188 189 190 191 192 193 194 195 196\n",
            " 197 199 200 201 202 203 204 207 209 210 211 212 213 214 215 216 218 219\n",
            " 220 221 222 223 224 226 227 228 229 230 232 233 234 235 236 237 238 239\n",
            " 240 241 242 243 245 246 247 248 249 250 251 252 253 254 255 256 257 259\n",
            " 261 262 263 264 265 266 268 269 270 271 272 273 274 275 276 277 278 279\n",
            " 280 281 282 283 284 285 286 287 289 290 291 292 293 294 295 296 297 298\n",
            " 299 300 302 303 304 305 307 308 309 310 311 312 313 314 316 317 318 319\n",
            " 320 321 322 323 324 325 326 327 328 329 330 331 333 334 335 336 337 338\n",
            " 339 340 341 342 343 344 345 347 348 350 351 352 353 354 355 356 358 359\n",
            " 360 361 362 363 364 365 366 369 370 371 372 373 374 375 376 377 378 379\n",
            " 380 381 382 383 384 385 387 388 389 390 391 392 394 395 396 397 398] Val: [  1   5  21  28  33  54  64  69  75 134 136 149 158 166 174 178 187 198\n",
            " 205 206 208 217 225 231 244 258 260 267 288 301 306 315 332 346 349 357\n",
            " 367 368 386 393]\n",
            "TRAIN: [  0   1   2   4   5   7   8   9  10  11  12  13  14  16  17  18  21  22\n",
            "  23  24  25  26  28  29  30  32  33  34  35  36  37  38  39  40  41  42\n",
            "  43  44  45  47  48  49  51  52  53  54  55  56  57  58  59  61  62  63\n",
            "  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  81  82\n",
            "  83  85  86  87  88  89  91  92  93  94  95  96  97  98  99 100 101 102\n",
            " 103 104 105 106 107 108 109 111 112 113 114 115 116 117 118 119 120 122\n",
            " 123 124 125 126 127 128 129 130 131 132 134 135 136 137 138 139 140 141\n",
            " 142 143 144 145 146 147 148 149 150 151 153 154 155 156 158 159 160 161\n",
            " 162 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180\n",
            " 181 182 183 184 185 186 187 189 192 193 194 196 198 199 200 201 202 203\n",
            " 205 206 207 208 209 210 211 212 214 215 216 217 218 219 220 221 222 223\n",
            " 224 225 226 227 228 230 231 232 234 235 236 237 238 239 240 242 243 244\n",
            " 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262\n",
            " 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 280 281\n",
            " 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299\n",
            " 300 301 302 303 304 305 306 307 308 309 311 312 313 314 315 316 317 318\n",
            " 319 320 321 323 324 325 326 327 328 329 330 331 332 333 334 336 337 338\n",
            " 340 341 342 343 344 345 346 348 349 350 351 352 353 354 355 356 357 358\n",
            " 359 360 361 363 364 365 366 367 368 369 370 371 372 373 374 375 377 378\n",
            " 380 381 382 383 384 386 387 388 389 390 391 392 393 395 396 397 398] Val: [  3   6  15  19  20  27  31  46  50  60  80  84  90 110 121 133 152 157\n",
            " 163 188 190 191 195 197 204 213 229 233 241 279 310 322 335 339 347 362\n",
            " 376 379 385 394]\n",
            "TRAIN: [  0   1   2   3   4   5   6   7   8   9  11  12  13  14  15  16  17  18\n",
            "  19  20  21  22  23  24  25  27  28  29  30  31  32  33  34  36  37  38\n",
            "  40  42  43  44  46  48  49  50  51  52  53  54  55  56  57  58  60  61\n",
            "  62  63  64  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80\n",
            "  81  82  83  84  85  86  87  89  90  91  92  93  94  95  96  97  98  99\n",
            " 100 101 102 103 105 106 107 108 109 110 111 112 113 114 115 116 117 118\n",
            " 119 121 122 123 124 125 126 127 129 130 131 133 134 135 136 137 138 139\n",
            " 140 141 143 144 145 146 148 149 151 152 153 154 155 156 157 158 159 162\n",
            " 163 164 165 166 167 168 169 170 172 173 174 175 176 177 178 179 180 181\n",
            " 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 200\n",
            " 201 202 203 204 205 206 207 208 210 211 212 213 214 215 216 217 218 220\n",
            " 221 222 223 224 225 226 227 228 229 230 231 232 233 235 236 237 238 239\n",
            " 240 241 242 243 244 245 246 248 250 251 252 253 255 256 257 258 259 260\n",
            " 261 262 263 264 266 267 268 269 270 271 272 273 274 275 276 277 278 279\n",
            " 280 281 282 283 284 285 286 287 288 289 290 292 294 295 296 297 298 299\n",
            " 301 302 303 305 306 307 308 309 310 311 312 313 314 315 316 317 319 321\n",
            " 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339\n",
            " 340 341 342 343 344 345 346 347 348 349 350 351 353 354 355 356 357 358\n",
            " 359 361 362 363 364 365 366 367 368 369 370 371 372 373 374 376 377 378\n",
            " 379 380 381 383 384 385 386 387 388 389 390 391 393 394 395 396 397] Val: [ 10  26  35  39  41  45  47  59  65  88 104 120 128 132 142 147 150 160\n",
            " 161 171 199 209 219 234 247 249 254 265 291 293 300 304 318 320 352 360\n",
            " 375 382 392 398]\n",
            "TRAIN: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
            "  18  19  20  21  22  23  24  25  26  27  28  30  31  33  34  35  36  37\n",
            "  38  39  41  42  43  44  45  46  47  49  50  51  53  54  55  56  57  58\n",
            "  59  60  62  63  64  65  66  67  68  69  70  72  73  74  75  76  77  78\n",
            "  79  80  81  82  83  84  86  87  88  89  90  91  92  93  94  95  96  97\n",
            "  98  99 100 101 102 103 104 105 106 107 108 109 110 112 113 114 116 117\n",
            " 118 119 120 121 122 124 125 127 128 129 130 131 132 133 134 135 136 137\n",
            " 138 140 141 142 143 144 145 146 147 148 149 150 151 152 153 155 156 157\n",
            " 158 160 161 163 165 166 167 168 169 170 171 172 173 174 176 178 179 180\n",
            " 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198\n",
            " 199 200 201 202 204 205 206 207 208 209 210 211 212 213 214 215 216 217\n",
            " 218 219 220 221 222 223 224 225 226 228 229 230 231 232 233 234 235 236\n",
            " 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254\n",
            " 255 256 257 258 260 261 262 263 264 265 266 267 268 269 270 271 272 273\n",
            " 274 275 276 277 278 279 280 282 283 284 285 286 288 289 290 291 292 293\n",
            " 294 295 296 297 298 299 300 301 303 304 305 306 307 309 310 312 313 314\n",
            " 315 316 317 318 319 320 322 323 324 326 328 329 331 332 334 335 336 339\n",
            " 340 341 342 343 344 346 347 348 349 350 352 353 354 355 356 357 358 359\n",
            " 360 361 362 363 364 366 367 368 369 370 372 373 374 375 376 377 379 380\n",
            " 381 382 383 384 385 386 387 388 389 390 392 393 394 395 396 397 398] Val: [ 29  32  40  48  52  61  71  85 111 115 123 126 139 154 159 162 164 175\n",
            " 177 203 227 259 281 287 302 308 311 321 325 327 330 333 337 338 345 351\n",
            " 365 371 378 391]\n",
            "TRAIN: [  0   1   2   3   4   5   6   7   8   9  10  12  13  14  15  16  17  18\n",
            "  19  20  21  23  24  26  27  28  29  30  31  32  33  34  35  36  38  39\n",
            "  40  41  43  44  45  46  47  48  49  50  52  53  54  56  57  58  59  60\n",
            "  61  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78  79\n",
            "  80  82  84  85  86  88  89  90  91  92  93  94  95  96  97  98 100 102\n",
            " 103 104 105 106 107 109 110 111 113 114 115 116 118 119 120 121 122 123\n",
            " 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141\n",
            " 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159\n",
            " 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177\n",
            " 178 180 181 182 183 184 185 186 187 188 190 191 192 193 195 196 197 198\n",
            " 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 217\n",
            " 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235\n",
            " 236 237 238 240 241 242 243 244 245 247 248 249 250 251 253 254 256 258\n",
            " 259 260 261 262 263 264 265 267 269 270 271 272 273 274 275 276 277 278\n",
            " 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 298\n",
            " 299 300 301 302 303 304 306 307 308 310 311 312 313 314 315 316 317 318\n",
            " 320 321 322 323 325 326 327 328 329 330 331 332 333 334 335 336 337 338\n",
            " 339 340 341 342 343 344 345 346 347 349 350 351 352 353 354 355 356 357\n",
            " 358 359 360 361 362 363 364 365 366 367 368 369 371 372 375 376 378 379\n",
            " 380 381 382 385 386 387 388 389 390 391 392 393 394 395 396 397 398] Val: [ 11  22  25  37  42  51  55  62  81  83  87  99 101 108 112 117 179 189\n",
            " 194 216 239 246 252 255 257 266 268 296 297 305 309 319 324 348 370 373\n",
            " 374 377 383 384]\n",
            "TRAIN: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
            "  18  19  20  21  22  23  25  26  27  28  29  30  31  32  33  35  36  37\n",
            "  38  39  40  41  42  43  45  46  47  48  49  50  51  52  53  54  55  56\n",
            "  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72  75  76  78\n",
            "  79  80  81  83  84  85  86  87  88  89  90  91  93  94  95  96  97  98\n",
            "  99 100 101 102 103 104 105 106 107 108 109 110 111 112 114 115 116 117\n",
            " 118 119 120 121 122 123 124 125 126 127 128 129 131 132 133 134 135 136\n",
            " 137 138 139 140 141 142 143 147 149 150 151 152 153 154 155 157 158 159\n",
            " 160 161 162 163 164 165 166 167 169 170 171 172 173 174 175 176 177 178\n",
            " 179 180 181 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197\n",
            " 198 199 201 202 203 204 205 206 207 208 209 211 212 213 214 215 216 217\n",
            " 219 220 222 223 225 226 227 228 229 231 233 234 235 236 237 238 239 240\n",
            " 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258\n",
            " 259 260 261 262 263 265 266 267 268 270 271 272 273 274 275 276 277 278\n",
            " 279 280 281 282 283 285 286 287 288 289 290 291 292 293 294 295 296 297\n",
            " 299 300 301 302 303 304 305 306 308 309 310 311 312 313 314 315 316 317\n",
            " 318 319 320 321 322 323 324 325 327 329 330 331 332 333 334 335 336 337\n",
            " 338 339 341 342 343 344 345 346 347 348 349 350 351 352 354 355 357 359\n",
            " 360 361 362 363 364 365 367 368 369 370 371 373 374 375 376 377 378 379\n",
            " 380 381 382 383 384 385 386 388 389 390 391 392 393 394 396 397 398] Val: [ 24  34  44  57  73  74  77  82  92 113 130 144 145 146 148 156 168 182\n",
            " 200 210 218 221 224 230 232 264 269 284 298 307 326 328 340 353 356 358\n",
            " 366 372 387 395]\n",
            "TRAIN: [  0   1   2   3   4   5   6   7   8  10  11  13  14  15  17  18  19  20\n",
            "  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  39\n",
            "  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57\n",
            "  58  59  60  61  62  63  64  65  66  68  69  70  71  72  73  74  75  77\n",
            "  78  79  80  81  82  83  84  85  86  87  88  90  91  92  93  96  97  98\n",
            "  99 100 101 103 104 105 106 108 110 111 112 113 115 117 118 120 121 123\n",
            " 124 125 126 127 128 129 130 131 132 133 134 135 136 137 139 140 141 142\n",
            " 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160\n",
            " 161 162 163 164 166 167 168 169 171 172 173 174 175 176 177 178 179 180\n",
            " 181 182 183 184 185 187 188 189 190 191 192 193 194 195 197 198 199 200\n",
            " 201 202 203 204 205 206 207 208 209 210 211 212 213 215 216 217 218 219\n",
            " 221 224 225 226 227 228 229 230 231 232 233 234 235 236 238 239 240 241\n",
            " 242 244 245 246 247 248 249 251 252 254 255 257 258 259 260 261 263 264\n",
            " 265 266 267 268 269 270 271 272 273 274 275 278 279 280 281 282 283 284\n",
            " 285 287 288 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304\n",
            " 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322\n",
            " 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340\n",
            " 341 342 343 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359\n",
            " 360 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378\n",
            " 379 381 382 383 384 385 386 387 390 391 392 393 394 395 396 397 398] Val: [  9  12  16  38  67  76  89  94  95 102 107 109 114 116 119 122 138 165\n",
            " 170 186 196 214 220 222 223 237 243 250 253 256 262 276 277 286 289 344\n",
            " 361 380 388 389]\n",
            "TRAIN: [  1   3   4   5   6   7   8   9  10  11  12  14  15  16  18  19  20  21\n",
            "  22  23  24  25  26  27  28  29  31  32  33  34  35  37  38  39  40  41\n",
            "  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59\n",
            "  60  61  62  64  65  66  67  69  71  73  74  75  76  77  78  79  80  81\n",
            "  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99\n",
            " 100 101 102 103 104 107 108 109 110 111 112 113 114 115 116 117 118 119\n",
            " 120 121 122 123 124 125 126 127 128 129 130 132 133 134 135 136 137 138\n",
            " 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156\n",
            " 157 158 159 160 161 162 163 164 165 166 168 169 170 171 173 174 175 177\n",
            " 178 179 182 183 184 186 187 188 189 190 191 193 194 195 196 197 198 199\n",
            " 200 201 203 204 205 206 208 209 210 211 213 214 216 217 218 219 220 221\n",
            " 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 239 240\n",
            " 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258\n",
            " 259 260 262 263 264 265 266 267 268 269 270 271 272 275 276 277 278 279\n",
            " 280 281 282 284 285 286 287 288 289 290 291 292 293 295 296 297 298 300\n",
            " 301 302 304 305 306 307 308 309 310 311 313 314 315 316 317 318 319 320\n",
            " 321 322 323 324 325 326 327 328 330 332 333 335 337 338 339 340 341 343\n",
            " 344 345 346 347 348 349 350 351 352 353 354 356 357 358 359 360 361 362\n",
            " 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380\n",
            " 381 382 383 384 385 386 387 388 389 391 392 393 394 395 396 397 398] Val: [  0   2  13  17  30  36  63  68  70  72 105 106 131 167 172 176 180 181\n",
            " 185 192 202 207 212 215 238 261 273 274 283 294 299 303 312 329 331 334\n",
            " 336 342 355 390]\n",
            "TRAIN: [  0   1   2   3   5   6   7   9  10  11  12  13  15  16  17  18  19  20\n",
            "  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38\n",
            "  39  40  41  42  43  44  45  46  47  48  50  51  52  54  55  56  57  59\n",
            "  60  61  62  63  64  65  67  68  69  70  71  72  73  74  75  76  77  78\n",
            "  80  81  82  83  84  85  87  88  89  90  92  94  95  96  97  98  99 101\n",
            " 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119\n",
            " 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 136 138 139\n",
            " 140 142 144 145 146 147 148 149 150 151 152 153 154 156 157 158 159 160\n",
            " 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178\n",
            " 179 180 181 182 184 185 186 187 188 189 190 191 192 194 195 196 197 198\n",
            " 199 200 201 202 203 204 205 206 207 208 209 210 212 213 214 215 216 217\n",
            " 218 219 220 221 222 223 224 225 227 229 230 231 232 233 234 235 236 237\n",
            " 238 239 241 242 243 244 246 247 248 249 250 252 253 254 255 256 257 258\n",
            " 259 260 261 262 263 264 265 266 267 268 269 271 272 273 274 275 276 277\n",
            " 278 279 281 282 283 284 285 286 287 288 289 291 292 293 294 295 296 297\n",
            " 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315\n",
            " 318 319 320 321 322 324 325 326 327 328 329 330 331 332 333 334 335 336\n",
            " 337 338 339 340 341 342 344 345 346 347 348 349 351 352 353 355 356 357\n",
            " 358 360 361 362 365 366 367 368 369 370 371 372 373 374 375 376 377 378\n",
            " 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 398] Val: [  4   8  14  49  53  58  66  79  86  91  93 100 135 137 141 143 155 183\n",
            " 193 211 226 228 240 245 251 270 280 290 316 317 323 343 350 354 359 363\n",
            " 364 396 397]\n",
            "TRAIN: [  1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  17  18  20\n",
            "  21  22  23  24  25  26  27  28  29  30  31  32  33  35  36  37  38  39\n",
            "  40  42  43  44  45  46  47  49  50  51  52  53  54  55  56  57  58  59\n",
            "  60  61  62  63  64  66  67  68  69  70  71  72  74  75  76  77  78  79\n",
            "  80  81  82  83  84  85  86  88  89  90  91  92  93  94  95  96  97  99\n",
            " 100 102 103 104 105 107 108 109 110 111 112 113 115 116 117 119 120 121\n",
            " 122 123 124 125 126 127 128 129 130 132 133 135 136 137 138 139 140 141\n",
            " 142 143 144 146 147 148 149 150 151 153 154 155 156 157 158 159 160 161\n",
            " 162 163 164 165 166 167 168 169 171 172 173 174 175 176 177 178 179 180\n",
            " 181 182 183 184 185 187 189 190 191 192 193 194 195 196 197 198 199 200\n",
            " 201 202 203 204 206 207 208 209 211 212 213 214 215 216 217 218 219 220\n",
            " 222 223 224 225 226 227 228 230 231 232 233 234 235 236 237 238 239 240\n",
            " 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258\n",
            " 259 260 261 262 263 264 265 266 268 269 270 271 272 273 274 275 276 277\n",
            " 278 279 280 281 282 283 284 285 286 287 288 290 291 292 293 295 296 297\n",
            " 298 299 300 301 302 304 305 306 307 308 309 310 312 313 314 315 316 317\n",
            " 318 319 320 321 322 323 325 327 328 329 330 331 332 333 334 335 337 338\n",
            " 341 342 343 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359\n",
            " 360 361 363 364 365 366 367 368 369 370 371 372 373 374 377 378 379 380\n",
            " 381 382 383 384 385 386 387 388 390 391 392 393 394 395 396 397 398] Val: [  0  16  19  34  41  48  65  73  87  98 101 106 114 118 131 134 145 152\n",
            " 170 186 188 205 210 221 229 267 289 294 303 311 324 326 336 339 340 344\n",
            " 362 375 376 389]\n",
            "TRAIN: [  0   1   2   3   4   5   6   7   9  11  12  13  14  15  16  17  18  19\n",
            "  20  21  22  23  24  25  26  27  29  31  32  33  34  35  36  37  38  41\n",
            "  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59\n",
            "  60  61  62  63  64  65  66  67  68  69  70  71  73  74  75  76  77  78\n",
            "  79  80  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96\n",
            "  98  99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115\n",
            " 116 117 118 119 120 121 123 124 126 127 128 129 130 131 132 133 134 135\n",
            " 136 139 140 141 142 144 145 146 147 148 150 151 152 154 155 156 158 160\n",
            " 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178\n",
            " 179 180 181 182 183 184 186 187 188 189 190 191 192 193 194 195 196 197\n",
            " 199 203 204 205 206 207 208 209 210 211 214 216 217 218 219 220 221 222\n",
            " 223 224 225 226 227 228 229 230 231 232 234 235 236 237 238 239 240 241\n",
            " 242 243 244 245 246 247 248 249 250 251 252 253 254 257 258 259 260 261\n",
            " 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 278 279 280\n",
            " 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298\n",
            " 300 301 302 303 305 306 307 308 309 310 311 312 313 314 316 318 319 321\n",
            " 322 323 324 325 326 327 328 329 331 333 334 335 336 337 338 339 340 341\n",
            " 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360\n",
            " 362 363 364 365 366 367 368 369 370 371 372 374 375 376 378 379 380 381\n",
            " 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398] Val: [  8  10  28  30  39  40  72  97 122 125 137 138 143 149 153 157 159 185\n",
            " 198 200 201 202 212 213 215 233 255 256 277 299 304 315 317 320 330 332\n",
            " 342 361 373 377]\n",
            "TRAIN: [  0   1   2   3   4   5   6   7   8   9  10  11  12  14  15  16  17  18\n",
            "  19  20  21  22  23  25  26  27  28  29  30  31  32  33  34  35  36  37\n",
            "  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55\n",
            "  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72  73\n",
            "  75  76  77  78  79  81  82  83  84  85  86  87  88  89  90  91  92  93\n",
            "  94  95  96  97  98  99 100 101 102 104 105 106 107 108 111 112 113 114\n",
            " 115 116 117 118 120 121 122 123 124 125 126 127 128 129 130 131 132 134\n",
            " 135 136 137 138 139 140 141 142 143 144 145 147 148 149 150 151 152 153\n",
            " 154 155 157 159 161 162 163 165 166 167 168 169 170 171 172 174 176 177\n",
            " 178 179 180 181 182 183 184 185 186 187 188 189 190 192 193 194 196 197\n",
            " 198 199 200 201 202 203 204 205 207 208 209 210 211 212 213 214 215 216\n",
            " 217 218 220 221 223 224 225 226 227 229 230 231 232 233 234 235 236 237\n",
            " 238 239 240 241 243 244 245 246 247 248 249 251 253 254 255 256 257 258\n",
            " 259 260 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277\n",
            " 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295\n",
            " 297 298 299 300 301 302 303 304 305 306 307 309 311 312 313 314 315 316\n",
            " 317 319 320 321 322 324 325 326 327 328 329 330 331 332 333 334 335 336\n",
            " 337 338 339 340 341 342 343 344 346 347 349 351 352 354 355 356 357 359\n",
            " 360 361 362 363 364 365 366 367 368 370 371 372 373 374 375 376 377 378\n",
            " 379 380 382 383 384 385 386 387 388 389 390 391 392 394 396 397 398] Val: [ 13  24  74  80 103 109 110 119 133 146 156 158 160 164 173 175 191 195\n",
            " 206 219 222 228 242 250 252 261 296 308 310 318 323 345 348 350 353 358\n",
            " 369 381 393 395]\n",
            "TRAIN: [  0   1   3   4   5   6   7   8   9  10  11  13  14  15  16  19  20  23\n",
            "  24  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42\n",
            "  43  44  45  46  47  48  49  50  51  52  53  55  57  58  59  60  61  62\n",
            "  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80\n",
            "  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99\n",
            " 100 101 102 103 104 105 106 109 110 111 112 113 114 115 116 118 119 120\n",
            " 122 123 124 125 127 128 129 130 131 133 134 135 136 137 138 139 140 141\n",
            " 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159\n",
            " 160 161 162 163 164 165 166 167 168 169 170 172 173 174 175 176 177 179\n",
            " 180 181 182 183 185 186 187 188 190 191 192 193 194 195 196 198 199 200\n",
            " 201 202 203 205 206 208 209 210 211 212 213 215 217 218 219 220 221 222\n",
            " 223 224 225 226 227 228 229 230 231 232 233 236 237 238 239 240 241 242\n",
            " 243 245 246 247 248 249 250 251 252 253 254 255 256 257 258 260 261 262\n",
            " 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 280 281\n",
            " 282 283 284 285 286 287 288 289 292 293 294 295 296 297 298 299 300 301\n",
            " 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319\n",
            " 320 322 323 324 325 326 327 329 330 332 333 334 335 336 337 338 339 340\n",
            " 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358\n",
            " 359 360 361 362 365 367 368 369 370 371 372 373 374 375 376 377 380 381\n",
            " 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398] Val: [  2  12  17  18  21  22  25  54  56  81 107 108 117 121 126 132 171 178\n",
            " 184 189 197 204 207 214 216 234 235 244 259 279 290 291 321 328 331 363\n",
            " 364 366 378 379]\n",
            "TRAIN: [  0   1   2   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18\n",
            "  19  20  21  22  23  24  25  26  27  28  29  30  32  33  34  36  37  38\n",
            "  39  40  41  43  45  46  47  48  50  51  52  53  54  55  56  57  58  60\n",
            "  62  63  65  67  70  71  72  73  74  75  76  77  78  79  80  81  82  83\n",
            "  84  86  87  88  89  90  91  92  93  94  95  96  97  98  99 101 102 103\n",
            " 106 107 108 109 110 111 112 113 114 115 117 118 119 120 121 122 123 125\n",
            " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
            " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 162\n",
            " 163 164 166 167 168 169 170 171 172 173 174 175 176 177 178 180 181 182\n",
            " 183 184 185 186 187 188 189 190 191 192 193 194 195 197 198 199 200 201\n",
            " 202 204 205 206 207 208 210 211 212 213 214 215 216 219 221 222 223 225\n",
            " 227 228 229 230 231 232 233 234 235 236 237 238 240 241 242 243 244 245\n",
            " 246 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264\n",
            " 265 266 267 269 270 271 272 274 275 276 277 278 279 280 281 282 284 285\n",
            " 286 287 288 289 290 291 292 293 294 295 296 297 299 300 302 303 304 305\n",
            " 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324\n",
            " 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342\n",
            " 343 344 345 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361\n",
            " 362 363 364 365 366 367 368 369 370 371 373 374 375 376 377 378 379 381\n",
            " 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398] Val: [  3  31  35  42  44  49  59  61  64  66  68  69  85 100 104 105 116 124\n",
            " 161 165 179 196 203 209 217 218 220 224 226 239 247 268 273 283 298 301\n",
            " 306 346 372 380]\n",
            "TRAIN: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
            "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
            "  36  37  39  40  41  42  43  44  45  47  48  49  50  51  53  54  55  56\n",
            "  57  58  59  61  62  63  64  65  66  68  69  70  72  73  74  75  76  77\n",
            "  78  79  80  81  82  84  85  86  87  88  89  90  91  92  93  94  96  97\n",
            "  98 100 101 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117\n",
            " 118 119 120 121 122 123 124 125 126 127 128 130 131 132 133 134 135 136\n",
            " 137 138 139 140 142 143 144 145 146 149 150 151 152 153 154 155 156 157\n",
            " 158 159 160 161 162 164 165 166 167 169 170 171 172 173 174 175 176 177\n",
            " 178 179 180 181 182 183 184 185 186 187 188 189 191 194 195 196 197 198\n",
            " 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216\n",
            " 217 218 219 220 221 222 223 224 226 227 228 229 230 231 232 233 234 235\n",
            " 236 237 238 239 240 241 242 244 245 246 247 250 251 252 253 255 256 257\n",
            " 259 260 261 263 265 266 267 268 270 271 272 273 274 275 276 277 279 280\n",
            " 281 282 283 284 285 286 287 289 290 291 292 293 294 295 296 297 298 299\n",
            " 300 301 302 303 304 305 306 307 308 310 311 312 314 315 316 317 318 319\n",
            " 320 321 323 324 325 326 328 330 331 332 333 334 336 337 338 339 340 341\n",
            " 342 343 344 345 346 347 348 350 351 352 353 354 355 356 358 360 361 362\n",
            " 363 364 365 366 367 368 369 371 372 373 374 375 376 377 378 379 380 381\n",
            " 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398] Val: [ 38  46  52  60  67  71  83  95  99 102 129 141 147 148 163 168 190 192\n",
            " 193 225 243 248 249 254 258 262 264 269 278 288 309 313 322 327 329 335\n",
            " 349 357 359 370]\n",
            "TRAIN: [  0   1   2   3   4   5   6   7   8  10  11  12  13  14  15  16  17  18\n",
            "  19  20  21  22  23  24  25  26  27  28  30  31  32  33  34  35  36  37\n",
            "  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  56\n",
            "  57  59  60  61  62  64  65  66  67  68  69  71  72  73  74  76  77  80\n",
            "  81  82  83  84  85  86  87  89  90  91  92  93  94  95  96  97  98  99\n",
            " 100 101 102 103 104 105 106 107 108 109 110 112 113 114 115 116 117 118\n",
            " 119 120 121 122 124 125 126 128 129 131 132 133 134 135 136 137 138 139\n",
            " 140 141 143 145 146 147 148 149 152 153 154 156 157 158 159 160 161 162\n",
            " 163 164 165 166 167 168 170 171 173 175 176 177 178 179 180 181 183 184\n",
            " 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202\n",
            " 203 204 205 206 207 208 209 210 212 213 214 215 216 217 218 219 220 221\n",
            " 222 223 224 225 226 228 229 230 231 233 234 235 236 237 238 239 240 241\n",
            " 242 243 244 245 247 248 249 250 251 252 254 255 256 257 258 259 260 261\n",
            " 262 263 264 266 267 268 269 270 273 274 275 277 278 279 281 282 283 284\n",
            " 285 286 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303\n",
            " 304 305 306 307 308 309 310 311 312 313 314 315 317 318 319 320 321 322\n",
            " 323 324 326 327 328 329 330 331 332 333 334 335 336 338 339 340 341 342\n",
            " 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360\n",
            " 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378\n",
            " 379 380 381 382 383 384 385 387 388 389 390 391 392 393 394 395 398] Val: [  9  29  55  58  63  70  75  78  79  88 111 123 127 130 142 144 150 151\n",
            " 155 169 172 174 182 211 227 232 246 253 265 271 272 276 280 287 316 325\n",
            " 337 386 396 397]\n",
            "TRAIN: [  0   1   2   3   4   5   6   7   8   9  10  12  13  14  16  17  18  19\n",
            "  20  21  22  23  24  25  27  28  29  30  31  32  34  35  37  38  39  40\n",
            "  41  42  43  44  46  47  48  49  50  51  52  54  55  56  58  59  60  61\n",
            "  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  78  79  80\n",
            "  81  82  83  84  85  86  87  88  89  90  91  92  95  96  97  98  99 100\n",
            " 101 102 103 104 105 106 107 108 109 110 111 113 114 116 117 118 119 120\n",
            " 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 137 138 139\n",
            " 141 142 143 144 145 146 147 148 149 150 151 152 153 155 156 157 158 159\n",
            " 160 161 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178\n",
            " 179 180 182 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198\n",
            " 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216\n",
            " 217 218 219 220 221 222 223 224 225 226 227 228 229 230 232 233 234 235\n",
            " 238 239 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256\n",
            " 257 258 259 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275\n",
            " 276 277 278 279 280 282 283 284 285 287 288 289 290 291 293 294 296 297\n",
            " 298 299 300 301 303 304 305 306 308 309 310 311 312 313 314 315 316 317\n",
            " 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 335 336\n",
            " 337 338 339 340 341 342 343 344 345 346 348 349 350 352 353 354 355 357\n",
            " 358 359 360 361 362 363 364 365 366 367 369 370 372 373 375 376 377 378\n",
            " 379 380 381 383 384 385 386 387 388 389 391 392 393 395 396 397 398] Val: [ 11  15  26  33  36  45  53  57  77  93  94 112 115 136 140 154 162 181\n",
            " 183 231 236 237 240 260 281 286 292 295 302 307 334 347 351 356 368 371\n",
            " 374 382 390 394]\n",
            "TRAIN: [  0   1   2   3   8   9  10  11  12  13  14  15  16  17  18  19  20  21\n",
            "  22  23  24  25  26  28  29  30  31  33  34  35  36  38  39  40  41  42\n",
            "  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60\n",
            "  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  77  78  79\n",
            "  80  81  82  83  85  87  88  89  90  91  92  93  94  95  97  98  99 100\n",
            " 101 102 103 104 105 106 107 108 109 110 111 112 114 115 116 117 118 119\n",
            " 120 121 122 123 124 125 126 127 129 130 131 132 133 134 135 136 137 138\n",
            " 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157\n",
            " 158 159 160 161 162 163 164 165 167 168 169 170 171 172 173 174 175 176\n",
            " 178 179 181 182 183 184 185 186 187 188 189 190 191 192 193 195 196 197\n",
            " 198 200 201 202 203 204 205 206 207 209 210 211 212 213 214 215 216 217\n",
            " 218 219 220 221 222 224 225 226 227 228 229 231 232 233 234 235 236 237\n",
            " 239 240 242 243 244 246 247 248 249 250 251 252 253 254 255 256 258 259\n",
            " 260 261 262 263 264 265 266 267 268 269 271 272 273 274 276 277 278 279\n",
            " 280 281 282 283 286 287 288 289 290 291 292 294 295 296 297 298 299 301\n",
            " 302 303 304 305 306 307 308 309 310 311 313 315 316 317 318 319 320 321\n",
            " 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339\n",
            " 340 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358\n",
            " 359 360 361 362 363 364 366 367 368 369 370 371 372 373 374 375 376 377\n",
            " 378 379 380 381 382 385 386 388 389 390 391 392 393 394 395 396 397] Val: [  4   5   6   7  27  32  37  76  84  86  96 113 128 139 166 177 180 194\n",
            " 199 208 223 230 238 241 245 257 270 275 284 285 293 300 312 314 341 365\n",
            " 383 384 387 398]\n",
            "TRAIN: [  0   2   3   4   5   6   7   8   9  10  11  12  13  15  16  17  18  19\n",
            "  21  22  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39\n",
            "  40  41  42  44  45  46  48  49  52  53  54  55  56  57  58  59  60  61\n",
            "  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80\n",
            "  81  83  84  85  86  87  88  93  94  95  96  97  98  99 100 101 102 103\n",
            " 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 121 122\n",
            " 123 124 125 126 127 128 129 130 131 132 133 134 136 137 138 139 140 141\n",
            " 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159\n",
            " 160 161 162 163 164 165 166 168 169 170 171 172 173 174 175 177 178 179\n",
            " 180 181 182 183 184 185 186 188 189 190 191 192 193 194 195 196 197 198\n",
            " 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216\n",
            " 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234\n",
            " 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 252 253\n",
            " 254 255 256 257 258 259 260 261 262 264 265 267 268 269 270 271 272 273\n",
            " 275 276 277 278 279 280 281 283 284 285 286 287 288 289 290 291 292 293\n",
            " 294 295 296 298 299 300 301 302 303 304 306 307 308 309 310 311 312 313\n",
            " 314 315 316 317 318 320 321 322 323 324 325 326 327 328 329 330 331 332\n",
            " 334 335 336 337 339 340 341 342 344 345 346 347 348 349 350 351 353 356\n",
            " 357 358 359 361 362 363 364 365 366 368 369 370 371 372 373 374 375 376\n",
            " 377 378 379 380 381 382 383 384 386 387 389 390 393 394 395 396 397 398] Val: [  1  14  20  23  43  47  50  51  62  82  89  90  91  92 120 135 167 176\n",
            " 187 251 263 266 274 282 297 305 319 333 338 343 352 354 355 360 367 385\n",
            " 388 391 392]\n",
            "TRAIN: [  0   1   3   4   6   7   8   9  10  11  12  14  15  16  17  21  22  23\n",
            "  24  25  26  27  28  29  30  32  33  34  35  36  37  38  39  40  41  42\n",
            "  44  45  46  47  48  50  51  52  54  55  56  57  58  60  61  62  63  64\n",
            "  65  66  67  68  69  71  72  73  74  75  76  77  79  80  81  82  83  84\n",
            "  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 101 102\n",
            " 103 105 106 107 108 109 110 111 113 114 116 117 118 119 120 121 122 123\n",
            " 124 125 126 127 128 129 130 131 132 133 134 136 137 138 139 140 144 145\n",
            " 146 147 148 149 150 151 153 155 156 158 159 160 161 162 163 164 165 166\n",
            " 167 168 170 171 172 173 175 176 177 178 179 180 181 182 183 184 185 186\n",
            " 187 188 189 190 191 192 193 194 195 196 197 198 199 200 202 203 204 205\n",
            " 206 207 208 209 210 211 212 213 214 215 216 218 219 220 221 222 223 224\n",
            " 225 226 227 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243\n",
            " 244 245 246 247 249 251 252 253 254 255 257 258 259 260 261 263 264 265\n",
            " 266 267 268 269 270 271 272 274 275 276 277 278 279 280 281 282 283 284\n",
            " 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302\n",
            " 303 304 305 306 307 309 310 311 313 314 315 316 317 318 319 320 321 323\n",
            " 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341\n",
            " 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 360\n",
            " 361 362 363 365 366 367 368 369 370 371 372 373 375 376 377 378 379 380\n",
            " 381 382 383 384 385 386 387 388 389 390 391 392 394 395 396 397 398] Val: [  2   5  13  18  19  20  31  43  49  53  59  70  78 104 112 115 135 141\n",
            " 142 143 152 154 157 169 174 201 217 228 248 250 256 262 273 308 312 322\n",
            " 359 364 374 393]\n",
            "TRAIN: [  0   1   2   3   4   5   6   7   8  10  11  13  14  15  16  17  18  19\n",
            "  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  38\n",
            "  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  55  56  57\n",
            "  58  59  60  61  62  63  64  65  66  68  69  70  71  72  73  74  75  77\n",
            "  78  79  81  82  83  84  85  86  87  88  89  94  95  96  97  98  99 100\n",
            " 101 103 104 105 106 107 108 109 111 112 113 114 115 117 119 120 121 122\n",
            " 123 124 125 126 128 130 131 132 133 134 135 136 137 138 139 141 142 143\n",
            " 144 145 146 147 148 149 152 153 154 155 156 157 159 160 161 162 163 164\n",
            " 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182\n",
            " 183 184 185 186 188 189 190 191 192 193 194 195 196 197 198 199 200 201\n",
            " 202 203 204 205 207 208 209 210 211 212 213 214 215 216 217 218 219 220\n",
            " 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 238 239\n",
            " 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257\n",
            " 258 259 260 261 262 263 264 265 266 267 269 270 271 272 273 274 275 276\n",
            " 277 278 279 280 281 283 284 285 286 288 289 290 291 293 294 295 296 297\n",
            " 298 299 302 303 304 306 307 308 309 310 312 313 314 315 316 317 318 319\n",
            " 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337\n",
            " 339 340 341 342 344 345 346 347 348 349 350 351 353 354 355 356 357 358\n",
            " 359 360 361 362 363 364 365 366 367 369 370 371 373 374 375 376 377 379\n",
            " 381 382 383 384 385 386 387 388 390 391 392 393 394 395 396 397 398] Val: [  9  12  37  54  67  76  80  90  91  92  93 102 110 116 118 127 129 140\n",
            " 150 151 158 187 206 237 268 282 287 292 300 301 305 311 338 343 352 368\n",
            " 372 378 380 389]\n",
            "TRAIN: [  0   1   2   3   4   5   6   8   9  10  11  12  13  14  15  17  18  19\n",
            "  20  21  22  23  24  25  27  28  29  31  32  33  34  35  36  37  38  39\n",
            "  40  41  42  43  44  45  46  49  50  51  52  53  54  56  57  58  59  60\n",
            "  61  62  63  64  65  67  68  69  70  72  73  74  75  76  77  78  79  80\n",
            "  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  98  99\n",
            " 100 101 102 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118\n",
            " 119 120 121 122 123 124 125 126 127 128 129 130 131 133 135 136 137 138\n",
            " 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157\n",
            " 158 159 160 161 162 163 164 165 166 167 168 169 171 172 174 175 176 177\n",
            " 179 180 181 182 183 184 185 186 187 189 190 191 192 193 194 195 196 197\n",
            " 198 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 217\n",
            " 218 219 220 221 222 223 224 225 226 227 228 231 232 233 235 236 237 238\n",
            " 239 240 242 243 244 245 246 247 248 249 250 252 253 254 255 256 258 259\n",
            " 260 261 262 263 264 265 266 267 268 269 270 271 272 273 275 276 277 278\n",
            " 279 280 281 282 283 285 286 287 288 289 290 292 293 294 296 297 298 299\n",
            " 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317\n",
            " 318 319 320 321 322 323 324 325 326 327 329 331 332 335 336 337 338 339\n",
            " 340 341 342 343 345 347 348 349 350 351 352 353 355 356 357 358 359 360\n",
            " 361 362 363 364 365 366 367 368 369 371 372 374 375 377 378 379 380 381\n",
            " 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398] Val: [  7  16  26  30  47  48  55  66  71  97 103 132 134 139 170 173 178 188\n",
            " 199 216 229 230 234 241 251 257 274 284 291 295 328 330 333 334 344 346\n",
            " 354 370 373 376]\n",
            "TRAIN: [  0   1   2   3   4   5   6   7   8   9  10  12  13  14  15  16  17  18\n",
            "  19  20  21  22  23  24  26  27  28  29  30  31  32  33  34  35  36  37\n",
            "  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55\n",
            "  56  57  58  59  60  61  62  64  65  66  67  68  69  70  71  72  73  74\n",
            "  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91  92\n",
            "  93  94  95  96  97  98 100 101 102 103 104 105 107 110 111 112 113 115\n",
            " 116 117 118 119 120 121 122 125 126 127 128 129 130 131 132 133 134 135\n",
            " 136 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155\n",
            " 156 157 158 159 161 162 163 164 165 166 167 168 169 170 171 173 174 175\n",
            " 177 178 179 180 182 183 184 185 186 187 188 189 192 194 195 196 197 198\n",
            " 199 201 202 203 204 205 206 207 208 209 211 212 213 214 215 216 217 218\n",
            " 219 220 221 222 224 226 227 228 229 230 231 232 233 234 235 236 237 238\n",
            " 239 240 241 242 244 245 246 247 248 250 251 252 253 254 255 256 257 258\n",
            " 259 260 261 262 263 264 265 267 268 269 270 271 272 273 274 275 276 277\n",
            " 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295\n",
            " 296 297 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 315\n",
            " 316 317 318 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334\n",
            " 335 336 337 338 339 341 343 344 345 346 347 349 350 352 354 355 359 360\n",
            " 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379\n",
            " 380 381 382 383 384 385 386 387 389 390 391 392 393 394 396 397 398] Val: [ 11  25  63  99 106 108 109 114 123 124 137 138 160 172 176 181 190 191\n",
            " 193 200 210 223 225 243 249 266 298 314 319 340 342 348 351 353 356 357\n",
            " 358 361 388 395]\n",
            "TRAIN: [  0   1   2   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19\n",
            "  20  21  23  25  26  27  28  29  30  31  32  33  34  35  37  38  39  41\n",
            "  42  43  44  45  46  47  48  49  50  52  53  54  55  56  57  58  59  60\n",
            "  61  62  63  64  65  66  67  68  69  70  71  72  73  75  76  77  78  79\n",
            "  80  81  82  83  84  85  86  88  89  90  91  92  93  95  96  97  98  99\n",
            " 100 102 103 104 105 106 107 108 109 110 112 113 114 115 116 117 118 119\n",
            " 120 121 122 123 124 125 126 127 129 131 132 133 134 135 136 137 138 139\n",
            " 140 141 142 143 144 146 147 148 149 150 151 152 153 154 155 156 157 158\n",
            " 159 160 161 162 163 165 166 167 168 169 170 171 172 173 174 175 176 177\n",
            " 178 179 180 181 182 183 184 186 187 188 189 190 191 192 193 195 196 197\n",
            " 199 200 201 202 203 204 206 208 209 210 212 214 215 216 217 218 219 220\n",
            " 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 237 238 239\n",
            " 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257\n",
            " 258 259 260 262 263 264 265 266 268 269 270 271 272 273 274 275 276 277\n",
            " 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295\n",
            " 296 298 299 300 301 302 303 305 307 308 309 310 311 312 313 314 315 317\n",
            " 318 319 320 321 322 323 324 325 326 328 329 330 332 333 334 335 336 337\n",
            " 338 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356\n",
            " 357 358 359 360 361 362 363 364 365 366 367 368 370 371 372 373 374 375\n",
            " 376 377 378 380 383 385 386 387 388 389 390 391 392 393 395 396 398] Val: [  3   4  22  24  36  40  51  74  87  94 101 111 128 130 145 164 185 194\n",
            " 198 205 207 211 213 236 261 267 297 304 306 316 327 331 339 369 379 381\n",
            " 382 384 394 397]\n",
            "TRAIN: [  0   1   2   3   4   5   6   7   9  10  11  12  13  16  18  19  20  21\n",
            "  22  23  24  25  26  27  29  30  31  32  33  34  35  36  37  39  40  41\n",
            "  42  43  46  47  48  49  51  52  53  54  55  56  58  59  60  61  62  63\n",
            "  64  65  66  67  68  70  71  72  73  74  75  76  77  78  79  80  81  85\n",
            "  86  87  88  89  90  91  92  93  94  96  97  98  99 100 101 102 103 104\n",
            " 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122\n",
            " 123 124 125 127 128 129 130 132 133 134 135 136 137 138 139 140 141 142\n",
            " 143 144 145 147 148 149 150 151 152 153 154 157 158 159 160 161 162 163\n",
            " 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181\n",
            " 182 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201\n",
            " 203 204 205 206 207 209 210 211 212 213 214 215 216 217 218 219 220 221\n",
            " 222 223 224 225 226 227 228 229 230 231 232 234 235 236 237 238 239 240\n",
            " 241 242 243 244 245 247 248 249 250 251 252 253 254 255 256 257 258 260\n",
            " 261 262 263 264 265 266 267 268 270 271 272 273 274 276 277 278 279 280\n",
            " 281 282 283 284 287 288 289 290 291 292 293 294 295 296 297 298 299 300\n",
            " 301 303 304 305 306 308 309 310 311 312 313 314 315 316 317 318 319 320\n",
            " 322 323 324 326 327 328 329 330 331 332 333 334 335 337 338 339 340 341\n",
            " 342 343 344 345 346 347 348 349 350 351 352 353 354 356 357 358 359 360\n",
            " 361 362 363 364 366 367 368 369 370 371 372 373 374 375 376 377 378 379\n",
            " 380 381 382 383 384 386 388 389 390 391 392 393 394 395 396 397 398] Val: [  8  14  15  17  28  38  44  45  50  57  69  82  83  84  95 126 131 146\n",
            " 155 156 183 184 202 208 233 246 259 269 275 285 286 302 307 321 325 336\n",
            " 355 365 385 387]\n",
            "TRAIN: [  0   1   2   3   4   5   6   7   8   9  11  12  13  14  15  16  17  18\n",
            "  19  20  21  22  23  24  25  26  27  28  29  30  31  33  34  35  36  37\n",
            "  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55\n",
            "  57  58  59  60  63  65  66  67  68  69  70  71  72  73  74  75  76  78\n",
            "  79  80  82  83  84  85  86  87  89  90  91  92  93  94  95  96  97  98\n",
            "  99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116\n",
            " 117 118 120 121 122 123 124 125 126 127 128 129 130 131 132 134 135 137\n",
            " 138 139 140 141 142 143 145 146 147 148 150 151 152 153 154 155 156 157\n",
            " 158 159 160 162 164 167 169 170 171 172 173 174 175 176 177 178 179 181\n",
            " 182 183 184 185 186 187 188 190 191 192 193 194 195 197 198 199 200 201\n",
            " 202 204 205 206 207 208 209 210 211 212 213 214 216 217 218 219 220 221\n",
            " 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 240\n",
            " 241 243 244 245 246 247 248 249 250 251 252 253 255 256 257 258 259 260\n",
            " 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 278 279\n",
            " 280 281 282 284 285 286 287 288 290 291 292 293 294 295 296 297 298 300\n",
            " 301 302 304 305 306 307 308 310 311 312 313 314 315 316 318 319 320 321\n",
            " 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 338 339 340\n",
            " 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358\n",
            " 359 361 362 363 364 365 367 368 369 370 371 372 373 374 375 376 377 378\n",
            " 379 380 381 382 383 384 385 387 388 389 391 393 394 395 396 397 398] Val: [ 10  32  56  61  62  64  77  81  88 119 133 136 144 149 161 163 165 166\n",
            " 168 180 189 196 203 215 239 242 254 277 283 289 299 303 309 317 337 360\n",
            " 366 386 390 392]\n",
            "TRAIN: [  0   2   3   4   5   7   8   9  10  11  12  13  14  15  16  17  18  19\n",
            "  20  21  22  23  24  25  26  28  29  30  31  32  33  34  36  37  38  39\n",
            "  40  41  42  43  44  45  47  48  49  50  51  52  53  54  55  56  57  58\n",
            "  59  61  62  63  64  66  67  69  70  71  72  74  75  76  77  78  79  80\n",
            "  81  82  83  84  86  87  88  89  90  91  92  93  94  95  96  97  99 101\n",
            " 102 103 104 105 106 108 109 110 111 112 113 114 115 116 117 118 119 123\n",
            " 124 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142\n",
            " 143 144 145 146 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
            " 162 163 164 165 166 168 169 170 171 172 173 174 175 176 177 178 179 180\n",
            " 181 182 183 184 185 187 188 189 190 191 192 193 194 195 196 197 198 199\n",
            " 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217\n",
            " 218 221 222 223 225 226 227 228 229 230 231 233 234 235 236 237 238 239\n",
            " 240 241 242 243 244 246 247 248 249 250 251 253 254 255 256 257 259 261\n",
            " 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279\n",
            " 281 282 283 284 285 286 287 289 290 291 292 293 294 295 297 298 299 300\n",
            " 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 319\n",
            " 321 322 323 324 325 326 327 328 330 331 332 333 334 335 336 337 338 339\n",
            " 340 342 343 344 346 348 349 350 351 352 353 354 355 356 357 358 359 360\n",
            " 361 362 363 364 365 366 367 368 369 370 372 373 374 375 376 378 379 380\n",
            " 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397] Val: [  1   6  27  35  46  60  65  68  73  85  98 100 107 120 121 122 125 147\n",
            " 167 186 219 220 224 232 245 252 258 260 280 288 296 318 320 329 341 345\n",
            " 347 371 377 398]\n",
            "TRAIN: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
            "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  35  36  37\n",
            "  38  40  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58\n",
            "  59  60  61  62  63  64  65  66  67  68  69  70  71  72  73  74  76  77\n",
            "  78  80  81  82  83  84  85  86  87  88  90  91  92  93  94  95  97  98\n",
            "  99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116\n",
            " 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135\n",
            " 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 154\n",
            " 155 156 157 158 159 160 161 163 164 165 166 167 168 169 170 171 172 173\n",
            " 174 176 177 178 180 181 183 184 185 186 187 188 189 190 191 193 194 195\n",
            " 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 213 215\n",
            " 216 217 219 220 222 223 224 225 226 227 228 229 230 231 232 233 234 236\n",
            " 237 239 241 242 243 245 246 247 248 249 250 251 252 254 256 257 258 259\n",
            " 260 261 262 265 266 267 268 269 270 273 274 275 276 277 278 280 282 283\n",
            " 284 285 286 287 288 289 290 291 292 294 295 296 297 298 299 300 301 302\n",
            " 303 304 305 306 307 308 309 310 311 312 314 316 317 318 319 320 321 322\n",
            " 323 324 325 327 328 329 330 331 333 334 335 336 337 338 339 340 341 342\n",
            " 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360\n",
            " 361 362 363 364 365 366 368 369 370 371 372 373 374 376 377 378 379 380\n",
            " 381 382 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398] Val: [ 33  34  39  41  42  75  79  89  96 117 153 162 175 179 182 192 212 214\n",
            " 218 221 235 238 240 244 253 255 263 264 271 272 279 281 293 313 315 326\n",
            " 332 367 375 383]\n",
            "TRAIN: [  1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18\n",
            "  19  20  22  24  25  26  27  28  30  31  32  33  34  35  36  37  38  39\n",
            "  40  41  42  43  44  45  46  47  48  49  50  51  53  54  55  56  57  59\n",
            "  60  61  62  63  64  65  66  67  68  69  70  71  73  74  75  76  77  78\n",
            "  79  80  81  82  83  84  85  87  88  89  90  91  92  93  94  95  96  97\n",
            "  98  99 100 101 102 103 104 106 107 108 109 110 111 112 114 115 116 117\n",
            " 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135\n",
            " 136 137 138 139 140 141 142 143 144 145 146 147 149 150 151 152 153 154\n",
            " 155 156 157 158 160 161 162 163 164 165 166 167 168 169 170 172 173 174\n",
            " 175 176 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193\n",
            " 194 196 198 199 200 201 202 203 205 206 207 208 210 211 212 213 214 215\n",
            " 216 217 218 219 220 221 223 224 225 228 229 230 232 233 234 235 236 237\n",
            " 238 239 240 241 242 243 244 245 246 248 249 250 251 252 253 254 255 256\n",
            " 257 258 259 260 261 262 263 264 266 267 268 269 271 272 273 274 275 277\n",
            " 279 280 281 282 283 284 285 286 287 288 289 291 292 293 295 296 297 298\n",
            " 299 300 301 302 303 304 305 306 307 308 309 311 312 313 314 315 316 317\n",
            " 318 319 320 321 322 325 326 327 328 329 330 331 332 333 334 336 337 338\n",
            " 339 340 341 342 343 344 345 346 347 348 351 352 353 354 355 356 357 358\n",
            " 359 360 361 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378\n",
            " 379 380 381 382 383 384 385 386 387 388 389 390 392 393 394 395 397 398] Val: [  0  21  23  29  52  58  72  86 105 113 148 159 171 177 195 197 204 209\n",
            " 222 226 227 231 247 265 270 276 278 290 294 310 323 324 335 349 350 362\n",
            " 363 391 396]\n",
            "TRAIN: [  0   1   2   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19\n",
            "  20  21  22  23  24  25  26  28  29  30  31  32  33  34  35  36  37  38\n",
            "  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  55  56  57\n",
            "  58  59  60  61  62  63  64  66  67  68  69  70  72  73  74  75  76  77\n",
            "  78  79  80  81  82  83  84  86  87  88  89  90  91  92  93  95  96  97\n",
            "  98  99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115\n",
            " 116 117 118 119 120 121 122 123 124 125 126 127 128 129 131 132 133 134\n",
            " 135 136 137 138 139 140 141 142 143 144 145 146 147 148 150 151 154 155\n",
            " 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173\n",
            " 174 175 176 177 178 179 181 182 184 185 186 187 189 190 191 192 193 194\n",
            " 195 196 197 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214\n",
            " 215 216 217 220 221 222 224 226 227 228 229 230 231 232 233 234 235 236\n",
            " 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254\n",
            " 255 256 257 259 260 262 263 264 265 266 267 268 269 270 272 273 274 275\n",
            " 276 278 280 281 282 283 284 285 287 288 289 290 291 292 293 294 295 296\n",
            " 297 299 300 301 302 303 304 305 306 307 309 310 311 312 313 314 315 316\n",
            " 317 318 320 321 322 323 324 325 326 328 329 330 331 332 333 334 335 337\n",
            " 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355\n",
            " 356 358 359 360 362 363 364 365 366 368 369 370 371 372 373 374 375 376\n",
            " 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 395] Val: [  3   4  27  54  65  71  85  94 130 149 152 153 180 183 188 198 199 218\n",
            " 219 223 225 258 261 271 277 279 286 298 308 319 327 336 357 361 367 393\n",
            " 394 396 397 398]\n",
            "TRAIN: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
            "  18  19  21  22  23  24  25  27  28  29  30  31  32  33  34  35  36  37\n",
            "  38  39  40  41  42  43  44  45  46  47  48  50  51  52  54  55  57  58\n",
            "  59  60  62  63  64  65  66  67  68  69  70  71  72  74  76  77  78  79\n",
            "  80  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97\n",
            "  98  99 100 101 102 103 104 105 106 107 108 109 110 112 114 117 118 119\n",
            " 120 121 122 123 124 125 126 127 128 129 130 131 133 134 135 136 137 139\n",
            " 140 142 143 144 145 146 148 149 150 151 152 153 154 155 156 157 159 160\n",
            " 162 163 165 166 167 169 170 171 172 175 176 177 178 179 180 181 182 183\n",
            " 184 186 187 188 189 190 191 195 196 197 198 199 200 201 202 203 204 205\n",
            " 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223\n",
            " 224 225 226 227 228 229 230 231 233 234 236 237 238 239 241 242 243 244\n",
            " 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 263\n",
            " 265 266 267 268 270 271 272 273 274 275 276 277 278 279 280 281 282 283\n",
            " 284 285 286 287 289 290 291 292 293 294 295 297 298 299 300 301 302 303\n",
            " 304 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322\n",
            " 323 324 325 326 327 328 329 330 331 332 333 334 336 337 338 339 340 341\n",
            " 342 343 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360\n",
            " 361 362 363 364 365 367 368 369 370 371 372 373 374 375 376 377 379 380\n",
            " 381 382 383 384 385 386 387 388 389 390 391 393 394 395 396 397 398] Val: [ 20  26  49  53  56  61  73  75 111 113 115 116 132 138 141 147 158 161\n",
            " 164 168 173 174 185 192 193 194 232 235 240 262 264 269 288 296 305 335\n",
            " 344 366 378 392]\n",
            "TRAIN: [  0   1   2   3   4   5   6   7   8   9  10  11  13  14  15  16  18  19\n",
            "  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37\n",
            "  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55\n",
            "  56  57  58  59  61  62  63  64  65  67  69  70  71  72  73  75  76  77\n",
            "  78  79  80  81  82  83  84  85  86  88  89  90  91  92  93  94  95  96\n",
            "  97  98  99 100 102 103 104 105 106 107 108 109 110 111 112 113 114 115\n",
            " 116 117 118 119 120 121 122 123 124 125 127 129 130 131 132 133 134 136\n",
            " 137 138 139 140 141 142 144 145 146 147 148 149 150 151 152 153 154 155\n",
            " 156 157 158 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174\n",
            " 175 176 177 178 179 180 182 183 184 185 186 187 188 189 190 191 192 193\n",
            " 194 195 196 198 199 200 201 202 203 205 206 207 208 209 210 212 214 215\n",
            " 216 217 218 219 220 221 222 223 224 225 227 231 232 233 234 235 236 237\n",
            " 238 239 240 241 242 243 244 245 246 247 248 249 253 254 255 256 258 259\n",
            " 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277\n",
            " 279 280 281 282 283 284 286 287 288 289 290 291 292 294 295 296 297 298\n",
            " 300 301 302 303 304 305 306 307 308 309 310 312 313 315 316 317 318 319\n",
            " 320 321 322 323 324 325 326 327 328 329 330 331 333 334 335 336 338 339\n",
            " 340 341 342 343 344 345 346 347 349 351 352 353 354 355 356 357 358 359\n",
            " 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377\n",
            " 378 380 381 383 384 385 386 388 389 390 392 393 394 395 396 397 398] Val: [ 12  17  60  66  68  74  87 101 126 128 135 143 159 181 197 204 211 213\n",
            " 226 228 229 230 250 251 252 257 278 285 293 299 311 314 332 337 348 350\n",
            " 379 382 387 391]\n",
            "TRAIN: [  0   1   3   4   5   6   7   8   9  10  12  13  14  15  16  17  18  19\n",
            "  20  21  22  24  25  26  27  29  32  33  35  36  37  38  40  41  42  43\n",
            "  44  45  46  47  49  50  51  53  54  56  58  60  61  62  63  64  65  66\n",
            "  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  84  85\n",
            "  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 101 102 103\n",
            " 104 105 106 107 108 110 111 112 113 114 115 116 117 118 119 121 122 123\n",
            " 124 126 128 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144\n",
            " 146 147 148 149 150 152 153 154 155 156 157 158 159 160 161 162 163 164\n",
            " 165 166 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183\n",
            " 184 185 186 187 188 189 190 192 193 194 195 196 197 198 199 200 202 204\n",
            " 205 206 207 208 209 210 211 212 213 214 216 217 218 219 220 221 222 223\n",
            " 224 225 226 227 228 229 230 231 232 233 234 235 236 237 239 240 242 243\n",
            " 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261\n",
            " 262 263 264 265 267 268 269 270 271 273 274 275 276 277 278 279 281 282\n",
            " 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300\n",
            " 302 303 304 305 306 307 308 309 311 312 313 314 315 316 317 318 319 320\n",
            " 321 322 323 324 325 326 327 328 329 331 332 334 335 336 337 338 339 340\n",
            " 342 344 345 346 347 348 349 350 352 353 354 355 356 357 358 359 360 361\n",
            " 362 363 364 365 366 367 368 369 370 371 372 373 374 375 377 378 379 380\n",
            " 381 382 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398] Val: [  2  11  23  28  30  31  34  39  48  52  55  57  59  83 109 120 125 127\n",
            " 129 145 151 167 191 201 203 215 238 241 266 272 280 301 310 330 333 341\n",
            " 343 351 376 383]\n",
            "TRAIN: [  0   1   2   3   4   5   6   7   8  10  11  12  13  14  15  17  18  19\n",
            "  20  21  23  24  25  26  27  28  30  31  32  33  34  35  36  37  38  39\n",
            "  41  42  43  44  45  47  48  49  50  52  53  54  55  56  57  58  59  60\n",
            "  61  62  63  64  65  66  67  68  70  71  72  73  74  75  77  78  79  80\n",
            "  81  82  83  84  85  87  88  90  91  92  94  95  96  97  98  99 100 101\n",
            " 102 103 104 105 106 108 109 110 111 112 113 114 115 116 117 119 120 122\n",
            " 123 125 126 127 128 129 130 131 132 133 134 135 136 138 140 141 142 143\n",
            " 144 145 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162\n",
            " 163 164 165 166 167 168 170 171 173 174 176 177 178 179 180 181 182 183\n",
            " 184 185 187 188 190 191 192 193 194 195 196 197 198 199 200 201 202 203\n",
            " 204 205 206 207 209 210 211 212 213 214 215 216 217 218 219 220 221 222\n",
            " 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240\n",
            " 241 242 244 245 247 249 250 251 252 253 255 256 257 258 259 260 261 262\n",
            " 263 264 265 266 267 268 269 270 271 272 273 274 276 277 278 279 280 281\n",
            " 283 284 285 286 287 288 289 290 291 292 293 295 296 297 298 299 300 301\n",
            " 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319\n",
            " 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337\n",
            " 338 339 340 341 342 343 344 345 346 348 349 350 351 352 353 355 356 357\n",
            " 358 359 360 361 362 363 366 367 368 369 370 371 373 375 376 377 378 379\n",
            " 380 381 382 383 384 385 386 387 388 391 392 393 394 395 396 397 398] Val: [  9  16  22  29  40  46  51  69  76  86  89  93 107 118 121 124 137 139\n",
            " 146 169 172 175 186 189 208 243 246 248 254 275 282 294 347 354 364 365\n",
            " 372 374 389 390]\n",
            "TRAIN: [  1   2   3   4   5   6   7   8   9  10  11  12  14  15  16  17  18  19\n",
            "  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37\n",
            "  39  40  41  42  43  44  45  46  47  48  49  51  52  53  54  55  56  57\n",
            "  58  59  60  61  63  64  65  66  67  68  69  70  71  72  73  74  75  76\n",
            "  77  78  80  81  83  84  85  86  87  88  89  90  91  92  93  94  95  96\n",
            "  97  98  99 100 101 102 104 105 106 107 108 109 110 111 112 113 114 115\n",
            " 116 118 120 121 122 124 125 126 127 128 129 130 131 132 133 135 136 137\n",
            " 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 156\n",
            " 157 158 159 160 161 163 164 165 166 167 168 169 170 171 172 173 174 175\n",
            " 176 177 178 179 180 181 183 184 185 186 187 188 189 190 191 192 193 194\n",
            " 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212\n",
            " 213 215 217 218 219 220 222 223 224 225 226 227 228 229 230 231 232 234\n",
            " 235 236 238 239 240 241 242 243 244 246 247 248 249 250 251 252 254 255\n",
            " 256 257 258 259 261 262 263 264 265 266 269 270 271 272 273 274 275 276\n",
            " 277 278 279 280 281 282 283 285 286 287 288 290 291 292 293 294 295 296\n",
            " 298 299 300 301 302 303 305 306 307 308 309 310 311 312 313 314 315 316\n",
            " 317 318 319 320 321 322 323 324 325 327 328 329 330 331 332 333 334 335\n",
            " 336 337 338 339 340 341 343 344 345 346 347 348 349 350 351 354 355 357\n",
            " 358 360 361 362 364 365 366 367 369 370 371 372 373 374 375 376 377 378\n",
            " 379 382 383 384 385 386 387 389 390 391 392 393 394 395 396 397 398] Val: [  0  13  38  50  62  79  82 103 117 119 123 134 155 162 182 214 216 221\n",
            " 233 237 245 253 260 267 268 284 289 297 304 326 342 352 353 356 359 363\n",
            " 368 380 381 388]\n",
            "TRAIN: [  0   1   2   3   4   6   7   8   9  10  11  12  13  14  15  16  17  19\n",
            "  20  21  22  23  24  25  26  27  28  29  30  31  33  34  35  37  38  39\n",
            "  40  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58\n",
            "  59  60  61  62  64  65  66  67  68  69  70  71  73  74  75  76  77  78\n",
            "  79  80  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  98\n",
            "  99 100 101 102 103 105 106 107 109 110 111 112 113 114 115 116 117 118\n",
            " 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136\n",
            " 137 138 139 140 141 143 144 145 146 147 148 149 150 151 152 153 155 156\n",
            " 158 159 160 161 162 163 164 165 167 168 169 171 172 173 174 175 176 177\n",
            " 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196\n",
            " 197 198 199 201 202 203 204 205 208 209 210 211 213 214 215 216 217 218\n",
            " 219 220 221 222 223 225 226 227 228 229 230 232 233 234 235 236 237 238\n",
            " 239 240 241 243 245 246 248 249 250 251 252 253 254 255 256 257 258 259\n",
            " 260 261 262 263 264 266 267 268 269 270 271 272 274 275 276 277 278 279\n",
            " 280 282 283 284 285 286 287 288 289 290 292 293 294 295 296 297 298 299\n",
            " 300 301 302 303 304 305 306 307 308 310 311 312 313 314 315 316 317 319\n",
            " 320 321 322 323 324 326 327 330 332 333 334 335 336 337 338 339 340 341\n",
            " 342 343 344 345 347 348 350 351 352 353 354 355 356 357 358 359 360 361\n",
            " 362 363 364 365 366 367 368 369 372 373 374 375 376 377 378 379 380 381\n",
            " 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398] Val: [  5  18  32  36  41  63  72  96  97 104 108 142 154 157 166 170 178 200\n",
            " 206 207 212 224 231 242 244 247 265 273 281 291 309 318 325 328 329 331\n",
            " 346 349 370 371]\n",
            "TRAIN: [  0   1   2   3   4   5   6   8   9  10  11  12  13  14  15  16  17  18\n",
            "  19  20  22  23  24  25  26  27  28  29  30  31  32  34  35  36  37  38\n",
            "  39  40  41  42  44  46  47  48  49  50  51  52  53  54  55  56  57  58\n",
            "  59  60  61  62  63  65  66  68  69  71  72  73  74  75  76  77  79  81\n",
            "  82  83  84  85  86  87  89  91  92  93  94  96  97  98 100 101 102 103\n",
            " 104 105 106 107 108 109 110 111 113 114 115 116 117 118 119 120 121 122\n",
            " 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140\n",
            " 141 142 143 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159\n",
            " 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178\n",
            " 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 197\n",
            " 198 199 200 201 203 204 206 207 208 209 211 212 213 214 215 216 218 219\n",
            " 221 222 223 224 225 226 227 228 229 230 231 232 233 235 236 237 238 240\n",
            " 241 242 243 244 245 246 247 248 250 251 252 253 254 255 256 257 258 259\n",
            " 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277\n",
            " 278 279 280 281 282 283 284 285 286 288 289 290 291 293 294 296 297 298\n",
            " 299 301 302 304 305 306 307 308 309 310 311 312 314 315 316 317 318 319\n",
            " 320 321 322 323 325 326 327 328 329 330 331 332 333 334 335 336 337 338\n",
            " 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 356 357 359\n",
            " 360 361 363 364 365 366 367 368 369 370 371 372 373 374 375 376 378 379\n",
            " 380 381 382 383 386 387 388 389 390 391 392 393 394 395 396 397 398] Val: [  7  21  33  43  45  64  67  70  78  80  88  90  95  99 112 144 160 196\n",
            " 202 205 210 217 220 234 239 249 287 292 295 300 303 313 324 339 355 358\n",
            " 362 377 384 385]\n",
            "TRAIN: [  0   2   3   4   5   7   9  11  12  13  15  16  17  18  20  21  22  23\n",
            "  24  26  27  28  29  30  31  32  33  34  36  37  38  39  40  41  43  44\n",
            "  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60  61  62\n",
            "  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80\n",
            "  81  82  83  85  86  87  88  89  90  93  94  95  96  97  98  99 101 102\n",
            " 103 104 105 106 107 108 109 111 112 113 114 115 116 117 118 119 120 121\n",
            " 123 124 125 126 127 128 129 130 132 133 134 135 136 137 138 139 140 141\n",
            " 142 143 144 145 146 147 149 150 151 152 153 154 155 156 157 158 159 160\n",
            " 161 162 163 164 166 167 168 169 170 172 173 174 175 176 177 178 179 180\n",
            " 181 182 183 185 186 188 189 190 191 192 193 194 196 197 198 199 200 201\n",
            " 202 203 204 205 206 207 208 210 211 212 213 214 215 216 217 218 219 220\n",
            " 221 223 224 225 226 227 228 229 230 231 232 233 234 235 237 238 239 240\n",
            " 241 242 243 244 245 246 247 248 249 250 251 252 253 254 257 258 259 260\n",
            " 261 262 264 265 266 267 268 269 270 271 272 273 275 276 277 278 279 280\n",
            " 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298\n",
            " 299 300 301 302 303 304 305 306 307 308 309 310 311 313 314 316 317 318\n",
            " 319 321 322 324 325 326 327 328 329 330 331 332 333 335 336 337 338 339\n",
            " 341 342 343 344 346 347 348 349 350 351 352 353 354 355 356 357 358 359\n",
            " 360 361 362 363 364 365 366 367 368 370 371 372 374 375 376 377 378 379\n",
            " 380 381 382 383 384 385 387 388 389 390 391 392 393 394 396 397 398] Val: [  1   6   8  10  14  19  25  35  42  84  91  92 100 110 122 131 148 165\n",
            " 171 184 187 195 209 222 236 255 256 263 274 312 315 320 323 334 340 345\n",
            " 369 373 386 395]\n",
            "TRAIN: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  16  17  18\n",
            "  19  20  21  22  23  25  26  27  28  29  30  31  32  33  34  35  36  38\n",
            "  39  40  41  42  43  45  46  48  49  50  51  52  53  54  55  56  57  59\n",
            "  60  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  78\n",
            "  79  80  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97\n",
            "  99 100 101 103 104 107 108 109 110 111 112 113 115 116 117 118 119 120\n",
            " 121 122 123 124 125 126 127 128 129 130 131 132 134 135 137 138 139 141\n",
            " 142 143 144 145 146 147 148 149 151 152 153 154 155 157 158 159 160 161\n",
            " 162 164 165 166 167 168 169 170 171 172 173 174 175 178 180 181 182 183\n",
            " 184 185 186 187 188 189 191 192 193 194 195 196 197 198 199 200 201 202\n",
            " 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220\n",
            " 221 222 223 224 225 226 228 229 230 231 232 233 234 235 236 237 238 239\n",
            " 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257\n",
            " 258 260 261 262 263 264 265 266 267 268 269 271 272 273 274 275 277 278\n",
            " 279 280 281 282 284 285 286 287 288 289 291 292 293 294 295 296 297 298\n",
            " 299 300 301 303 304 305 308 309 310 311 312 313 314 315 318 319 320 323\n",
            " 324 325 326 327 328 329 330 331 332 333 334 335 336 337 339 340 341 342\n",
            " 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 361\n",
            " 362 363 364 365 366 367 368 369 370 371 372 373 374 376 377 378 379 380\n",
            " 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398] Val: [ 15  24  37  44  47  58  77  81  98 102 105 106 114 133 136 140 150 156\n",
            " 163 176 177 179 190 227 259 270 276 283 290 302 306 307 316 317 321 322\n",
            " 338 360 375]\n",
            "TRAIN: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
            "  19  21  22  24  25  26  27  28  30  31  32  33  34  36  38  39  40  41\n",
            "  42  43  44  45  46  48  49  51  52  53  54  55  56  57  58  59  61  62\n",
            "  64  65  66  67  68  69  70  71  73  74  76  77  78  79  80  81  83  84\n",
            "  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 101 102\n",
            " 103 104 105 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121\n",
            " 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140\n",
            " 141 143 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160\n",
            " 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 177 179 180\n",
            " 183 184 185 186 187 188 189 190 191 192 193 194 195 196 198 199 201 202\n",
            " 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 219 220 221\n",
            " 222 223 224 225 226 227 228 229 230 232 233 235 237 238 239 240 241 242\n",
            " 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260\n",
            " 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 278 279\n",
            " 280 281 282 283 284 285 286 287 288 289 290 292 293 294 295 296 297 298\n",
            " 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316\n",
            " 318 320 321 322 323 324 325 327 329 330 332 333 334 335 336 337 338 340\n",
            " 341 342 343 345 346 347 348 350 351 352 354 355 356 357 358 359 360 362\n",
            " 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380\n",
            " 381 382 383 384 385 387 388 389 390 391 392 393 394 395 396 397 398] Val: [ 18  20  23  29  35  37  47  50  60  63  72  75  82 106 122 142 144 176\n",
            " 178 181 182 197 200 218 231 234 236 277 291 317 319 326 328 331 339 344\n",
            " 349 353 361 386]\n",
            "TRAIN: [  1   2   3   4   5   6   8   9  10  11  12  13  14  15  16  17  18  19\n",
            "  20  21  22  23  24  25  26  28  29  30  31  32  33  35  37  38  39  40\n",
            "  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58\n",
            "  59  60  61  62  63  64  65  66  67  68  70  71  72  73  74  75  76  77\n",
            "  78  79  80  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95\n",
            "  96  97  98  99 100 101 102 103 105 106 107 108 109 110 111 112 113 114\n",
            " 116 117 118 119 120 122 123 124 125 126 129 130 131 132 134 136 137 138\n",
            " 139 140 141 142 143 144 145 147 149 150 151 153 154 155 156 158 159 160\n",
            " 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178\n",
            " 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196\n",
            " 197 198 200 201 202 203 204 205 206 207 208 210 211 212 213 215 216 217\n",
            " 218 219 221 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237\n",
            " 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256\n",
            " 257 258 260 262 263 264 265 267 268 269 270 271 273 274 275 276 277 278\n",
            " 279 280 281 282 283 284 285 286 288 289 290 291 292 293 294 295 296 297\n",
            " 298 299 300 301 302 303 304 306 307 309 310 311 312 314 315 316 317 318\n",
            " 319 320 322 323 324 325 326 327 328 329 330 331 332 334 335 336 337 338\n",
            " 339 340 341 342 343 344 345 347 348 349 350 351 352 353 354 355 356 357\n",
            " 358 359 360 361 362 363 364 366 367 368 369 370 371 372 373 374 375 377\n",
            " 378 379 380 381 382 383 384 386 387 388 389 390 392 395 396 397 398] Val: [  0   7  27  34  36  69 104 115 121 127 128 133 135 146 148 152 157 199\n",
            " 209 214 220 222 238 259 261 266 272 287 305 308 313 321 333 346 365 376\n",
            " 385 391 393 394]\n",
            "TRAIN: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  16  17  18\n",
            "  19  20  21  22  23  26  27  28  29  31  32  33  34  35  36  37  39  40\n",
            "  41  42  43  44  46  47  48  49  50  51  52  53  54  55  56  57  58  59\n",
            "  60  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  77  79\n",
            "  80  81  82  85  86  87  88  89  90  91  92  93  94  95  96  97  98 101\n",
            " 102 103 104 105 106 108 109 110 111 113 114 115 116 117 118 119 120 121\n",
            " 122 123 124 125 126 127 128 129 130 131 132 133 135 136 137 138 139 140\n",
            " 141 142 143 144 145 146 148 149 150 151 152 153 154 156 157 158 159 160\n",
            " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
            " 180 181 182 183 184 185 186 187 188 190 191 192 193 194 195 196 197 198\n",
            " 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216\n",
            " 217 218 219 220 221 222 223 224 225 226 227 228 230 231 232 233 234 235\n",
            " 236 237 238 240 241 242 243 244 247 248 249 250 251 252 253 254 255 256\n",
            " 257 258 259 261 262 263 264 265 266 267 268 269 270 272 274 275 276 277\n",
            " 278 279 280 281 282 283 285 286 287 289 290 291 293 294 295 296 297 298\n",
            " 299 300 301 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317\n",
            " 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335\n",
            " 336 337 338 339 340 341 342 344 345 346 347 348 349 350 351 352 353 355\n",
            " 356 357 359 360 361 362 363 365 366 367 368 369 370 371 372 375 376 377\n",
            " 378 379 380 381 382 383 384 385 386 387 389 391 392 393 394 396 398] Val: [ 15  24  25  30  38  45  76  78  83  84  99 100 107 112 134 147 155 161\n",
            " 189 229 239 245 246 260 271 273 284 288 292 302 343 354 358 364 373 374\n",
            " 388 390 395 397]\n",
            "TRAIN: [  0   1   2   4   5   6   7   8   9  10  12  13  14  15  18  19  20  21\n",
            "  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39\n",
            "  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  57  58  59\n",
            "  60  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78\n",
            "  79  80  81  82  83  84  85  86  87  88  89  90  92  93  94  95  96  97\n",
            "  98  99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 115 116\n",
            " 117 118 120 121 122 124 125 126 127 128 129 130 131 132 133 134 135 136\n",
            " 137 138 139 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155\n",
            " 156 157 158 159 160 161 162 163 165 167 168 169 170 171 172 173 175 176\n",
            " 177 178 179 180 181 182 183 184 185 186 187 188 189 191 192 193 194 195\n",
            " 196 197 198 199 200 201 202 203 204 205 206 207 208 209 211 212 213 214\n",
            " 215 216 217 218 219 220 222 223 226 227 228 229 230 231 232 233 234 235\n",
            " 236 237 238 239 240 241 242 243 244 245 246 248 249 250 251 252 253 254\n",
            " 255 256 257 258 259 260 261 262 264 265 266 268 269 270 271 272 273 274\n",
            " 275 276 277 278 279 280 281 282 283 284 285 286 287 288 290 291 292 293\n",
            " 294 295 297 298 299 300 301 302 303 304 305 306 307 308 310 311 312 313\n",
            " 314 315 316 317 318 319 320 321 324 325 326 327 328 329 330 331 333 334\n",
            " 336 337 338 339 340 342 343 344 345 346 347 348 349 350 352 353 354 355\n",
            " 356 358 360 361 362 363 364 365 367 368 369 371 372 373 374 375 376 377\n",
            " 378 382 383 385 386 387 388 389 390 391 392 393 394 395 396 397 398] Val: [  3  11  16  17  55  56  61  91 114 119 123 140 164 166 174 190 210 221\n",
            " 224 225 247 263 267 289 296 309 322 323 332 335 341 351 357 359 366 370\n",
            " 379 380 381 384]\n",
            "TRAIN: [  0   1   3   4   5   6   7   8   9  10  11  12  14  15  16  17  18  19\n",
            "  20  21  22  23  24  25  26  27  29  30  31  32  33  34  35  36  37  38\n",
            "  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57\n",
            "  59  60  61  63  64  66  68  69  70  72  73  74  75  76  77  78  81  82\n",
            "  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100\n",
            " 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118\n",
            " 119 120 121 122 123 127 128 129 130 131 133 134 135 136 137 138 139 140\n",
            " 141 142 143 144 145 146 147 148 150 151 152 153 154 155 157 158 159 160\n",
            " 161 162 163 164 165 166 167 168 170 171 172 173 174 175 176 177 178 179\n",
            " 180 181 182 183 185 186 188 189 190 191 192 193 194 195 196 197 198 199\n",
            " 200 201 202 204 205 206 208 209 210 211 212 213 214 216 218 220 221 222\n",
            " 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240\n",
            " 241 242 243 245 246 247 248 250 251 252 253 255 256 257 258 259 260 261\n",
            " 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 280\n",
            " 281 282 283 284 285 286 287 288 289 290 291 292 293 296 297 298 300 301\n",
            " 302 303 304 305 306 307 308 309 310 311 313 315 316 317 318 319 320 321\n",
            " 322 323 325 326 327 328 329 330 331 332 333 335 336 337 338 339 340 341\n",
            " 342 343 344 346 347 348 349 351 352 353 354 355 356 357 358 359 360 361\n",
            " 362 363 364 365 366 368 369 370 372 373 374 375 376 377 378 379 380 381\n",
            " 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398] Val: [  2  13  28  39  58  62  65  67  71  79  80 124 125 126 132 149 156 169\n",
            " 184 187 203 207 215 217 219 244 249 254 279 294 295 299 312 314 324 334\n",
            " 345 350 367 371]\n",
            "TRAIN: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  15  16  17  18\n",
            "  19  20  21  22  23  24  25  26  27  28  29  30  31  32  34  35  36  37\n",
            "  38  39  41  42  43  44  45  47  48  49  50  51  53  55  56  58  60  61\n",
            "  62  63  64  65  66  67  68  69  70  71  72  73  75  76  77  78  79  80\n",
            "  82  83  84  85  86  87  88  89  90  91  92  96  97  99 100 102 103 104\n",
            " 105 106 107 108 109 111 112 113 114 115 116 117 118 119 121 122 123 124\n",
            " 125 126 127 128 129 130 131 132 133 134 135 137 138 140 141 142 143 144\n",
            " 145 146 147 148 149 150 151 152 153 155 156 157 158 159 161 162 163 164\n",
            " 165 166 167 168 169 170 171 172 174 175 176 177 178 179 180 181 182 184\n",
            " 185 186 187 188 189 190 192 193 194 195 196 197 198 199 200 201 202 203\n",
            " 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221\n",
            " 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239\n",
            " 240 241 242 244 245 246 247 249 250 251 252 253 254 256 258 259 260 261\n",
            " 262 263 264 265 266 267 268 270 271 272 273 274 277 279 280 281 282 283\n",
            " 284 285 286 287 288 289 290 291 292 293 294 295 296 298 299 300 301 302\n",
            " 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320\n",
            " 321 322 323 324 326 327 328 329 331 332 333 334 335 336 337 338 339 340\n",
            " 341 343 344 345 346 348 349 350 351 352 353 354 355 356 357 358 359 360\n",
            " 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 379\n",
            " 380 381 382 384 385 386 387 388 389 390 391 393 394 395 396 397 398] Val: [ 14  33  40  46  52  54  57  59  74  81  93  94  95  98 101 110 120 136\n",
            " 139 154 160 173 183 191 243 248 255 257 269 275 276 278 297 325 330 342\n",
            " 347 378 383 392]\n",
            "TRAIN: [  0   1   2   3   4   5   7   8   9  10  11  12  13  14  15  16  17  18\n",
            "  20  21  23  24  25  26  27  28  29  30  33  34  35  36  37  38  39  40\n",
            "  41  44  45  46  47  48  49  50  52  53  54  55  56  57  58  59  60  61\n",
            "  62  63  64  65  66  67  69  70  71  72  74  75  76  77  78  79  80  81\n",
            "  82  83  84  85  86  89  90  91  92  93  94  95  96  97  98  99 100 101\n",
            " 102 104 106 107 109 110 111 112 113 114 115 116 118 119 120 121 122 123\n",
            " 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 142\n",
            " 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160\n",
            " 161 162 163 164 166 167 168 169 170 173 174 175 176 177 178 179 180 181\n",
            " 182 183 184 187 188 189 190 191 192 193 194 197 198 199 200 201 203 204\n",
            " 205 207 208 209 210 212 213 214 215 216 217 218 219 220 221 222 223 224\n",
            " 225 226 227 228 229 230 231 232 233 234 235 236 238 239 240 241 242 243\n",
            " 244 245 246 247 248 249 250 251 253 254 255 256 257 258 259 260 261 262\n",
            " 263 264 265 266 267 268 269 271 272 273 275 276 277 278 279 280 281 282\n",
            " 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301\n",
            " 302 303 305 306 307 308 309 310 311 312 313 314 315 317 319 321 322 323\n",
            " 324 325 326 327 328 330 331 332 333 334 335 336 337 338 339 341 342 343\n",
            " 344 345 346 347 348 349 350 351 353 354 355 357 358 359 360 361 362 363\n",
            " 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381\n",
            " 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398] Val: [  6  19  22  31  32  42  43  51  68  73  87  88 103 105 108 117 141 165\n",
            " 171 172 185 186 195 196 202 206 211 237 252 270 274 283 304 316 318 320\n",
            " 329 340 352 356]\n",
            "TRAIN: [  0   2   3   5   6   7   9  10  11  12  13  14  15  16  17  18  19  20\n",
            "  21  22  23  24  25  27  28  29  30  31  32  33  34  35  36  37  38  39\n",
            "  40  42  43  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59\n",
            "  60  61  62  63  64  65  67  68  69  71  72  73  74  75  76  77  78  79\n",
            "  80  81  82  83  84  85  87  88  89  90  91  92  93  94  95  96  97  98\n",
            "  99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116\n",
            " 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134\n",
            " 135 136 137 138 139 140 141 142 143 144 146 147 148 149 150 152 153 154\n",
            " 155 156 157 159 160 161 162 164 165 166 167 168 169 170 171 172 173 174\n",
            " 175 176 178 179 181 182 183 184 185 186 187 188 189 190 191 192 193 195\n",
            " 196 197 199 200 202 203 204 206 207 208 209 210 211 214 215 216 217 218\n",
            " 219 220 221 222 223 224 225 227 228 229 231 232 233 234 235 236 237 238\n",
            " 239 240 241 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257\n",
            " 258 259 260 261 262 263 264 266 267 269 270 271 272 273 274 275 276 277\n",
            " 278 279 280 281 283 284 285 287 288 289 290 291 292 294 295 296 297 299\n",
            " 300 301 302 303 304 305 306 307 308 309 310 312 313 314 315 316 317 318\n",
            " 319 320 321 322 323 324 325 326 328 329 330 331 332 333 334 335 338 339\n",
            " 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 356 357 358\n",
            " 359 360 361 362 364 365 366 367 368 369 370 371 373 374 376 378 379 380\n",
            " 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 397 398] Val: [  1   4   8  26  41  44  66  70  86 145 151 158 163 177 180 194 198 201\n",
            " 205 212 213 226 230 242 265 268 282 286 293 298 311 327 336 337 355 363\n",
            " 372 375 377 396]\n",
            "TRAIN: [  0   1   2   3   4   6   7   8   9  11  13  14  15  16  17  18  19  20\n",
            "  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38\n",
            "  39  40  41  42  43  44  45  46  47  49  50  51  52  53  54  55  56  57\n",
            "  58  59  60  61  62  63  65  66  67  68  69  70  71  72  73  74  75  76\n",
            "  78  79  80  81  82  83  84  85  86  87  88  89  91  92  93  94  95  98\n",
            "  99 100 101 103 104 105 106 107 108 110 111 112 114 115 117 118 119 120\n",
            " 121 122 123 124 125 126 127 128 130 132 133 134 135 136 138 139 140 141\n",
            " 142 143 144 145 146 147 148 149 151 152 154 155 156 157 158 159 160 161\n",
            " 162 163 164 165 166 167 169 171 172 173 174 175 176 177 178 180 181 182\n",
            " 183 184 185 186 187 188 189 190 191 193 194 195 196 197 198 199 200 201\n",
            " 202 203 204 205 206 207 208 209 210 211 212 213 214 215 217 218 219 220\n",
            " 221 222 224 225 226 227 229 230 231 232 233 234 236 237 238 239 242 243\n",
            " 244 245 246 247 248 249 250 252 253 254 255 257 258 259 260 261 262 263\n",
            " 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282\n",
            " 283 284 286 287 288 289 291 292 293 294 295 296 297 298 299 301 302 303\n",
            " 304 305 306 307 308 309 310 311 312 313 314 316 317 318 319 320 321 322\n",
            " 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340\n",
            " 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358\n",
            " 359 361 363 364 365 366 367 369 370 371 372 373 374 375 376 377 378 379\n",
            " 380 381 382 383 384 385 386 387 388 390 391 392 393 394 395 396 397] Val: [  5  10  12  48  64  77  90  96  97 102 109 113 116 129 131 137 150 153\n",
            " 168 170 179 192 216 223 228 235 240 241 251 256 264 285 290 300 315 360\n",
            " 362 368 389 398]\n",
            "TRAIN: [  0   1   2   3   4   5   6   7   8  10  11  12  13  14  15  16  17  18\n",
            "  19  20  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37\n",
            "  38  39  40  41  42  43  44  45  46  47  48  50  51  52  54  55  56  57\n",
            "  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75\n",
            "  76  77  78  79  80  81  82  83  84  86  87  88  90  91  93  94  95  96\n",
            "  97  98  99 100 101 102 103 104 105 106 107 108 109 110 112 113 114 115\n",
            " 116 117 119 120 121 122 123 124 125 126 127 128 129 131 132 133 134 135\n",
            " 136 137 139 140 141 142 144 145 146 147 148 149 150 151 152 153 154 155\n",
            " 156 157 158 160 161 163 164 165 166 168 169 170 171 172 173 174 176 177\n",
            " 178 179 180 181 182 183 184 185 186 187 189 190 191 192 194 195 196 197\n",
            " 198 199 200 201 202 203 205 206 207 209 210 211 212 213 214 215 216 217\n",
            " 218 219 220 221 222 223 224 225 226 228 229 230 231 234 235 236 237 238\n",
            " 239 240 241 242 243 244 245 246 247 248 249 251 252 254 255 256 257 259\n",
            " 260 261 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278\n",
            " 279 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298\n",
            " 299 300 302 304 305 308 309 311 312 313 314 315 316 317 318 319 320 321\n",
            " 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 339 340\n",
            " 341 342 343 344 345 346 347 349 350 351 352 353 354 355 356 357 358 359\n",
            " 360 361 362 363 364 365 366 367 368 370 371 372 373 374 375 376 377 378\n",
            " 379 380 381 383 384 385 386 388 389 390 391 392 393 394 395 396 397 398] Val: [  9  21  49  53  85  89  92 111 118 130 138 143 159 162 167 175 188 193\n",
            " 204 208 227 232 233 250 253 258 262 280 281 301 303 306 307 310 338 348\n",
            " 369 382 387]\n",
            "TRAIN: [  0   1   2   3   4   5   6   7   8   9  11  12  13  14  15  16  17  18\n",
            "  19  21  22  23  24  25  26  27  28  29  31  32  33  34  36  37  38  39\n",
            "  42  43  44  45  46  47  48  49  50  51  52  54  55  56  57  58  59  60\n",
            "  61  62  63  64  65  66  67  68  69  70  72  73  74  75  76  77  79  80\n",
            "  82  84  85  86  87  88  89  90  91  92  93  94  95  99 100 101 102 103\n",
            " 104 105 106 107 108 109 110 111 112 113 114 116 117 118 119 120 121 122\n",
            " 123 124 126 127 129 131 132 133 134 135 136 137 138 140 141 142 143 144\n",
            " 145 146 147 148 149 150 151 152 154 155 157 160 161 162 163 164 165 166\n",
            " 168 169 170 171 172 173 174 175 176 177 178 180 181 182 183 184 185 186\n",
            " 187 188 190 191 192 193 194 196 197 198 199 200 201 203 204 205 206 208\n",
            " 209 210 211 213 214 216 218 219 220 221 222 223 224 225 226 227 228 229\n",
            " 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247\n",
            " 248 249 250 251 252 253 254 255 256 257 259 260 261 262 263 264 266 267\n",
            " 268 269 270 271 272 273 275 276 277 278 279 280 281 282 283 284 285 286\n",
            " 287 288 289 290 292 293 294 295 297 298 299 300 301 302 303 304 305 306\n",
            " 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324\n",
            " 325 326 327 328 329 330 331 332 333 334 336 337 338 339 340 341 342 343\n",
            " 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361\n",
            " 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379\n",
            " 380 381 382 383 384 385 386 387 388 389 390 392 393 394 395 397 398] Val: [ 10  20  30  35  40  41  53  71  78  81  83  96  97  98 115 125 128 130\n",
            " 139 153 156 158 159 167 179 189 195 202 207 212 215 217 258 265 274 291\n",
            " 296 335 391 396]\n",
            "TRAIN: [  0   1   2   3   4   5   6   8   9  10  11  14  15  16  18  19  20  21\n",
            "  22  23  24  25  26  27  28  29  30  31  32  33  35  36  37  38  39  40\n",
            "  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58\n",
            "  59  60  61  62  64  65  66  68  69  70  71  72  73  74  75  76  77  78\n",
            "  79  81  82  83  84  85  87  88  89  90  91  92  93  94  95  96  97  98\n",
            "  99 100 101 102 103 104 105 106 107 109 110 111 112 113 114 115 116 117\n",
            " 118 119 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136\n",
            " 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154\n",
            " 155 156 158 159 161 162 163 164 165 166 167 168 169 170 171 173 175 176\n",
            " 177 178 179 180 181 182 183 185 186 187 189 190 191 193 195 196 197 199\n",
            " 200 201 202 203 204 206 207 208 209 210 211 212 215 216 217 218 220 222\n",
            " 223 224 226 227 228 229 230 232 234 235 236 237 238 239 240 241 242 243\n",
            " 244 245 246 247 248 249 250 251 252 253 254 256 257 258 259 260 261 262\n",
            " 263 264 265 266 268 269 270 272 273 274 276 277 278 279 280 281 282 283\n",
            " 284 285 286 287 288 290 291 292 293 294 295 296 297 298 299 300 301 302\n",
            " 303 304 305 306 307 308 309 310 311 312 313 316 317 319 320 321 322 323\n",
            " 324 325 326 327 328 329 331 332 333 334 335 336 337 338 339 340 341 342\n",
            " 343 344 345 346 347 348 349 350 351 352 354 355 356 357 358 359 360 361\n",
            " 362 363 364 365 366 367 369 370 371 372 373 375 376 377 378 379 380 381\n",
            " 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398] Val: [  7  12  13  17  34  63  67  80  86 108 120 157 160 172 174 184 188 192\n",
            " 194 198 205 213 214 219 221 225 231 233 255 267 271 275 289 314 315 318\n",
            " 330 353 368 374]\n",
            "TRAIN: [  0   1   2   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18\n",
            "  19  20  21  22  23  24  25  26  27  28  29  30  31  32  34  35  36  37\n",
            "  38  39  40  41  42  43  44  45  46  48  49  51  53  54  55  56  57  58\n",
            "  59  60  61  62  63  64  65  66  67  68  69  71  72  74  75  76  77  78\n",
            "  79  80  81  82  83  84  85  86  87  88  90  91  94  95  96  97  98  99\n",
            " 100 101 102 103 104 105 106 108 109 110 111 112 113 114 115 116 117 118\n",
            " 119 120 121 122 123 124 125 127 128 129 130 131 132 133 134 135 136 137\n",
            " 138 139 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158\n",
            " 159 160 161 162 163 164 165 166 167 168 169 170 172 173 174 175 177 178\n",
            " 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 197\n",
            " 198 199 200 201 202 204 205 207 208 209 210 211 212 213 214 215 216 217\n",
            " 218 219 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236\n",
            " 237 238 239 240 241 242 243 244 246 247 248 249 250 251 252 253 255 256\n",
            " 257 258 259 261 263 265 266 267 269 270 271 273 274 275 276 278 279 280\n",
            " 281 282 283 284 285 286 287 288 289 291 292 293 294 295 296 297 298 299\n",
            " 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317\n",
            " 318 320 321 322 323 324 325 326 327 328 330 331 332 333 334 335 336 337\n",
            " 338 339 341 342 343 344 345 346 349 350 351 352 353 354 355 356 357 358\n",
            " 359 360 361 362 363 364 365 368 369 370 371 372 373 374 375 376 377 378\n",
            " 379 380 381 382 383 386 387 388 389 390 391 392 393 394 395 396 397] Val: [  3  33  47  50  52  70  73  89  92  93 107 126 140 141 142 171 176 196\n",
            " 203 206 220 245 254 260 262 264 268 272 277 290 319 329 340 347 348 366\n",
            " 367 384 385 398]\n",
            "TRAIN: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
            "  18  19  20  22  23  25  26  27  28  30  32  33  34  35  36  37  38  39\n",
            "  40  41  42  43  45  46  47  48  49  50  51  52  53  54  55  56  57  59\n",
            "  61  62  63  64  67  68  69  70  71  72  73  74  76  77  78  79  80  81\n",
            "  82  83  84  85  86  87  89  90  91  92  93  94  95  96  97  98  99 100\n",
            " 101 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119\n",
            " 120 121 122 124 125 126 127 128 129 130 131 133 135 136 137 138 139 140\n",
            " 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158\n",
            " 159 160 162 163 164 165 166 167 168 171 172 173 174 176 177 178 179 181\n",
            " 182 183 184 185 186 187 188 189 190 192 193 194 195 196 197 198 199 200\n",
            " 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218\n",
            " 219 220 221 222 223 224 225 226 227 228 229 231 232 233 234 235 236 237\n",
            " 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255\n",
            " 257 258 260 262 263 264 265 266 267 268 269 270 271 272 274 275 277 278\n",
            " 280 281 282 283 285 287 288 289 290 291 292 293 295 296 298 299 300 301\n",
            " 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 318 319 320\n",
            " 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 337 338 339\n",
            " 340 341 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359\n",
            " 360 361 362 363 364 365 366 367 368 369 371 372 373 374 375 376 377 379\n",
            " 381 382 383 384 385 387 388 389 390 391 392 393 394 395 396 397 398] Val: [ 21  24  29  31  44  58  60  65  66  75  88 102 123 132 134 161 169 170\n",
            " 175 180 191 230 256 259 261 273 276 279 284 286 294 297 317 336 342 343\n",
            " 370 378 380 386]\n",
            "TRAIN: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  15  16  17  18\n",
            "  19  20  21  23  24  25  28  29  30  31  32  33  34  35  36  37  38  39\n",
            "  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57\n",
            "  58  59  60  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76\n",
            "  78  79  80  81  82  83  84  86  88  89  90  91  92  93  94  95  96  97\n",
            "  98  99 100 101 102 103 104 105 106 107 108 111 112 113 114 115 116 117\n",
            " 118 119 120 121 123 125 126 127 128 129 130 131 132 133 134 135 136 138\n",
            " 139 140 141 142 143 144 146 147 148 149 150 151 152 153 155 156 157 158\n",
            " 159 160 161 162 164 165 166 167 168 169 170 171 172 173 174 175 176 177\n",
            " 178 179 180 181 183 184 185 187 188 189 190 191 192 193 194 195 196 197\n",
            " 198 199 200 201 202 203 205 206 207 208 210 211 212 213 214 215 217 218\n",
            " 219 220 221 222 223 225 227 228 229 230 231 232 233 234 235 236 239 240\n",
            " 242 243 244 245 246 247 248 249 250 251 252 254 255 256 257 258 259 260\n",
            " 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278\n",
            " 279 280 282 283 284 285 286 287 288 289 290 291 292 294 295 296 297 298\n",
            " 299 301 302 305 306 307 308 310 311 312 314 315 316 317 318 319 320 321\n",
            " 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 339 340\n",
            " 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358\n",
            " 359 360 361 362 363 364 365 366 367 368 369 370 372 373 374 377 378 379\n",
            " 380 381 382 383 384 385 386 387 389 390 391 392 394 395 396 397 398] Val: [ 14  22  26  27  61  77  85  87 109 110 122 124 137 145 154 163 182 186\n",
            " 204 209 216 224 226 237 238 241 253 281 293 300 303 304 309 313 338 371\n",
            " 375 376 388 393]\n",
            "TRAIN: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  17  19  20\n",
            "  21  22  23  24  26  27  28  29  30  31  32  33  34  35  36  37  38  40\n",
            "  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58\n",
            "  59  60  61  62  63  64  65  66  67  70  71  72  73  74  75  76  77  78\n",
            "  79  80  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96\n",
            "  97  98  99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 115\n",
            " 116 117 118 119 120 121 122 123 124 125 126 128 129 130 132 133 134 136\n",
            " 137 139 140 141 142 144 145 146 147 148 150 152 153 154 155 156 157 158\n",
            " 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176\n",
            " 177 178 179 180 181 182 183 184 186 187 188 189 190 191 192 193 194 195\n",
            " 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213\n",
            " 214 215 216 217 218 219 220 221 223 224 225 226 228 229 230 231 232 233\n",
            " 234 235 236 237 238 240 241 242 244 245 247 248 249 250 251 252 253 254\n",
            " 255 256 257 258 259 260 261 262 263 264 265 266 267 268 270 271 272 273\n",
            " 274 275 276 277 278 279 280 281 283 284 285 286 287 288 289 290 291 293\n",
            " 294 295 296 297 298 300 301 302 303 304 306 307 309 310 311 313 314 315\n",
            " 317 318 319 320 321 323 324 325 326 327 329 330 331 332 333 335 336 337\n",
            " 338 339 340 341 342 343 344 345 346 347 348 349 351 352 353 355 357 358\n",
            " 359 360 361 362 363 365 366 367 368 370 371 372 373 374 375 376 377 378\n",
            " 379 380 381 383 384 385 386 388 389 390 391 392 393 395 396 397 398] Val: [ 15  16  18  25  39  68  69 114 127 131 135 138 143 149 151 185 222 227\n",
            " 239 243 246 269 282 292 299 305 308 312 316 322 328 334 350 354 356 364\n",
            " 369 382 387 394]\n",
            "TRAIN: [  0   1   2   3   5   6   7   9  10  12  13  14  15  16  17  18  20  21\n",
            "  22  23  24  25  26  27  29  30  31  33  34  35  36  37  38  39  40  41\n",
            "  42  43  44  45  46  47  48  50  51  52  53  54  55  57  58  59  60  61\n",
            "  62  63  64  65  66  67  68  69  70  71  72  73  75  76  77  78  80  81\n",
            "  82  83  84  85  86  87  88  89  90  92  93  94  96  97  98  99 100 101\n",
            " 102 104 106 107 108 109 110 111 113 114 115 118 119 120 121 122 123 124\n",
            " 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142\n",
            " 143 144 145 146 149 150 151 152 153 154 156 157 158 159 160 161 162 163\n",
            " 164 167 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184\n",
            " 185 186 188 189 191 192 193 194 195 196 197 198 199 200 201 202 203 204\n",
            " 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222\n",
            " 223 224 225 226 227 228 230 231 233 234 235 236 237 238 239 240 241 242\n",
            " 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260\n",
            " 261 262 263 264 265 267 268 269 271 272 273 274 275 276 277 278 279 280\n",
            " 281 282 283 284 285 286 288 289 290 291 292 293 294 295 296 297 298 299\n",
            " 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317\n",
            " 318 319 320 322 323 324 325 326 327 328 329 330 331 333 334 335 336 338\n",
            " 340 341 342 343 344 346 347 348 350 351 352 353 354 355 356 357 358 359\n",
            " 360 362 363 364 365 366 367 368 369 370 371 373 374 375 376 377 378 379\n",
            " 380 381 382 383 384 385 386 387 388 389 391 393 394 395 396 397 398] Val: [  4   8  11  19  28  32  49  56  74  79  91  95 103 105 112 116 117 147\n",
            " 148 155 165 166 168 187 190 229 232 266 270 287 321 332 337 339 345 349\n",
            " 361 372 390 392]\n",
            "TRAIN: [  0   2   3   4   5   6   7   8  10  11  12  13  14  15  16  17  18  19\n",
            "  20  21  22  24  25  26  27  28  29  30  31  32  33  34  35  36  37  39\n",
            "  40  41  42  43  44  46  47  48  49  50  51  52  53  54  56  57  58  59\n",
            "  60  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  77  78\n",
            "  79  80  81  82  83  84  85  86  87  88  89  91  92  93  95  96  97  98\n",
            "  99 100 102 103 104 105 106 107 108 109 110 112 113 114 115 116 117 118\n",
            " 120 122 123 124 125 126 127 128 129 130 131 132 134 135 136 137 138 139\n",
            " 140 141 142 143 144 145 146 147 148 149 150 151 153 154 155 156 157 158\n",
            " 159 160 161 162 163 165 166 167 168 169 170 171 172 174 175 176 177 179\n",
            " 180 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198\n",
            " 200 201 202 203 204 205 206 207 208 209 211 212 213 214 215 216 217 219\n",
            " 220 221 222 223 224 225 226 227 229 230 231 232 233 235 236 237 238 239\n",
            " 240 241 243 244 245 246 248 250 251 252 253 254 255 256 257 258 259 260\n",
            " 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278\n",
            " 279 280 281 282 283 284 285 286 287 289 290 291 292 293 294 296 297 298\n",
            " 299 300 301 303 304 305 306 307 308 309 311 312 313 314 315 316 317 318\n",
            " 319 320 321 322 323 324 325 327 328 329 330 331 332 334 335 336 337 338\n",
            " 339 340 341 342 343 344 345 346 347 348 349 350 352 353 354 356 357 358\n",
            " 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376\n",
            " 378 379 380 382 384 385 386 387 388 390 391 392 393 394 395 396 398] Val: [  1   9  23  38  45  55  76  90  94 101 111 119 121 133 152 164 173 178\n",
            " 181 199 210 218 228 234 242 247 249 288 295 302 310 326 333 351 355 377\n",
            " 381 383 389 397]\n",
            "TRAIN: [  0   1   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18\n",
            "  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36\n",
            "  37  38  39  40  41  42  43  44  45  47  49  50  51  52  53  55  56  57\n",
            "  58  59  60  61  63  64  65  66  67  68  69  70  71  72  73  74  75  76\n",
            "  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91  92  93  94\n",
            "  95  96  97  98 101 102 103 104 105 106 107 108 109 110 111 112 113 114\n",
            " 115 116 117 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133\n",
            " 134 135 136 137 138 139 140 141 142 143 145 147 148 149 150 151 152 153\n",
            " 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171\n",
            " 172 173 174 175 176 178 179 180 181 182 183 184 185 186 187 188 189 190\n",
            " 191 192 193 194 195 196 198 199 200 201 202 203 204 205 206 207 208 209\n",
            " 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227\n",
            " 228 229 230 231 232 233 234 236 237 238 239 240 241 242 243 245 246 247\n",
            " 249 253 254 255 256 258 259 260 261 262 263 264 265 266 267 268 269 270\n",
            " 271 272 273 274 275 276 277 278 279 281 282 284 286 287 288 289 290 291\n",
            " 292 293 294 295 296 297 299 300 302 303 304 305 308 309 310 312 313 314\n",
            " 315 316 317 318 319 321 322 326 327 328 329 330 331 332 333 334 335 336\n",
            " 337 338 339 340 342 343 345 346 347 348 349 350 351 353 354 355 356 359\n",
            " 361 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380\n",
            " 381 382 383 384 385 386 387 388 389 390 391 392 393 394 396 397 398] Val: [  2  46  48  54  62  99 100 118 144 146 177 197 235 244 248 250 251 252\n",
            " 257 280 283 285 298 301 306 307 311 320 323 324 325 341 344 352 357 358\n",
            " 360 362 363 395]\n",
            "TRAIN: [  1   2   3   4   7   8   9  10  11  12  13  14  15  16  17  18  19  20\n",
            "  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  38  39  40\n",
            "  41  44  45  46  47  48  49  50  52  53  54  55  56  58  60  61  62  63\n",
            "  65  66  67  68  69  70  71  73  74  75  76  77  78  79  80  81  83  85\n",
            "  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 101 102 103\n",
            " 105 107 108 109 110 111 112 114 115 116 117 118 119 120 121 122 123 124\n",
            " 125 126 127 128 130 131 132 133 134 135 137 138 139 140 141 142 143 144\n",
            " 145 146 147 148 149 151 152 153 154 155 156 157 158 159 160 161 163 164\n",
            " 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182\n",
            " 184 185 186 187 188 189 190 191 192 194 195 196 197 198 199 202 203 204\n",
            " 205 206 207 209 210 212 213 214 215 216 217 218 219 220 221 222 224 225\n",
            " 226 227 228 229 230 231 232 233 234 235 237 238 239 241 242 243 244 245\n",
            " 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 264\n",
            " 265 266 267 268 269 270 271 272 273 274 275 276 277 279 280 281 282 283\n",
            " 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301\n",
            " 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319\n",
            " 320 321 322 323 324 325 326 328 329 330 332 333 334 335 336 337 338 339\n",
            " 340 341 342 343 344 345 347 348 349 350 351 352 353 354 355 356 357 358\n",
            " 360 361 362 363 364 366 367 368 369 370 371 372 374 375 376 377 378 380\n",
            " 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398] Val: [  0   5   6  36  37  42  43  51  57  59  64  72  82  84 104 106 113 129\n",
            " 136 150 162 183 193 200 201 208 211 223 236 240 263 278 327 331 346 359\n",
            " 365 373 379]\n",
            "TRAIN: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
            "  18  19  20  21  23  24  26  27  30  31  32  34  35  36  38  40  41  42\n",
            "  43  44  46  47  48  49  50  52  53  54  55  56  57  59  60  62  63  64\n",
            "  65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82\n",
            "  84  85  86  87  88  89  90  91  92  94  95  96  97  98  99 101 102 103\n",
            " 104 105 106 107 108 109 110 112 113 114 115 116 117 118 119 120 121 122\n",
            " 124 125 126 127 128 129 130 131 132 133 135 136 137 139 140 141 142 143\n",
            " 144 145 146 147 150 151 152 153 154 155 156 157 158 159 160 161 162 163\n",
            " 164 165 166 167 168 169 170 171 172 173 175 176 177 178 179 180 181 182\n",
            " 183 184 185 186 188 191 192 193 194 195 196 197 198 199 201 202 203 205\n",
            " 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223\n",
            " 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241\n",
            " 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259\n",
            " 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 278 279\n",
            " 280 281 282 283 284 285 286 288 289 290 291 292 293 294 295 296 297 298\n",
            " 299 300 301 303 304 306 307 310 311 312 313 314 315 316 317 318 319 320\n",
            " 321 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339\n",
            " 340 341 342 343 344 345 346 347 348 349 350 351 354 355 356 357 358 359\n",
            " 360 363 364 365 366 367 368 369 370 371 372 373 374 375 376 378 379 380\n",
            " 381 382 383 384 385 386 387 388 389 390 392 393 394 395 396 397 398] Val: [ 22  25  28  29  33  37  39  45  51  58  61  83  93 100 111 123 134 138\n",
            " 148 149 174 187 189 190 200 204 276 277 287 302 305 308 309 322 352 353\n",
            " 361 362 377 391]\n",
            "TRAIN: [  1   2   3   4   5   6   8  10  12  14  15  17  18  19  20  21  22  24\n",
            "  25  26  27  28  29  30  31  33  34  35  36  37  38  39  40  41  43  44\n",
            "  45  46  47  48  49  50  51  52  53  54  55  56  58  59  60  61  62  63\n",
            "  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78  80  81  82\n",
            "  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  99 100 101\n",
            " 102 103 104 105 106 107 108 109 110 111 112 113 114 115 118 119 120 121\n",
            " 122 123 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140\n",
            " 141 142 143 144 145 146 148 149 150 151 152 153 154 155 156 158 159 161\n",
            " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
            " 180 181 182 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198\n",
            " 199 200 201 202 204 205 206 208 211 212 213 214 215 216 217 218 219 220\n",
            " 221 222 223 224 225 226 227 228 229 230 231 232 234 236 237 239 240 241\n",
            " 242 244 245 246 247 248 249 250 251 252 254 255 256 257 258 259 260 261\n",
            " 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 280\n",
            " 281 282 283 284 285 287 288 289 290 291 292 293 294 295 296 297 298 300\n",
            " 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318\n",
            " 319 321 322 323 324 326 327 328 329 330 331 333 334 335 336 337 338 339\n",
            " 340 341 342 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358\n",
            " 360 361 362 363 364 365 366 368 369 370 371 372 373 374 375 376 377 378\n",
            " 379 380 381 382 383 384 385 386 387 389 390 391 392 393 394 397 398] Val: [  0   7   9  11  13  16  23  32  42  57  79  98 116 117 124 147 157 160\n",
            " 183 203 207 209 210 233 235 238 243 253 279 286 299 320 325 332 343 359\n",
            " 367 388 395 396]\n",
            "TRAIN: [  0   1   4   6   7   8   9  10  11  12  13  14  15  16  17  18  19  21\n",
            "  22  23  24  25  26  27  28  29  30  31  32  33  37  39  40  42  43  44\n",
            "  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60  61  62\n",
            "  63  64  65  66  67  68  69  70  72  73  74  77  78  79  80  81  82  83\n",
            "  85  86  87  88  89  91  92  93  94  95  96  97  98  99 100 101 102 103\n",
            " 104 106 107 108 109 110 111 112 113 115 116 117 118 119 120 121 122 123\n",
            " 124 125 126 127 128 129 130 132 133 134 135 136 137 138 139 141 143 144\n",
            " 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162\n",
            " 164 165 166 167 168 169 171 172 173 174 175 177 178 179 180 181 183 184\n",
            " 186 187 188 189 190 191 192 194 195 196 197 198 199 200 202 203 204 205\n",
            " 206 207 208 209 210 211 212 213 214 215 216 217 218 219 221 222 223 224\n",
            " 225 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243\n",
            " 244 245 247 248 249 250 251 252 253 254 255 257 259 260 261 262 263 264\n",
            " 265 266 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284\n",
            " 285 286 287 288 289 290 291 292 293 294 295 296 297 299 301 302 304 305\n",
            " 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323\n",
            " 325 326 327 328 329 330 331 332 333 334 336 337 338 339 340 341 342 343\n",
            " 344 345 346 347 348 350 351 352 353 354 355 356 357 358 359 360 361 362\n",
            " 363 364 365 366 367 368 369 370 371 372 373 375 376 377 378 379 380 381\n",
            " 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398] Val: [  2   3   5  20  34  35  36  38  41  71  75  76  84  90 105 114 131 140\n",
            " 142 163 170 176 182 185 193 201 220 226 246 256 258 267 268 298 300 303\n",
            " 324 335 349 374]\n",
            "TRAIN: [  0   1   2   3   4   5   7   8   9  11  13  14  15  16  18  19  20  21\n",
            "  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39\n",
            "  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  57  58\n",
            "  59  60  61  63  64  65  66  67  68  69  70  71  72  74  75  76  77  78\n",
            "  79  80  81  82  83  84  85  86  87  88  90  92  93  94  95  96  97  98\n",
            "  99 100 101 102 103 104 105 106 107 108 110 111 112 114 115 116 117 118\n",
            " 120 121 123 124 126 127 128 129 130 131 133 134 135 136 138 139 140 141\n",
            " 142 143 145 146 147 148 149 150 151 152 153 154 155 157 158 160 162 163\n",
            " 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181\n",
            " 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199\n",
            " 200 201 202 203 204 206 207 208 209 210 211 212 213 214 215 216 217 218\n",
            " 220 222 223 224 226 227 228 229 230 231 232 233 234 235 236 237 238 240\n",
            " 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258\n",
            " 259 260 261 262 264 265 266 267 268 269 270 271 272 273 274 275 276 277\n",
            " 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295\n",
            " 296 298 299 300 301 302 303 304 305 306 307 308 309 311 312 313 314 315\n",
            " 316 317 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335\n",
            " 336 337 338 339 340 341 343 344 345 346 347 348 349 350 351 352 353 355\n",
            " 357 358 359 360 361 362 363 364 366 367 369 372 373 374 375 376 377 379\n",
            " 380 382 383 384 385 386 387 388 390 391 392 393 394 395 396 397 398] Val: [  6  10  12  17  56  62  73  89  91 109 113 119 122 125 132 137 144 156\n",
            " 159 161 205 219 221 225 239 263 297 310 318 319 342 354 356 365 368 370\n",
            " 371 378 381 389]\n",
            "TRAIN: [  0   1   2   3   5   6   7   9  10  11  12  13  14  15  16  17  18  20\n",
            "  22  23  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40\n",
            "  41  42  43  44  45  46  47  48  49  51  52  53  54  56  57  58  59  61\n",
            "  62  64  65  66  68  69  71  72  73  74  75  76  77  78  79  80  81  82\n",
            "  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100\n",
            " 102 103 104 105 106 107 108 109 110 111 113 114 115 116 117 118 119 120\n",
            " 121 122 123 124 125 126 129 130 131 132 133 134 135 136 137 138 139 140\n",
            " 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158\n",
            " 159 160 161 162 163 165 166 167 168 170 171 172 173 174 175 176 177 178\n",
            " 179 180 181 182 183 185 186 187 188 189 190 191 192 193 194 195 196 197\n",
            " 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 214 215 216\n",
            " 217 218 219 220 221 222 223 224 225 226 227 228 230 232 233 234 235 237\n",
            " 238 239 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256\n",
            " 257 258 259 261 262 263 265 266 267 268 270 271 272 274 275 276 277 278\n",
            " 279 281 283 285 286 287 289 290 291 292 293 294 295 296 297 298 299 300\n",
            " 301 302 303 304 305 306 307 308 309 310 312 313 314 315 317 318 319 320\n",
            " 321 322 323 324 325 328 330 331 332 333 335 336 337 338 339 340 341 342\n",
            " 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360\n",
            " 361 362 363 364 365 366 367 368 369 370 371 372 373 374 376 377 378 379\n",
            " 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396] Val: [  4   8  19  21  24  50  55  60  63  67  70 101 112 127 128 164 169 184\n",
            " 213 229 231 236 240 260 264 269 273 280 282 284 288 311 316 326 327 329\n",
            " 334 375 397 398]\n",
            "TRAIN: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  16  17  18  19\n",
            "  20  21  22  23  24  25  26  27  28  29  31  32  33  34  35  36  37  38\n",
            "  39  40  41  42  43  44  45  46  47  48  50  51  52  53  54  55  56  57\n",
            "  58  59  60  61  62  63  66  67  69  70  71  72  73  74  75  76  77  78\n",
            "  79  80  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96\n",
            "  97  98  99 100 101 102 103 104 105 106 107 108 109 111 112 113 114 115\n",
            " 116 117 118 119 120 122 123 124 125 126 127 128 129 130 131 132 134 135\n",
            " 136 137 138 140 141 142 143 144 145 146 147 148 149 150 151 152 154 155\n",
            " 156 157 158 159 160 161 162 163 164 165 166 169 170 171 173 174 175 176\n",
            " 177 178 180 181 182 183 184 185 186 187 188 189 190 191 193 194 195 196\n",
            " 197 199 200 201 202 203 204 205 207 208 209 210 211 212 213 214 215 216\n",
            " 217 218 219 220 221 222 225 226 227 228 229 230 231 232 233 234 235 236\n",
            " 237 238 239 240 241 242 243 244 245 246 248 249 250 251 252 253 256 258\n",
            " 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 276 277 278\n",
            " 279 280 281 282 284 285 286 287 288 290 292 293 294 295 296 297 298 299\n",
            " 300 302 303 304 305 306 307 308 309 310 311 313 314 315 316 318 319 320\n",
            " 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338\n",
            " 339 340 341 342 343 344 345 346 347 349 350 351 352 353 354 355 356 357\n",
            " 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375\n",
            " 377 378 379 380 381 383 386 387 388 389 391 392 394 395 396 397 398] Val: [ 14  15  30  49  64  65  68 110 121 133 139 153 167 168 172 179 192 198\n",
            " 206 223 224 247 254 255 257 259 275 283 289 291 301 312 317 348 376 382\n",
            " 384 385 390 393]\n",
            "TRAIN: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
            "  18  19  20  21  22  23  24  25  26  28  29  30  31  32  33  34  35  36\n",
            "  37  38  39  40  41  42  43  45  46  47  48  49  50  51  53  54  55  56\n",
            "  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72  73  74\n",
            "  75  76  78  79  80  82  83  84  85  86  87  88  89  90  91  92  93  94\n",
            "  97  98  99 100 101 102 103 104 105 106 107 109 110 111 112 113 114 115\n",
            " 116 117 118 119 121 122 123 124 125 126 127 128 129 130 131 132 133 134\n",
            " 136 137 138 139 140 142 144 145 146 147 148 149 150 151 152 153 154 155\n",
            " 156 157 158 159 160 161 163 164 165 167 168 169 170 171 172 174 176 177\n",
            " 178 179 180 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196\n",
            " 197 198 200 201 202 203 204 205 206 207 208 209 210 212 213 214 218 219\n",
            " 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237\n",
            " 238 239 240 241 242 243 244 245 246 247 249 250 251 253 254 255 256 257\n",
            " 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275\n",
            " 276 277 278 279 280 281 282 283 284 285 286 287 288 289 291 293 294 296\n",
            " 297 298 299 300 301 302 303 304 305 307 308 309 310 311 312 314 316 317\n",
            " 318 319 320 321 322 323 324 325 326 327 329 330 331 332 334 335 338 339\n",
            " 341 342 343 344 345 346 348 349 350 352 353 354 355 356 358 359 360 361\n",
            " 362 363 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380\n",
            " 381 382 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398] Val: [ 27  44  52  77  81  95  96 108 120 135 141 143 162 166 173 175 181 199\n",
            " 211 215 216 217 248 252 290 292 295 306 313 315 328 333 336 337 340 347\n",
            " 351 357 364 383]\n",
            "TRAIN: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
            "  19  20  21  22  23  24  25  26  27  28  29  30  32  33  34  35  36  37\n",
            "  38  39  40  41  42  43  44  45  46  47  49  50  51  52  54  55  56  57\n",
            "  58  59  60  61  62  63  64  65  67  68  69  70  71  72  73  74  75  76\n",
            "  77  78  79  80  81  83  84  88  89  90  91  92  93  95  96  97  98  99\n",
            " 100 101 102 105 106 108 109 110 111 112 113 114 115 116 117 119 120 121\n",
            " 122 123 124 125 126 127 128 129 130 131 132 133 134 135 137 138 139 140\n",
            " 141 142 143 144 145 146 147 148 149 151 152 153 154 155 156 157 158 159\n",
            " 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177\n",
            " 178 179 180 181 182 183 184 185 186 187 189 190 192 193 194 198 199 200\n",
            " 201 202 203 204 205 206 207 208 209 210 211 213 215 216 217 218 219 220\n",
            " 221 222 223 224 225 226 227 228 229 230 231 233 234 235 236 237 238 239\n",
            " 240 241 243 245 246 247 248 249 251 252 253 254 255 256 257 258 259 260\n",
            " 261 263 264 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280\n",
            " 281 282 283 284 285 286 287 288 289 290 291 292 295 296 297 298 299 300\n",
            " 301 302 303 305 306 307 308 309 310 311 312 313 315 316 317 318 319 320\n",
            " 322 323 324 325 326 327 328 329 331 332 333 334 335 336 337 338 339 340\n",
            " 341 342 343 344 345 346 347 348 349 350 351 352 353 354 356 357 358 359\n",
            " 360 361 362 364 365 366 367 368 369 370 371 372 374 375 376 377 378 379\n",
            " 380 381 382 383 384 385 386 387 388 389 390 391 393 395 396 397 398] Val: [ 18  31  48  53  66  82  85  86  87  94 103 104 107 118 136 150 188 191\n",
            " 195 196 197 212 214 232 242 244 250 262 265 293 294 304 314 321 330 355\n",
            " 363 373 392 394]\n",
            "TRAIN: [  0   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18\n",
            "  19  20  21  22  23  24  25  27  28  29  30  31  32  33  34  35  36  37\n",
            "  38  39  41  42  44  45  46  47  48  49  50  51  52  53  55  56  57  58\n",
            "  59  60  61  62  63  64  65  66  67  68  69  70  71  73  74  75  76  77\n",
            "  79  80  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96\n",
            "  98  99 100 101 103 104 105 107 108 109 110 111 112 113 114 116 117 118\n",
            " 119 120 121 122 123 124 125 126 127 128 130 131 132 133 134 135 136 137\n",
            " 138 139 140 141 142 143 144 146 147 148 149 150 152 153 155 156 157 159\n",
            " 160 161 162 163 164 165 166 167 168 169 170 172 173 174 175 176 177 178\n",
            " 179 180 181 182 183 184 185 187 188 189 190 191 192 193 194 195 196 197\n",
            " 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215\n",
            " 216 217 218 219 220 221 223 224 225 226 229 231 232 233 234 235 236 238\n",
            " 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256\n",
            " 257 258 259 260 262 263 264 265 267 268 269 270 271 273 274 275 276 277\n",
            " 279 280 282 283 284 286 287 288 289 290 291 292 293 294 295 297 298 299\n",
            " 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317\n",
            " 318 319 320 321 322 324 325 326 327 328 329 330 331 332 333 334 335 336\n",
            " 337 340 342 343 346 347 348 349 351 352 353 354 355 356 357 358 359 361\n",
            " 362 363 364 365 367 368 369 370 371 372 373 374 375 376 377 378 379 381\n",
            " 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398] Val: [  1  26  40  43  54  72  78  97 102 106 115 129 145 151 154 158 171 186\n",
            " 222 227 228 230 237 261 266 272 278 281 285 296 323 338 339 341 344 345\n",
            " 350 360 366 380]\n",
            "TRAIN: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
            "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
            "  36  37  38  39  40  41  42  43  44  45  48  49  50  51  52  53  54  55\n",
            "  56  57  58  60  61  62  63  64  65  66  67  68  70  71  72  73  75  76\n",
            "  77  78  79  81  82  83  84  85  86  87  89  90  91  93  94  95  96  97\n",
            "  98 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116\n",
            " 117 118 119 120 121 122 123 124 125 127 128 129 131 132 133 134 135 136\n",
            " 137 138 139 140 141 142 143 144 145 147 148 149 150 151 153 154 156 157\n",
            " 158 159 160 161 162 163 164 166 167 168 169 170 171 172 173 174 175 176\n",
            " 179 181 182 183 184 185 186 187 188 189 190 191 192 193 195 196 197 198\n",
            " 199 200 201 203 204 205 206 207 209 210 211 212 213 214 215 216 217 219\n",
            " 220 221 222 223 224 225 226 227 228 229 230 231 232 233 235 236 237 238\n",
            " 239 240 242 243 244 246 247 248 250 252 253 254 255 256 257 258 259 260\n",
            " 261 262 263 264 265 266 267 268 269 272 273 275 276 277 278 279 280 281\n",
            " 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299\n",
            " 300 301 302 303 304 305 306 308 309 310 311 312 313 314 315 316 317 318\n",
            " 319 320 321 322 323 324 325 326 327 328 329 330 332 333 334 335 336 337\n",
            " 338 339 340 341 342 343 344 345 347 348 349 350 351 352 353 354 355 356\n",
            " 357 359 360 361 362 363 364 365 366 367 368 370 371 373 374 375 376 377\n",
            " 378 380 381 382 383 384 385 388 389 390 391 392 393 394 395 396 397 398] Val: [ 46  47  59  69  74  80  88  92  99 126 130 146 152 155 165 177 178 180\n",
            " 194 202 208 218 234 241 245 249 251 270 271 274 307 331 346 358 369 372\n",
            " 379 386 387]\n",
            "TRAIN: [  0   1   2   3   4   5   6   7   8   9  10  11  12  14  15  16  17  18\n",
            "  19  21  22  23  24  25  26  27  28  30  31  32  33  34  36  37  38  39\n",
            "  40  41  42  43  44  45  47  48  49  50  51  54  55  56  57  58  59  60\n",
            "  62  63  64  65  66  68  69  70  71  72  73  74  75  76  77  78  79  80\n",
            "  81  82  83  85  86  87  88  89  90  91  94  95  96  97  98  99 100 101\n",
            " 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 119 120\n",
            " 121 122 123 124 125 126 127 128 129 130 132 133 135 136 137 138 139 140\n",
            " 141 142 143 144 145 146 147 148 149 150 152 153 154 155 156 157 158 161\n",
            " 162 163 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180\n",
            " 181 182 183 184 185 186 187 189 190 191 192 193 194 196 197 199 200 201\n",
            " 202 203 204 205 208 209 210 211 212 213 214 215 216 217 218 219 220 221\n",
            " 222 223 224 225 227 228 229 230 231 232 233 234 235 237 238 239 240 241\n",
            " 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 258 260 261\n",
            " 262 263 264 265 266 268 269 270 271 272 273 274 275 276 277 278 279 280\n",
            " 281 282 283 284 285 286 287 288 290 291 292 293 294 295 296 297 298 299\n",
            " 300 302 303 304 305 306 307 308 309 310 311 312 313 314 316 317 318 320\n",
            " 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 338 339\n",
            " 340 341 342 344 345 346 347 348 349 351 352 353 355 356 357 358 359 360\n",
            " 361 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379\n",
            " 380 381 382 383 386 387 388 389 390 391 392 393 394 395 396 397 398] Val: [ 13  20  29  35  46  52  53  61  67  84  92  93 118 131 134 151 159 160\n",
            " 164 188 195 198 206 207 226 236 257 259 267 289 301 315 319 337 343 350\n",
            " 354 362 384 385]\n",
            "TRAIN: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  15  16  17  18\n",
            "  19  20  21  22  23  24  25  26  27  28  29  30  32  33  35  36  37  39\n",
            "  40  41  43  44  45  46  47  48  50  51  52  53  54  55  56  57  58  59\n",
            "  60  61  62  63  65  66  67  68  69  70  71  72  73  75  76  77  78  80\n",
            "  81  82  83  84  86  87  89  90  91  92  93  94  95  96  97 100 101 102\n",
            " 104 106 107 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
            " 126 127 128 129 131 132 133 134 135 136 137 138 139 140 141 142 143 144\n",
            " 145 146 147 148 149 150 151 152 153 156 157 158 159 160 161 162 163 164\n",
            " 165 166 167 168 170 171 172 174 175 176 177 178 179 180 181 182 183 185\n",
            " 186 187 188 189 191 192 193 194 195 196 197 198 199 200 201 202 203 204\n",
            " 205 206 207 208 209 210 211 212 213 214 215 216 218 219 220 221 223 224\n",
            " 226 227 228 229 230 231 233 234 235 236 237 238 239 240 241 242 243 244\n",
            " 245 247 248 249 250 251 252 253 254 255 257 258 259 260 261 262 264 265\n",
            " 266 267 269 271 272 273 274 275 276 277 278 279 280 281 282 284 285 286\n",
            " 287 288 289 290 292 293 295 296 297 298 299 300 301 302 303 304 305 306\n",
            " 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324\n",
            " 325 326 327 328 329 330 331 332 333 334 335 336 337 339 340 341 342 343\n",
            " 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361\n",
            " 362 363 364 365 366 367 368 369 370 371 374 375 376 377 378 379 380 381\n",
            " 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398] Val: [ 14  31  34  38  42  49  64  74  79  85  88  98  99 103 105 108 109 110\n",
            " 130 154 155 169 173 184 190 217 222 225 232 246 256 263 268 270 283 291\n",
            " 294 338 372 373]\n",
            "TRAIN: [  0   2   3   4   5   6   8   9  10  11  12  13  14  16  17  19  20  21\n",
            "  23  24  25  26  27  28  29  30  31  32  33  34  35  36  38  39  40  41\n",
            "  42  43  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60\n",
            "  61  62  63  64  65  66  67  68  69  70  71  72  74  75  77  78  79  80\n",
            "  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98\n",
            "  99 100 101 102 103 105 106 108 109 110 111 113 114 115 116 117 118 119\n",
            " 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 139\n",
            " 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157\n",
            " 158 159 160 161 162 163 164 165 166 167 169 170 171 172 173 174 175 177\n",
            " 179 180 182 183 184 185 188 189 190 191 192 193 194 195 196 197 198 199\n",
            " 200 201 202 203 204 206 207 208 209 210 212 214 215 216 217 218 219 220\n",
            " 221 222 223 225 226 227 228 229 230 231 232 233 234 235 236 238 239 240\n",
            " 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258\n",
            " 259 260 261 262 263 264 265 267 268 269 270 271 272 273 274 275 276 278\n",
            " 279 280 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297\n",
            " 298 299 300 301 302 303 305 306 307 308 309 310 312 314 315 316 317 318\n",
            " 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336\n",
            " 337 338 339 340 341 342 343 347 349 350 351 352 353 354 355 356 357 358\n",
            " 359 360 362 363 364 365 367 368 369 370 371 372 373 374 375 376 378 379\n",
            " 380 381 382 383 384 385 386 388 389 390 391 392 393 394 395 396 397] Val: [  1   7  15  18  22  37  44  73  76 104 107 112 120 138 168 176 178 181\n",
            " 186 187 205 211 213 224 237 266 277 281 304 311 313 344 345 346 348 361\n",
            " 366 377 387 398]\n",
            "TRAIN: [  0   1   2   3   4   5   6   7   8   9  11  13  14  15  16  17  18  19\n",
            "  20  21  22  23  24  25  26  27  29  30  31  32  34  35  36  37  38  40\n",
            "  41  42  44  45  46  47  48  49  50  51  52  53  54  55  56  58  59  60\n",
            "  61  62  63  64  65  66  67  68  70  71  72  73  74  75  76  77  78  79\n",
            "  80  81  82  83  84  85  87  88  89  90  91  92  93  94  95  97  98  99\n",
            " 100 101 102 103 104 105 106 107 108 109 110 111 112 113 115 117 118 120\n",
            " 121 122 123 124 125 126 127 128 129 130 131 132 134 135 136 137 138 140\n",
            " 141 142 143 144 145 146 147 149 150 151 154 155 157 158 159 160 161 162\n",
            " 163 164 165 166 167 168 169 170 171 172 173 174 176 177 178 179 181 182\n",
            " 183 184 186 187 188 189 190 191 192 193 194 195 196 197 198 199 201 202\n",
            " 203 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221\n",
            " 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239\n",
            " 240 241 242 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260\n",
            " 261 262 263 264 265 266 267 268 269 270 271 272 274 275 276 277 278 279\n",
            " 280 281 282 283 284 285 287 288 289 290 291 292 294 295 296 297 298 299\n",
            " 301 302 303 304 307 308 309 310 311 312 313 314 315 316 317 318 319 320\n",
            " 321 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339\n",
            " 341 342 343 344 345 346 347 348 349 350 352 353 354 355 356 357 359 360\n",
            " 361 362 363 364 365 366 367 368 369 370 372 373 375 376 377 378 379 380\n",
            " 381 382 383 384 385 386 387 388 390 391 392 393 394 395 396 397 398] Val: [ 10  12  28  33  39  43  57  69  86  96 114 116 119 133 139 148 152 153\n",
            " 156 175 180 185 200 204 243 244 245 273 286 293 300 305 306 322 340 351\n",
            " 358 371 374 389]\n",
            "TRAIN: [  0   1   2   3   4   5   6   7   9  10  11  12  13  14  15  16  17  18\n",
            "  19  20  21  22  23  24  25  26  28  29  30  31  33  34  35  36  37  38\n",
            "  39  41  42  43  44  45  46  47  48  49  50  51  52  53  55  56  57  58\n",
            "  59  60  61  62  63  64  65  67  69  71  72  73  74  75  76  77  78  79\n",
            "  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98\n",
            "  99 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117\n",
            " 118 119 120 121 123 124 125 126 127 128 129 130 131 132 133 134 135 136\n",
            " 137 138 139 140 142 143 144 145 147 148 149 150 151 152 153 154 155 156\n",
            " 157 158 159 160 161 162 164 166 168 169 170 171 172 173 174 175 176 177\n",
            " 178 180 181 182 183 184 185 186 187 188 189 190 191 193 195 196 197 198\n",
            " 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 215 216 217\n",
            " 218 219 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236\n",
            " 237 238 239 241 243 244 245 246 247 248 249 250 252 254 255 256 257 258\n",
            " 259 260 261 262 263 264 265 266 267 268 269 270 272 273 274 276 277 278\n",
            " 281 282 283 285 286 288 289 290 291 292 293 294 295 296 297 298 299 300\n",
            " 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 317 319 320\n",
            " 321 322 323 324 326 327 328 329 330 331 332 333 334 335 337 338 339 340\n",
            " 341 342 343 344 345 346 348 349 350 351 352 353 354 356 357 358 359 360\n",
            " 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378\n",
            " 379 381 382 383 384 385 387 388 389 390 391 392 393 394 396 397 398] Val: [  8  27  32  40  54  66  68  70  80 100 122 141 146 163 165 167 179 192\n",
            " 194 214 220 240 242 251 253 271 275 279 280 284 287 316 318 325 336 347\n",
            " 355 380 386 395]\n",
            "TRAIN: [  0   1   2   3   5   6   7   8   9  10  11  12  13  14  15  16  17  18\n",
            "  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  37\n",
            "  38  39  40  41  42  43  44  45  46  48  49  50  51  52  53  54  55  56\n",
            "  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72  73  74\n",
            "  76  77  79  80  81  83  84  85  86  88  89  90  91  92  93  94  95  96\n",
            "  97  98  99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114\n",
            " 115 116 118 119 120 121 122 123 124 127 129 130 131 132 133 134 135 136\n",
            " 138 139 141 142 144 146 147 148 149 150 151 152 153 154 155 156 158 159\n",
            " 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177\n",
            " 178 179 180 181 182 183 184 185 186 187 188 190 191 192 193 194 195 196\n",
            " 197 198 199 200 201 202 203 204 205 206 207 209 210 211 212 213 214 215\n",
            " 216 217 219 220 221 222 224 225 226 227 228 230 231 232 234 235 236 237\n",
            " 238 239 240 241 242 243 244 245 246 247 249 250 251 252 253 255 256 257\n",
            " 258 259 260 261 263 264 266 267 268 269 270 271 272 273 274 275 276 277\n",
            " 278 279 280 281 282 283 284 286 287 288 289 290 291 292 293 294 295 297\n",
            " 298 299 300 301 302 303 304 305 306 307 308 309 310 311 313 315 316 317\n",
            " 318 319 322 324 325 326 327 328 329 330 331 333 334 335 336 337 338 339\n",
            " 340 341 343 344 345 346 347 348 349 350 351 352 354 355 356 357 358 359\n",
            " 360 361 362 363 364 365 366 367 368 369 371 372 373 374 375 376 377 378\n",
            " 380 381 382 383 384 385 386 387 388 389 391 392 393 394 395 396 398] Val: [  4  36  47  75  78  82  87 117 125 126 128 137 140 143 145 157 189 208\n",
            " 218 223 229 233 248 254 262 265 285 296 312 314 320 321 323 332 342 353\n",
            " 370 379 390 397]\n",
            "TRAIN: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  18\n",
            "  19  20  22  23  24  25  26  27  28  29  31  32  33  34  35  36  37  38\n",
            "  39  40  42  43  44  45  46  47  49  50  52  53  54  55  57  60  61  62\n",
            "  63  64  65  66  67  68  69  70  71  72  73  74  75  76  78  79  80  81\n",
            "  82  83  84  85  86  87  88  89  91  92  93  94  95  96  97  98  99 100\n",
            " 101 102 103 104 105 107 108 109 110 111 112 114 116 117 118 119 120 121\n",
            " 122 123 124 125 126 128 129 130 131 132 133 134 135 136 137 138 139 140\n",
            " 141 142 143 145 146 148 149 151 152 153 154 155 156 157 158 159 160 161\n",
            " 162 163 164 165 167 168 169 170 172 173 174 175 176 177 178 179 180 181\n",
            " 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 200\n",
            " 201 202 203 204 205 206 207 208 210 211 212 213 214 215 216 217 218 219\n",
            " 220 222 223 224 225 226 228 229 231 232 233 235 236 237 238 239 240 241\n",
            " 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259\n",
            " 260 262 263 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279\n",
            " 280 281 282 283 284 285 286 287 289 290 291 292 293 294 295 296 297 298\n",
            " 299 300 301 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318\n",
            " 319 320 321 322 323 324 325 326 327 328 330 331 332 334 335 336 337 338\n",
            " 339 340 342 343 344 345 346 347 348 349 350 351 352 353 354 355 357 358\n",
            " 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 377 378\n",
            " 379 380 383 384 385 386 387 388 389 390 391 392 393 395 396 397 398] Val: [ 17  21  30  41  48  51  56  58  59  77  90 106 113 115 127 144 147 150\n",
            " 166 171 199 209 221 227 230 234 261 264 288 302 303 329 333 341 356 359\n",
            " 376 381 382 394]\n",
            "TRAIN: [  1   2   4   5   7   8   9  10  11  12  13  14  15  17  18  20  21  22\n",
            "  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40\n",
            "  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58\n",
            "  59  60  61  62  63  64  65  66  67  68  69  70  71  73  74  75  76  77\n",
            "  78  79  80  81  82  83  84  85  86  87  88  89  90  91  92  93  94  96\n",
            "  98  99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115\n",
            " 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133\n",
            " 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 150 151 152\n",
            " 153 154 155 156 157 159 160 163 164 165 166 167 168 169 171 172 173 174\n",
            " 175 176 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 194\n",
            " 195 196 198 199 200 201 202 204 205 206 207 208 209 211 212 213 214 215\n",
            " 216 217 218 220 221 222 223 224 225 226 227 229 230 231 232 233 234 235\n",
            " 236 237 238 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254\n",
            " 256 257 258 259 261 262 263 264 265 266 267 268 269 270 271 272 273 275\n",
            " 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293\n",
            " 294 295 296 298 299 300 301 302 303 304 305 306 308 311 312 313 314 315\n",
            " 316 318 319 320 321 322 323 325 326 327 329 330 331 332 333 336 337 338\n",
            " 339 340 341 342 343 344 345 346 347 348 349 350 351 353 354 355 356 358\n",
            " 359 360 361 362 364 365 366 367 369 370 371 372 373 374 376 377 379 380\n",
            " 381 382 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398] Val: [  0   3   6  16  19  72  95  97 149 158 161 162 170 177 193 197 203 210\n",
            " 219 228 239 255 260 274 297 307 309 310 317 324 328 334 335 352 357 363\n",
            " 368 375 378 383]\n",
            "TRAIN: [  0   1   2   3   4   5   6   7   8  10  11  12  13  14  15  16  17  18\n",
            "  19  20  21  22  24  25  27  28  29  30  31  32  33  34  35  36  37  38\n",
            "  39  40  41  42  43  44  46  47  48  49  51  52  53  54  55  56  57  58\n",
            "  59  60  61  64  66  67  68  69  70  71  72  73  74  75  76  77  78  79\n",
            "  80  81  82  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98\n",
            "  99 100 101 102 103 104 105 106 107 108 109 110 112 113 114 115 116 117\n",
            " 118 119 120 122 123 125 126 127 128 129 130 131 132 133 134 137 138 139\n",
            " 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157\n",
            " 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 175 176\n",
            " 177 178 179 180 181 182 184 185 186 187 188 189 190 191 192 193 194 195\n",
            " 197 198 199 200 202 203 204 205 206 207 208 209 210 211 213 214 215 217\n",
            " 218 219 220 221 222 223 224 225 226 227 228 229 230 232 233 234 235 236\n",
            " 237 238 239 240 242 243 244 245 246 248 250 251 253 254 255 256 257 259\n",
            " 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 277 278\n",
            " 279 280 281 282 283 284 285 286 287 288 289 290 291 293 294 296 297 298\n",
            " 300 301 302 303 304 305 306 307 309 310 311 312 313 314 315 316 317 318\n",
            " 319 320 321 322 323 324 325 328 329 332 333 334 335 336 337 338 339 340\n",
            " 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358\n",
            " 359 361 362 363 366 368 369 370 371 372 373 374 375 376 377 378 379 380\n",
            " 381 382 383 384 385 386 387 388 389 390 391 392 394 395 396 397 398] Val: [  9  23  26  45  50  62  63  65  83 111 121 124 135 136 174 183 196 201\n",
            " 212 216 231 241 247 249 252 258 276 292 295 299 308 326 327 330 331 360\n",
            " 364 365 367 393]\n",
            "TRAIN: [  0   1   3   4   6   7   8   9  10  12  13  14  15  16  17  18  19  20\n",
            "  21  22  23  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40\n",
            "  41  42  43  44  45  46  47  48  49  50  51  52  53  54  56  57  58  59\n",
            "  61  62  63  64  65  66  67  68  69  70  72  73  74  75  76  77  78  79\n",
            "  80  82  83  84  85  86  87  88  90  92  93  95  96  97  98  99 100 103\n",
            " 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121\n",
            " 122 124 125 126 127 128 130 131 133 134 135 136 137 138 139 140 141 143\n",
            " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
            " 162 163 164 165 166 167 168 169 170 171 173 174 175 176 177 178 179 180\n",
            " 181 183 184 185 186 187 188 189 190 192 193 194 195 196 197 198 199 200\n",
            " 201 203 204 205 206 207 208 209 210 211 212 213 214 216 217 218 219 220\n",
            " 221 222 223 224 225 226 227 228 229 230 231 232 233 234 236 237 239 240\n",
            " 241 242 243 244 245 246 247 248 249 251 252 253 254 255 256 257 258 259\n",
            " 260 261 262 263 264 265 266 267 268 270 271 273 274 275 276 277 279 280\n",
            " 281 283 284 285 286 287 288 289 291 292 293 294 295 296 297 299 300 301\n",
            " 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319\n",
            " 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337\n",
            " 338 340 341 342 343 344 345 346 347 348 350 351 352 353 354 355 356 357\n",
            " 358 359 360 361 362 363 364 365 366 367 368 370 371 372 373 374 375 376\n",
            " 377 378 379 380 381 382 383 384 385 386 387 389 390 393 394 395 397 398] Val: [  2   5  11  24  25  55  60  71  81  89  91  94 101 102 123 129 132 142\n",
            " 172 182 191 202 215 235 238 250 269 272 278 282 290 298 339 349 369 388\n",
            " 391 392 396]\n",
            "TRAIN: [  0   1   2   3   4   5   6   9  10  11  12  13  14  17  18  19  20  21\n",
            "  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40\n",
            "  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58\n",
            "  59  60  61  62  63  64  65  66  67  68  69  70  71  72  73  74  76  77\n",
            "  78  79  80  81  82  83  84  85  86  87  88  89  90  92  94  95  96  97\n",
            "  98  99 100 101 102 103 104 105 106 107 109 110 111 112 113 114 115 116\n",
            " 117 118 119 121 122 123 124 125 126 127 128 129 130 132 133 134 135 136\n",
            " 137 138 139 140 141 142 143 145 148 149 150 151 152 153 154 155 157 158\n",
            " 159 160 161 162 163 164 165 167 168 169 170 171 172 173 174 175 177 178\n",
            " 179 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197\n",
            " 198 199 200 201 202 203 205 206 207 209 210 211 212 213 214 215 216 217\n",
            " 218 219 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236\n",
            " 237 238 239 240 242 243 244 245 246 247 248 249 251 252 253 254 255 256\n",
            " 257 258 259 260 261 262 263 265 266 267 268 269 270 271 272 273 275 276\n",
            " 277 278 279 280 282 283 284 285 286 287 288 289 290 291 292 293 296 297\n",
            " 298 300 301 302 304 305 306 308 309 310 311 312 313 314 315 316 317 318\n",
            " 319 320 321 322 323 324 325 326 328 329 330 331 332 333 334 335 336 337\n",
            " 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355\n",
            " 356 357 358 361 362 363 365 366 367 368 369 370 372 373 374 376 378 379\n",
            " 380 381 382 383 384 385 387 388 389 390 391 392 393 394 395 396 398] Val: [  7   8  15  16  22  75  91  93 108 120 131 144 146 147 156 166 176 180\n",
            " 204 208 220 241 250 264 274 281 294 295 299 303 307 327 359 360 364 371\n",
            " 375 377 386 397]\n",
            "TRAIN: [  0   1   4   5   6   7   8   9  10  11  12  13  14  15  16  18  19  20\n",
            "  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  37  38  39\n",
            "  40  41  42  43  44  46  48  49  51  52  53  54  55  56  57  58  59  60\n",
            "  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78\n",
            "  79  80  82  83  84  85  86  87  88  89  90  91  92  93  95  96  97  99\n",
            " 100 101 102 103 104 105 106 107 108 109 110 111 114 115 117 118 119 120\n",
            " 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138\n",
            " 139 140 141 142 143 144 145 146 147 148 149 150 151 155 156 157 158 159\n",
            " 160 161 162 163 164 166 167 168 169 171 172 173 174 175 176 177 178 179\n",
            " 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 197 198\n",
            " 199 200 202 203 204 205 206 207 208 211 212 214 217 218 219 220 221 222\n",
            " 223 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241\n",
            " 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 259 260\n",
            " 261 263 264 266 267 268 270 271 272 273 274 275 276 277 279 280 281 282\n",
            " 284 286 287 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303\n",
            " 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 321 322\n",
            " 324 325 326 327 328 330 331 333 334 335 336 337 338 339 340 341 342 343\n",
            " 344 345 346 347 348 349 350 351 352 353 354 355 357 358 359 360 361 362\n",
            " 363 364 365 366 368 369 370 371 372 373 374 375 376 377 378 379 380 381\n",
            " 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398] Val: [  2   3  17  36  45  47  50  81  94  98 112 113 116 152 153 154 165 170\n",
            " 196 201 209 210 213 215 216 224 258 262 265 269 278 283 285 288 320 323\n",
            " 329 332 356 367]\n",
            "TRAIN: [  0   2   3   4   6   7   8   9  10  11  12  13  14  15  16  17  18  19\n",
            "  20  21  22  23  24  25  27  28  29  30  31  32  33  35  36  37  38  39\n",
            "  40  41  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58\n",
            "  59  60  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76\n",
            "  77  78  79  80  81  82  83  84  85  86  87  89  91  93  94  95  96  98\n",
            "  99 100 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117\n",
            " 118 119 120 121 123 124 125 126 127 128 129 130 131 132 133 134 135 136\n",
            " 137 138 139 140 141 142 143 144 146 147 150 152 153 154 155 156 157 158\n",
            " 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176\n",
            " 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 194 195\n",
            " 196 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216\n",
            " 217 218 219 220 221 222 223 224 225 226 227 228 229 231 233 234 235 236\n",
            " 237 238 239 240 241 242 243 244 245 246 247 249 250 252 253 255 256 257\n",
            " 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275\n",
            " 276 278 279 280 281 282 283 285 286 287 288 289 290 291 292 293 294 295\n",
            " 297 298 299 300 301 302 303 304 305 306 307 308 309 311 313 314 315 318\n",
            " 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336\n",
            " 337 339 340 341 342 343 346 347 350 351 352 355 356 357 358 359 360 362\n",
            " 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380\n",
            " 381 382 383 384 385 386 388 389 390 391 392 393 394 395 396 397 398] Val: [  1   5  26  34  42  88  90  92  97 101 122 145 148 149 151 193 197 198\n",
            " 199 230 232 248 251 254 277 284 296 310 312 316 317 338 344 345 348 349\n",
            " 353 354 361 387]\n",
            "TRAIN: [  0   1   2   3   4   5   6   7   8   9  12  13  14  15  16  17  18  19\n",
            "  20  21  22  24  25  26  27  28  30  31  32  33  34  35  36  37  38  39\n",
            "  41  42  43  44  45  46  47  48  50  51  52  53  55  56  57  58  59  60\n",
            "  61  62  63  64  65  66  67  69  70  71  72  74  75  76  77  78  79  80\n",
            "  81  83  84  85  86  87  88  89  90  91  92  93  94  95  97  98  99 100\n",
            " 101 103 104 105 106 107 108 109 110 111 112 113 114 115 116 118 119 120\n",
            " 121 122 123 124 125 127 128 129 130 131 132 133 134 138 139 140 141 142\n",
            " 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 160 161\n",
            " 163 164 165 166 168 169 170 171 172 173 174 175 176 177 178 179 180 181\n",
            " 182 183 184 185 187 188 189 190 191 192 193 195 196 197 198 199 200 201\n",
            " 202 203 204 206 207 208 209 210 211 212 213 215 216 217 218 219 220 221\n",
            " 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239\n",
            " 240 241 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258\n",
            " 260 262 263 264 265 266 267 269 270 271 272 274 275 276 277 278 279 280\n",
            " 281 282 283 284 285 287 288 289 290 291 292 294 295 296 297 299 301 302\n",
            " 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320\n",
            " 321 322 323 324 325 326 327 328 329 331 332 333 334 335 336 337 338 340\n",
            " 341 342 343 344 345 346 347 348 349 350 352 353 354 355 356 357 358 359\n",
            " 360 361 362 363 364 365 366 367 368 369 371 373 374 375 376 377 378 379\n",
            " 380 381 382 384 385 386 387 388 389 390 391 392 393 394 396 397 398] Val: [ 10  11  23  29  40  49  54  68  73  82  96 102 117 126 135 136 137 159\n",
            " 162 167 186 194 205 214 242 259 261 268 273 286 293 298 300 330 339 351\n",
            " 370 372 383 395]\n",
            "TRAIN: [  0   1   2   3   5   6   7   8   9  10  11  12  14  15  16  17  18  19\n",
            "  21  22  23  24  26  27  28  29  30  32  33  34  36  37  38  39  40  41\n",
            "  42  43  44  45  46  47  48  49  50  51  52  53  54  55  57  58  59  60\n",
            "  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  77  78  79\n",
            "  80  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97\n",
            "  98  99 100 101 102 103 104 106 107 108 109 110 111 112 113 114 115 116\n",
            " 117 118 119 120 121 122 123 126 127 129 131 132 134 135 136 137 138 139\n",
            " 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159\n",
            " 160 161 162 163 164 165 166 167 169 170 171 172 174 175 176 177 178 179\n",
            " 180 181 182 183 185 186 187 188 189 191 192 193 194 196 197 198 199 200\n",
            " 201 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219\n",
            " 220 221 222 223 224 225 227 229 230 231 232 233 234 235 236 238 239 240\n",
            " 241 242 243 244 246 247 248 249 250 251 252 253 254 255 256 257 258 259\n",
            " 260 261 262 263 264 265 266 267 268 269 271 273 274 275 276 277 278 279\n",
            " 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297\n",
            " 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315\n",
            " 316 317 318 319 320 322 323 324 325 326 327 328 329 330 331 332 333 334\n",
            " 335 336 337 338 339 340 342 343 344 345 346 348 349 350 351 352 353 354\n",
            " 355 356 359 360 361 363 364 365 367 369 370 371 372 373 375 376 377 378\n",
            " 379 380 381 382 383 385 386 387 388 389 390 392 393 395 396 397 398] Val: [  4  13  20  25  31  35  56  76 105 124 125 128 130 133 140 141 168 173\n",
            " 184 190 195 202 226 228 237 245 270 272 321 341 347 357 358 362 366 368\n",
            " 374 384 391 394]\n",
            "TRAIN: [  0   1   2   3   4   5   6   7   8  10  11  12  13  14  15  16  17  18\n",
            "  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36\n",
            "  37  38  39  40  41  42  43  45  46  47  48  49  50  51  52  53  54  55\n",
            "  56  57  58  59  60  61  63  65  66  67  68  69  70  71  73  74  75  76\n",
            "  77  78  80  81  82  83  84  85  87  88  90  91  92  93  94  95  96  97\n",
            "  98 101 102 105 107 108 109 110 111 112 113 114 115 116 117 118 119 120\n",
            " 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138\n",
            " 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 156 158\n",
            " 159 160 161 162 165 166 167 168 169 170 171 173 174 175 176 177 178 179\n",
            " 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197\n",
            " 198 199 200 201 202 203 204 205 206 207 208 209 210 212 213 214 215 216\n",
            " 217 219 220 222 223 224 226 228 229 230 231 232 233 234 235 237 238 239\n",
            " 240 241 242 243 244 245 246 248 250 251 252 253 254 256 257 258 259 260\n",
            " 261 262 264 265 267 268 269 270 272 273 274 275 276 277 278 279 281 282\n",
            " 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300\n",
            " 301 302 303 304 305 306 307 309 310 311 312 313 314 315 316 317 318 320\n",
            " 321 322 323 324 325 327 329 330 331 332 333 334 337 338 339 340 341 342\n",
            " 343 344 345 346 347 348 349 351 352 353 354 355 356 357 358 359 360 361\n",
            " 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379\n",
            " 380 382 383 384 385 386 387 388 389 390 391 393 394 395 396 397 398] Val: [  9  44  62  64  72  79  86  89  99 100 103 104 106 155 157 163 164 172\n",
            " 211 218 221 225 227 236 247 249 255 263 266 271 280 308 319 326 328 335\n",
            " 336 350 381 392]\n",
            "TRAIN: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
            "  18  20  21  22  23  24  25  26  27  28  29  30  31  32  34  35  36  37\n",
            "  38  40  41  42  43  44  45  46  47  48  49  50  51  52  54  55  56  57\n",
            "  58  62  64  65  66  68  71  72  73  74  75  76  77  79  80  81  82  84\n",
            "  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 101 102\n",
            " 103 104 105 106 108 109 110 112 113 114 115 116 117 119 120 121 122 123\n",
            " 124 125 126 128 129 130 131 132 133 135 136 137 139 140 141 142 143 144\n",
            " 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 161 162 163\n",
            " 164 165 166 167 168 169 170 171 172 173 174 176 177 180 182 183 184 186\n",
            " 189 190 191 192 193 194 195 196 197 198 199 201 202 203 204 205 206 207\n",
            " 208 209 210 211 212 213 214 215 216 217 218 220 221 222 223 224 225 226\n",
            " 227 228 229 230 231 232 233 234 236 237 238 240 241 242 243 244 245 246\n",
            " 247 248 249 250 251 252 253 254 255 256 257 258 259 261 262 263 264 265\n",
            " 266 267 268 269 270 271 272 273 274 275 276 277 278 280 281 282 283 284\n",
            " 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 303 304\n",
            " 305 306 307 308 309 310 311 312 313 314 316 317 318 319 320 321 322 323\n",
            " 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341\n",
            " 342 343 344 345 346 347 348 349 350 351 352 353 354 356 357 358 359 360\n",
            " 361 362 364 365 366 367 368 369 370 371 372 373 374 375 377 378 379 380\n",
            " 381 382 383 384 385 386 387 388 390 391 392 393 394 395 396 397 398] Val: [ 19  33  39  53  59  60  61  63  67  69  70  78  83 107 111 118 127 134\n",
            " 138 160 175 178 179 181 185 187 188 200 219 235 239 260 279 301 302 315\n",
            " 355 363 376 389]\n",
            "TRAIN: [  0   1   2   3   4   5   7   8   9  10  11  12  13  14  15  16  17  18\n",
            "  19  20  22  23  24  25  26  27  28  29  30  31  33  34  35  36  37  39\n",
            "  40  42  43  44  45  46  47  48  49  50  51  53  54  55  56  58  59  60\n",
            "  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78\n",
            "  79  80  81  82  83  86  88  89  90  91  92  93  94  96  97  98  99 100\n",
            " 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118\n",
            " 120 121 122 123 124 125 126 127 128 129 130 131 133 134 135 136 137 138\n",
            " 139 140 141 142 143 144 145 146 147 148 149 151 152 153 154 155 156 157\n",
            " 158 159 160 161 162 163 164 165 166 167 168 170 172 173 174 175 176 177\n",
            " 178 179 180 181 182 184 185 186 187 188 190 193 194 195 196 197 198 199\n",
            " 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217\n",
            " 218 219 220 221 222 223 224 225 226 227 228 230 231 232 233 234 235 236\n",
            " 237 239 241 242 243 244 245 247 248 249 250 251 253 254 255 257 258 259\n",
            " 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 277 278\n",
            " 279 280 281 282 283 284 285 286 287 288 291 292 293 294 295 296 297 298\n",
            " 299 300 301 302 303 304 305 307 308 309 310 311 312 315 316 317 318 319\n",
            " 320 321 323 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339\n",
            " 341 342 343 344 345 347 348 349 350 351 353 354 355 356 357 358 359 360\n",
            " 361 362 363 364 365 366 367 368 369 370 371 372 374 375 376 377 380 381\n",
            " 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398] Val: [  6  21  32  38  41  52  57  84  85  87  95 119 132 150 169 171 183 189\n",
            " 191 192 229 238 240 246 252 256 276 289 290 306 313 314 322 324 340 346\n",
            " 352 373 378 379]\n",
            "TRAIN: [  0   1   2   3   4   5   6   7   8   9  10  11  13  15  16  17  19  20\n",
            "  21  22  23  24  25  26  28  29  31  32  33  34  35  36  37  38  39  40\n",
            "  41  42  44  45  46  47  48  49  50  52  53  54  55  56  57  58  59  60\n",
            "  61  62  63  64  65  66  67  68  69  70  71  72  73  75  76  78  79  81\n",
            "  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99\n",
            " 100 101 102 103 104 105 106 107 108 109 111 112 113 115 116 117 118 119\n",
            " 120 121 122 123 124 125 126 127 128 130 131 132 133 134 135 136 137 138\n",
            " 139 140 141 142 144 145 146 147 148 149 150 151 152 153 154 155 156 157\n",
            " 159 160 162 163 164 165 166 167 168 169 170 171 172 173 175 176 177 178\n",
            " 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196\n",
            " 197 198 199 200 201 202 203 204 205 206 208 209 210 211 212 213 214 215\n",
            " 216 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 235\n",
            " 236 237 238 239 240 241 242 245 246 247 248 249 250 251 252 254 255 256\n",
            " 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274\n",
            " 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 293 294 295\n",
            " 296 297 298 299 300 301 302 303 305 306 307 308 310 312 313 314 315 316\n",
            " 317 319 320 321 322 323 324 325 326 327 328 329 330 332 334 335 336 338\n",
            " 339 340 341 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357\n",
            " 358 359 360 361 362 363 364 366 367 368 370 371 372 373 374 375 376 377\n",
            " 378 379 381 383 384 386 387 389 390 391 392 393 394 395 396 397 398] Val: [ 12  14  18  27  30  43  51  74  77  80 110 114 129 143 158 161 174 207\n",
            " 217 234 243 244 253 275 291 292 304 309 311 318 331 333 337 342 365 369\n",
            " 380 382 385 388]\n",
            "TRAIN: [  1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18\n",
            "  19  20  21  22  23  25  26  27  29  30  31  32  33  34  35  36  38  39\n",
            "  40  41  42  43  44  45  47  49  50  51  52  53  54  56  57  59  60  61\n",
            "  62  63  64  67  68  69  70  72  73  74  75  76  77  78  79  80  81  82\n",
            "  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100\n",
            " 101 102 103 104 105 106 107 108 110 111 112 113 114 116 117 118 119 120\n",
            " 122 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 140 141\n",
            " 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160\n",
            " 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 178 179\n",
            " 180 181 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198\n",
            " 199 200 201 202 204 205 207 208 209 210 211 213 214 215 216 217 218 219\n",
            " 220 221 224 225 226 227 228 229 230 232 234 235 236 237 238 239 240 241\n",
            " 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 258 259 260\n",
            " 261 262 263 264 265 266 268 269 270 271 272 273 274 275 276 277 278 279\n",
            " 280 281 283 284 285 286 288 289 290 291 292 293 294 295 296 298 299 300\n",
            " 301 302 303 304 306 307 308 309 310 311 312 313 314 315 316 317 318 319\n",
            " 320 321 322 323 324 326 327 328 329 330 331 332 333 335 336 337 338 339\n",
            " 340 341 342 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358\n",
            " 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376\n",
            " 377 378 379 380 381 382 383 384 385 386 387 388 389 391 392 394 395 397] Val: [  0  24  28  37  46  48  55  58  65  66  71 109 115 121 123 139 142 177\n",
            " 182 203 206 212 222 223 231 233 257 267 282 287 297 305 325 334 343 390\n",
            " 393 396 398]\n",
            "TRAIN: [  0   1   2   3   4   6   7   8   9  10  11  12  13  14  15  16  17  18\n",
            "  19  20  22  23  24  25  26  27  28  29  31  32  33  34  36  37  38  39\n",
            "  40  41  42  43  44  45  46  49  50  51  52  53  54  55  56  57  58  59\n",
            "  60  61  62  63  64  65  66  67  68  70  71  72  73  74  75  76  77  78\n",
            "  79  80  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96\n",
            "  98  99 100 101 102 104 105 106 107 108 111 112 113 114 115 116 117 118\n",
            " 119 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137\n",
            " 138 139 140 141 144 147 149 150 151 152 153 154 155 158 159 160 161 162\n",
            " 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 179 180 181\n",
            " 182 183 184 185 186 187 188 189 190 191 192 193 196 197 199 200 201 202\n",
            " 203 204 205 206 207 208 209 210 211 212 213 215 216 217 218 219 220 221\n",
            " 222 223 225 226 227 228 229 230 231 232 234 235 236 237 238 240 241 242\n",
            " 243 244 245 247 248 250 251 252 253 256 257 258 259 260 261 262 263 265\n",
            " 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283\n",
            " 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301\n",
            " 302 303 304 305 306 307 309 310 311 313 314 316 317 318 319 320 321 322\n",
            " 323 324 325 326 327 328 329 330 331 332 333 335 336 337 338 339 340 341\n",
            " 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359\n",
            " 360 361 362 363 364 365 366 367 368 371 372 373 374 376 377 378 379 380\n",
            " 381 382 383 384 385 386 388 389 390 391 392 393 394 395 396 397 398] Val: [  5  21  30  35  47  48  69  97 103 109 110 120 142 143 145 146 148 156\n",
            " 157 178 194 195 198 214 224 233 239 246 249 254 255 264 308 312 315 334\n",
            " 369 370 375 387]\n",
            "TRAIN: [  0   1   3   4   5   6   7   8  10  11  12  13  14  16  17  18  19  20\n",
            "  21  22  24  25  26  28  29  30  31  32  33  34  35  36  37  38  39  40\n",
            "  41  42  43  44  45  46  47  48  50  51  52  53  54  55  56  57  58  59\n",
            "  60  61  62  63  65  66  68  69  70  71  72  73  74  75  77  78  79  80\n",
            "  81  83  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100\n",
            " 101 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119\n",
            " 120 121 122 123 124 125 126 127 128 129 130 132 133 135 136 137 138 139\n",
            " 141 142 143 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159\n",
            " 161 162 163 164 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
            " 180 181 183 184 185 186 187 188 190 191 192 193 194 195 196 197 198 199\n",
            " 200 201 202 203 204 205 206 207 208 210 211 212 214 215 216 217 218 220\n",
            " 221 222 223 224 225 228 229 230 231 232 233 234 235 237 238 239 240 241\n",
            " 242 243 244 245 246 247 248 249 250 252 253 254 255 256 257 258 259 261\n",
            " 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279\n",
            " 280 281 282 283 284 285 286 287 288 289 290 291 292 294 295 296 297 299\n",
            " 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 318\n",
            " 319 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337\n",
            " 338 340 341 342 343 345 346 347 348 349 350 351 353 354 355 356 357 358\n",
            " 359 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377\n",
            " 378 381 382 383 384 386 387 388 390 391 392 393 394 395 396 397 398] Val: [  2   9  15  23  27  49  64  67  76  82  84 102 131 134 140 144 160 165\n",
            " 182 189 209 213 219 226 227 236 251 260 293 298 317 320 339 344 352 360\n",
            " 379 380 385 389]\n",
            "TRAIN: [  0   1   2   3   4   5   6   7   8   9  10  11  12  14  15  16  17  18\n",
            "  19  20  21  22  23  24  25  26  27  29  30  31  32  33  34  35  36  37\n",
            "  38  39  40  41  42  43  44  45  47  48  49  51  52  53  54  55  56  57\n",
            "  58  61  62  63  64  66  67  68  69  70  71  72  73  75  76  77  78  79\n",
            "  80  81  82  84  85  86  87  88  89  90  91  92  93  94  95  97  98  99\n",
            " 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117\n",
            " 118 120 122 123 124 126 127 129 131 132 134 136 137 138 139 140 141 142\n",
            " 143 144 145 146 147 148 149 150 151 152 154 155 156 157 158 159 160 161\n",
            " 162 163 164 165 166 167 168 169 170 171 173 174 175 177 178 179 181 182\n",
            " 183 184 185 186 187 188 189 190 192 193 194 195 196 197 198 199 200 201\n",
            " 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219\n",
            " 222 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240\n",
            " 241 242 243 245 246 247 248 249 250 251 253 254 255 256 257 258 259 260\n",
            " 261 263 264 265 266 267 268 269 270 271 272 273 274 276 277 278 279 280\n",
            " 281 282 283 284 285 286 287 288 289 290 291 293 294 295 296 297 298 299\n",
            " 300 301 302 303 304 305 306 308 309 310 311 312 313 314 315 317 318 319\n",
            " 320 321 322 323 324 325 326 327 328 331 332 333 334 335 336 337 338 339\n",
            " 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357\n",
            " 358 359 360 362 363 364 365 366 367 369 370 371 372 373 374 375 377 378\n",
            " 379 380 381 382 384 385 386 387 388 389 390 392 393 395 396 397 398] Val: [ 13  28  46  50  59  60  65  74  83  96 119 121 125 128 130 133 135 153\n",
            " 172 176 180 191 220 221 223 244 252 262 275 292 307 316 329 330 361 368\n",
            " 376 383 391 394]\n",
            "TRAIN: [  1   2   3   4   5   6   7   8   9  11  12  13  14  15  16  17  19  20\n",
            "  21  22  23  24  25  27  28  29  30  31  32  33  34  35  36  37  38  41\n",
            "  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59\n",
            "  60  61  62  63  64  65  66  67  69  70  71  72  73  74  75  76  78  79\n",
            "  80  81  82  83  84  85  86  87  88  89  90  91  92  94  95  96  97  98\n",
            "  99 100 101 102 103 105 106 107 108 109 110 111 113 114 115 116 117 118\n",
            " 119 120 121 122 123 124 125 126 128 129 130 131 133 134 135 137 138 139\n",
            " 140 141 142 143 144 145 146 148 151 152 153 154 155 156 157 158 159 160\n",
            " 161 162 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
            " 180 181 182 183 184 185 187 188 189 190 191 192 193 194 195 196 197 198\n",
            " 200 202 204 205 208 209 210 211 212 213 214 215 216 217 218 219 220 221\n",
            " 223 224 225 226 227 228 229 230 231 233 236 237 238 239 240 241 242 243\n",
            " 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261\n",
            " 262 263 264 265 266 267 268 269 272 273 274 275 276 277 278 279 280 281\n",
            " 282 284 285 286 287 288 289 290 291 292 293 294 295 297 298 299 300 301\n",
            " 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319\n",
            " 320 322 323 324 325 326 327 328 329 330 333 334 336 337 338 339 340 341\n",
            " 342 343 344 345 346 347 348 349 352 353 354 355 356 357 358 359 360 361\n",
            " 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380\n",
            " 381 382 383 384 385 386 387 388 389 391 392 393 394 395 396 397 398] Val: [  0  10  18  26  39  40  68  77  93 104 112 127 132 136 147 149 150 163\n",
            " 186 199 201 203 206 207 222 232 234 235 270 271 283 296 321 331 332 335\n",
            " 350 351 362 390]\n",
            "TRAIN: [  0   1   2   3   4   5   7   8   9  10  11  12  13  14  15  16  17  18\n",
            "  19  20  21  22  23  25  26  27  28  30  31  32  33  34  35  36  37  38\n",
            "  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  55  57  58\n",
            "  59  60  61  62  63  64  65  67  68  69  70  71  72  73  74  75  76  77\n",
            "  78  79  82  83  84  87  88  89  91  92  93  94  95  96  97  98  99 100\n",
            " 101 102 103 104 105 106 107 108 109 110 111 112 114 115 116 118 119 120\n",
            " 121 122 123 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139\n",
            " 140 141 142 143 144 145 146 147 148 149 150 151 153 154 155 156 157 158\n",
            " 159 160 161 163 164 165 166 167 168 170 172 173 174 175 176 177 178 179\n",
            " 180 182 183 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199\n",
            " 200 201 202 203 204 205 206 207 209 210 211 212 213 214 215 216 217 218\n",
            " 219 220 221 222 223 224 226 227 228 229 230 232 233 234 235 236 237 238\n",
            " 239 240 241 243 244 246 247 248 249 251 252 253 254 255 256 257 258 259\n",
            " 260 261 262 263 264 266 267 268 269 270 271 272 273 274 275 276 277 278\n",
            " 279 280 281 282 283 284 286 288 289 291 292 293 295 296 297 298 299 300\n",
            " 301 302 303 304 305 306 307 308 309 310 311 312 313 315 316 317 319 320\n",
            " 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 339\n",
            " 340 341 342 343 344 345 346 347 349 350 351 352 353 354 356 357 359 360\n",
            " 361 362 363 364 366 367 368 369 370 371 372 373 374 375 376 377 378 379\n",
            " 380 381 382 383 384 385 386 387 388 389 390 391 393 394 395 396 397] Val: [  6  24  29  54  56  66  80  81  85  86  90 113 117 124 152 162 169 171\n",
            " 181 184 208 225 231 242 245 250 265 285 287 290 294 314 318 338 348 355\n",
            " 358 365 392 398]\n",
            "TRAIN: [  0   2   3   4   5   6   7   9  10  13  14  15  16  17  18  19  21  22\n",
            "  23  24  25  26  27  28  29  30  31  33  34  35  37  38  39  40  41  42\n",
            "  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60  61\n",
            "  62  64  65  66  67  68  69  70  71  72  74  75  76  77  78  79  80  81\n",
            "  82  83  84  85  86  87  88  89  90  91  92  93  94  96  97  98  99 100\n",
            " 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118\n",
            " 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136\n",
            " 137 138 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 156\n",
            " 157 158 159 160 161 162 163 164 165 168 169 170 171 172 173 174 175 176\n",
            " 177 178 180 181 182 183 184 185 186 187 188 189 191 192 193 194 195 198\n",
            " 199 200 201 202 203 205 206 207 208 209 210 211 213 214 215 217 218 219\n",
            " 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237\n",
            " 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 254 255 256\n",
            " 257 258 259 260 262 263 264 265 268 269 270 271 272 273 275 276 277 278\n",
            " 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296\n",
            " 297 298 300 303 304 305 306 307 308 310 312 313 314 315 316 317 318 319\n",
            " 320 321 323 324 325 327 328 329 330 331 332 333 334 335 336 337 338 339\n",
            " 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357\n",
            " 358 359 360 361 362 364 365 367 368 369 370 372 373 375 376 377 379 380\n",
            " 381 382 383 385 386 387 388 389 390 391 392 393 394 395 396 397 398] Val: [  1   8  11  12  20  32  36  43  63  73  95 139 155 166 167 179 190 196\n",
            " 197 204 212 216 253 261 266 267 274 299 301 302 309 311 322 326 363 366\n",
            " 371 374 378 384]\n",
            "TRAIN: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  15  16  17  18\n",
            "  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37\n",
            "  38  39  40  43  44  46  47  48  49  50  52  54  56  57  58  59  60  61\n",
            "  62  63  64  65  66  67  68  69  70  71  73  74  75  76  77  79  80  81\n",
            "  82  83  84  85  86  87  88  89  90  91  92  93  95  96  97  99 102 103\n",
            " 104 106 107 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123\n",
            " 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141\n",
            " 142 143 144 145 146 147 148 149 150 152 153 155 156 157 158 159 160 161\n",
            " 162 163 165 166 167 168 169 171 172 173 174 175 176 178 179 180 181 182\n",
            " 183 184 185 186 188 189 190 191 192 194 195 196 197 198 199 200 201 202\n",
            " 203 204 205 206 207 208 209 211 212 213 214 215 216 217 218 219 220 221\n",
            " 222 223 224 225 226 227 229 231 232 233 234 235 236 237 238 239 240 241\n",
            " 242 243 244 245 246 248 249 250 251 252 253 254 255 256 257 259 260 261\n",
            " 262 264 265 266 267 268 270 271 272 273 274 275 276 278 279 280 281 282\n",
            " 283 284 285 286 287 288 290 291 292 293 294 295 296 297 298 299 300 301\n",
            " 302 303 304 305 307 308 309 310 311 312 313 314 315 316 317 318 319 320\n",
            " 321 322 323 324 325 326 327 329 330 331 332 333 334 335 336 337 338 339\n",
            " 340 343 344 345 346 347 348 349 350 351 352 354 355 356 358 359 360 361\n",
            " 362 363 364 365 366 368 369 370 371 372 373 374 375 376 377 378 379 380\n",
            " 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 397 398] Val: [ 14  19  41  42  45  51  53  55  72  78  94  98 100 101 105 108 151 154\n",
            " 164 170 177 187 193 210 228 230 247 258 263 269 277 289 306 328 341 342\n",
            " 353 357 367 396]\n",
            "TRAIN: [  0   1   2   5   6   8   9  10  11  12  13  14  15  16  18  19  20  21\n",
            "  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  39  40\n",
            "  41  42  43  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59\n",
            "  60  61  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78\n",
            "  80  81  82  83  84  85  86  87  88  90  91  92  93  94  95  96  97  98\n",
            "  99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 117 119 120\n",
            " 121 122 123 124 125 126 127 128 130 131 132 133 134 135 136 137 139 140\n",
            " 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 159 160\n",
            " 161 162 163 164 165 166 167 168 169 170 171 172 173 174 176 177 178 179\n",
            " 180 181 182 183 184 185 186 187 188 189 190 191 193 194 195 196 197 198\n",
            " 199 201 202 203 204 205 206 207 208 209 210 212 213 214 216 217 218 219\n",
            " 220 221 222 223 224 225 226 227 228 230 231 232 233 234 235 236 237 239\n",
            " 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257\n",
            " 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 274 275 276\n",
            " 277 278 279 280 281 282 283 284 285 286 287 289 290 291 292 293 294 295\n",
            " 296 297 298 299 300 301 302 306 307 308 309 310 311 312 314 315 316 317\n",
            " 318 320 321 322 323 324 325 326 327 328 329 330 331 332 334 335 337 338\n",
            " 339 340 341 342 343 344 348 349 350 351 352 353 354 355 357 358 360 361\n",
            " 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379\n",
            " 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 396 398] Val: [  3   4   7  17  38  44  62  79  89 114 115 116 118 129 138 141 158 175\n",
            " 192 200 211 215 229 238 273 288 303 304 305 313 319 333 336 345 346 347\n",
            " 356 359 395 397]\n",
            "TRAIN: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  17  18\n",
            "  19  20  21  23  24  26  27  28  29  30  31  32  33  34  35  36  38  39\n",
            "  40  41  42  43  44  45  46  47  48  49  50  51  53  54  55  56  57  59\n",
            "  60  61  62  63  64  65  66  67  68  69  70  72  73  74  75  76  77  78\n",
            "  79  80  81  82  83  84  85  86  89  90  92  93  94  95  96  97  98  99\n",
            " 100 101 102 103 104 105 106 108 109 110 112 113 114 115 116 117 118 119\n",
            " 120 121 122 124 125 126 127 128 129 130 131 132 133 134 135 136 138 139\n",
            " 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157\n",
            " 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 175 176\n",
            " 177 178 179 180 181 182 184 186 187 189 190 191 192 193 194 195 196 197\n",
            " 198 199 200 201 203 204 206 207 208 209 210 211 212 213 214 215 216 219\n",
            " 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 238\n",
            " 239 242 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 260\n",
            " 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 277 279 280\n",
            " 281 282 283 285 287 288 289 290 291 292 293 294 295 296 297 298 299 301\n",
            " 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319\n",
            " 320 321 322 323 325 326 327 328 329 330 331 332 333 334 335 336 337 338\n",
            " 339 340 341 342 343 344 345 346 347 348 350 351 352 353 355 356 357 358\n",
            " 359 360 361 362 363 364 365 366 367 368 369 370 371 374 375 376 378 379\n",
            " 380 382 383 384 385 386 387 389 390 391 392 393 394 395 396 397 398] Val: [ 16  22  25  37  52  58  71  87  88  91 107 111 123 137 174 183 185 188\n",
            " 202 205 217 218 237 240 241 243 259 276 278 284 286 300 324 349 354 372\n",
            " 373 377 381 388]\n",
            "TRAIN: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
            "  18  19  20  21  22  23  24  25  26  27  28  29  30  32  35  36  37  38\n",
            "  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56\n",
            "  58  59  60  62  63  64  65  66  67  68  69  71  72  73  74  76  77  78\n",
            "  79  80  81  82  83  84  85  86  87  88  89  90  91  93  94  95  96  97\n",
            "  98 100 101 102 103 104 105 107 108 109 110 111 112 113 114 115 116 117\n",
            " 118 119 120 121 123 124 125 127 128 129 130 131 132 133 134 135 136 137\n",
            " 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155\n",
            " 156 157 158 160 162 163 164 165 166 167 169 170 171 172 174 175 176 177\n",
            " 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195\n",
            " 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213\n",
            " 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231\n",
            " 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 249 250\n",
            " 251 252 253 254 255 258 259 260 261 262 263 264 265 266 267 269 270 271\n",
            " 273 274 275 276 277 278 283 284 285 286 287 288 289 290 292 293 294 296\n",
            " 298 299 300 301 302 303 304 305 306 307 308 309 311 312 313 314 315 316\n",
            " 317 318 319 320 321 322 324 326 328 329 330 331 332 333 334 335 336 338\n",
            " 339 341 342 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358\n",
            " 359 360 361 362 363 365 366 367 368 369 370 371 372 373 374 375 376 377\n",
            " 378 379 380 381 383 384 385 387 388 389 390 391 392 394 395 396 397 398] Val: [ 31  33  34  57  61  70  75  92  99 106 122 126 159 161 168 173 248 256\n",
            " 257 268 272 279 280 281 282 291 295 297 310 323 325 327 337 340 343 364\n",
            " 382 386 393]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:23: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:24: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:25: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:26: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trials=Trials()\n",
        "#trials = pickle.load(open(path+site+'0929_suffle_onlystep4_KfoldCV+accuracy+weightx_t+0_nomalx_ECx- 2019x.p', \"rb\"))    #trials불러오기\n",
        "starttime = time.time()\n",
        "#if optype == \"tpe\":\n",
        "best = fmin(XGB,space_xgb,algo=tpe.suggest,max_evals=100,trials=trials)\n",
        "optime = time.time() - starttime\n",
        "#else:\n",
        "#  best = fmin(XGB,space_xgb,algo=rand.suggest,max_evals=10,trials=trials,rstate= np.random.RandomState(seed_value))\n",
        "\n",
        "pickle.dump(trials, open(path+\"xgb_%s.p\" %(time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(starttime))), \"wb\"))     #trials저장하기\n",
        "#print(best)\n",
        "#print(space_eval(space_xgb, best))\n",
        "\n",
        "###테스트셋에 테스트\n",
        "###테스트셋에 테스트\n",
        "learn=best['learn']\n",
        "nesti=int(best['nesti'])\n",
        "maxd=int(best['maxd'])\n",
        "minc=int(best['minc'])\n",
        "gamm=best['gamm']\n",
        "subsa=best['subsa']\n",
        "colsample_b=best['colsample_b']\n",
        "learn=best['learn']\n",
        "# scalepos=best['scalepos']\n",
        "\n",
        "#\n",
        "'''\n",
        "learn=0.0107605633620193\n",
        "nesti=int(100)\n",
        "maxd=int(5)\n",
        "minc=int(2)\n",
        "gamm=22.5170863426934\n",
        "subsa=0.315853538049625\n",
        "colsample_b=0.801462768751276\n",
        "scalepos=int(9)\n",
        "'''\n",
        "######최적화모델성능\n",
        "xgb=XGBClassifier(\n",
        "        learning_rate =learn,\n",
        "        n_estimators=nesti,\n",
        "        max_depth=maxd,\n",
        "        min_child_weight=minc,\n",
        "        gamma=gamm,\n",
        "        subsample=subsa,\n",
        "        colsample_bytree=colsample_b,\n",
        "        objective= 'binary:logistic',\n",
        "        nthread=-1,\n",
        "        # scale_pos_weight=scalepos,\n",
        "        n_jobs = -1,\n",
        "        seed=100\n",
        "        )\n",
        "\n",
        "\n",
        "xgb.fit(x_prime_train, y_train)#,eval_set=eval_set)\n",
        "#최종점수#\n",
        "y_pred=xgb.predict(x_prime_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "id": "9I_qjKSbZuqT",
        "outputId": "3ab9e95a-cfe1-42c3-ed45-22bb01c3577f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  1%|          | 1/100 [33:30<55:16:38, 2010.09s/it, best loss: -0.42400759036847463]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-164-a7db86b6ff62>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mstarttime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#if optype == \"tpe\":\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mbest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXGB\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mspace_xgb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0malgo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtpe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuggest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_evals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0moptime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstarttime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#else:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mfmin\u001b[0;34m(fn, space, algo, max_evals, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar)\u001b[0m\n\u001b[1;32m    386\u001b[0m             \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m             \u001b[0mreturn_argmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_argmin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 388\u001b[0;31m             \u001b[0mshow_progressbar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshow_progressbar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    389\u001b[0m         )\n\u001b[1;32m    390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/hyperopt/base.py\u001b[0m in \u001b[0;36mfmin\u001b[0;34m(self, fn, space, algo, max_evals, rstate, verbose, pass_expr_memo_ctrl, catch_eval_exceptions, return_argmin, show_progressbar)\u001b[0m\n\u001b[1;32m    637\u001b[0m             \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m             \u001b[0mreturn_argmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_argmin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 639\u001b[0;31m             show_progressbar=show_progressbar)\n\u001b[0m\u001b[1;32m    640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mfmin\u001b[0;34m(fn, space, algo, max_evals, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar)\u001b[0m\n\u001b[1;32m    405\u001b[0m                     show_progressbar=show_progressbar)\n\u001b[1;32m    406\u001b[0m     \u001b[0mrval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m     \u001b[0mrval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexhaust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreturn_argmin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtrials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mexhaust\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    260\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexhaust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0mn_done\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_evals\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mn_done\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock_until_done\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masynchronous\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, N, block_until_done)\u001b[0m\n\u001b[1;32m    225\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m                         \u001b[0;31m# -- loop over trials and do the jobs directly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserial_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m                     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mserial_evaluate\u001b[0;34m(self, N)\u001b[0m\n\u001b[1;32m    139\u001b[0m                 \u001b[0mctrl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCtrl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_trial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdomain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctrl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m                     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'job exception: %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/hyperopt/base.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, config, ctrl, attach_attachments)\u001b[0m\n\u001b[1;32m    842\u001b[0m                 \u001b[0mmemo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmemo\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m                 print_node_on_error=self.rec_eval_print_node_on_error)\n\u001b[0;32m--> 844\u001b[0;31m             \u001b[0mrval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyll_rval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    845\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    846\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-158-ce214e789419>\u001b[0m in \u001b[0;36mXGB\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrain_set_X\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrain_set_y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVal_set_X\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVal_set_y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mxgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#validation_data=(x_val, y_val))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0my_pred\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0my_pred\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, callbacks)\u001b[0m\n\u001b[1;32m    730\u001b[0m                               \u001b[0mevals_result\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m                               \u001b[0mverbose_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxgb_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxgb_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 732\u001b[0;31m                               callbacks=callbacks)\n\u001b[0m\u001b[1;32m    733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobjective\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb_options\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"objective\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, learning_rates)\u001b[0m\n\u001b[1;32m    214\u001b[0m                            \u001b[0mevals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                            \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m                            xgb_model=xgb_model, callbacks=callbacks)\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/xgboost/training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;31m# Skip the first update if it is a recovery step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_rabit_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mversion\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   1107\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1108\u001b[0m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle, ctypes.c_int(iteration),\n\u001b[0;32m-> 1109\u001b[0;31m                                                     dtrain.handle))\n\u001b[0m\u001b[1;32m   1110\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "answer = np.array([label_description[label_decoder[int(val)]] for val in y_test])\n",
        "predss = np.array([label_description[label_decoder[int(val)]] for val in y_pred])\n",
        "\n",
        "new_crosstab = pd.crosstab(answer, predss, rownames=['answer'], colnames=['preds'])\n",
        "new_crosstab"
      ],
      "metadata": {
        "id": "zhE8BxFYfaXh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "# Print the confusion matrix\n",
        "print(metrics.confusion_matrix(y_test, y_pred))\n",
        "\n",
        "# Print the precision and recall, among other metrics\n",
        "print(metrics.classification_report(y_test, y_pred, digits=3))"
      ],
      "metadata": {
        "id": "R2xL46F4faXh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **RESNET50으로 싹다 전처리 후 XGB로 마무리**"
      ],
      "metadata": {
        "id": "34ZIbSRdpwb2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications.resnet import ResNet50\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ],
      "metadata": {
        "id": "97F8CgvTpwb5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30fc31d7-d220-4ff8-b0fe-33b334b73a71",
        "id": "FRMGHcf8pwb5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 498/498 [01:20<00:00,  6.22it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'내부 습도 1 최고': [36.5, 100.0],\n",
              " '내부 습도 1 최저': [32.4, 100.0],\n",
              " '내부 습도 1 평균': [34.1, 100.0],\n",
              " '내부 온도 1 최고': [14.5, 47.6],\n",
              " '내부 온도 1 최저': [14.4, 47.0],\n",
              " '내부 온도 1 평균': [14.4, 47.3],\n",
              " '내부 이슬점 최고': [12.8, 31.9],\n",
              " '내부 이슬점 최저': [12.1, 29.1],\n",
              " '내부 이슬점 평균': [12.4, 29.9]}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "# 분석에 사용할 feature 선택\n",
        "csv_features = ['내부 온도 1 평균', '내부 온도 1 최고', '내부 온도 1 최저', '내부 습도 1 평균', '내부 습도 1 최고', \n",
        "                '내부 습도 1 최저', '내부 이슬점 평균', '내부 이슬점 최고', '내부 이슬점 최저']\n",
        "\n",
        "# csv_files = sorted(glob('sample_data/*/*.csv'))\n",
        "allfile = glob(path + '/sample_data/sample_data/*/*.csv')\n",
        "csv_files = sorted(allfile)\n",
        "\n",
        "temp_csv = pd.read_csv(csv_files[0])[csv_features]\n",
        "max_arr, min_arr = temp_csv.max().to_numpy(), temp_csv.min().to_numpy()\n",
        "\n",
        "# feature 별 최대값, 최솟값 계산\n",
        "for csv in tqdm(csv_files[1:]):\n",
        "    temp_csv = pd.read_csv(csv)[csv_features]\n",
        "    temp_max, temp_min = temp_csv.max().to_numpy(), temp_csv.min().to_numpy()\n",
        "    max_arr = np.max([max_arr,temp_max], axis=0)\n",
        "    min_arr = np.min([min_arr,temp_min], axis=0)\n",
        "\n",
        "# feature 별 최대값, 최솟값 dictionary 생성\n",
        "csv_feature_dict = {csv_features[i]:[min_arr[i], max_arr[i]] for i in range(len(csv_features))}\n",
        "csv_feature_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J-zji6ZKpwb6"
      },
      "outputs": [],
      "source": [
        "# 제공된 sample data는 파프리카와 시설포도 2종류의 작물만 존재\n",
        "label_description = {\n",
        " '3_00_0': '파프리카_정상',\n",
        " '3_a9_1': '파프리카흰가루병_초기',\n",
        " '3_a9_2': '파프리카흰가루병_중기',\n",
        " '3_a9_3': '파프리카흰가루병_말기',\n",
        " '3_a10_1': '파프리카잘록병_초기',\n",
        " '3_a10_2': '파프리카잘록병_중기',\n",
        " '3_a10_3': '파프리카잘록병_말기',\n",
        " '3_b3_1': '칼슘결핍_초기',\n",
        " '3_b3_2': '칼슘결핍_중기',\n",
        " '3_b3_3': '칼슘결핍_말기',\n",
        " '3_b6_1': '다량원소결핍 (N)_초기',\n",
        " '3_b6_2': '다량원소결핍 (N)_중기',\n",
        " '3_b6_3': '다량원소결핍 (N)_말기',\n",
        " '3_b7_1': '다량원소결핍 (P)_초기',\n",
        " '3_b7_2': '다량원소결핍 (P)_중기',\n",
        " '3_b7_3': '다량원소결핍 (P)_말기',\n",
        " '3_b8_1': '다량원소결핍 (K)_초기',\n",
        " '3_b8_2': '다량원소결핍 (K)_중기',\n",
        " '3_b8_3': '다량원소결핍 (K)_말기',\n",
        " '6_00_0': '시설포도_정상',\n",
        " '6_a11_1': '시설포도탄저병_초기',\n",
        " '6_a11_2': '시설포도탄저병_중기',\n",
        " '6_a11_3': '시설포도탄저병_말기',\n",
        " '6_a12_1': '시설포도노균병_초기',\n",
        " '6_a12_2': '시설포도노균병_중기',\n",
        " '6_a12_3': '시설포도노균병_말기',\n",
        " '6_b4_1': '일소피해_초기',\n",
        " '6_b4_2': '일소피해_중기',\n",
        " '6_b4_3': '일소피해_말기',\n",
        " '6_b5_1': '축과병_초기',\n",
        " '6_b5_2': '축과병_중기',\n",
        " '6_b5_3': '축과병_말기',\n",
        "}\n",
        "\n",
        "label_encoder = {key:idx for idx, key in enumerate(label_description)}\n",
        "label_decoder = {val:key for key, val in label_encoder.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cf7kuOZwpwb6"
      },
      "outputs": [],
      "source": [
        "class DataController():\n",
        "    def __init__(self,csvfeatures,csvfeaturedict):\n",
        "        self.csv_features = csvfeatures\n",
        "        self.csv_feature_dict = csvfeaturedict\n",
        "    \n",
        "    def road_csv(self,foldnam,timenum):\n",
        "        df = pd.read_csv(foldnam)\n",
        "        return df.loc[:timenum-1,self.csv_features]      #csv파일 제일 짧은게 291개임 오류인지는 모르겠으나 일단 최소길이로 통일하여 처리\n",
        "    \n",
        "    def scaling(self,minmaxdic,df):\n",
        "        for col in minmaxdic.keys():\n",
        "            df.loc[:,col] = df.loc[:,col] - csv_feature_dict[col][0]\n",
        "            df.loc[:,col] = df.loc[:,col] / (csv_feature_dict[col][1]-csv_feature_dict[col][0])\n",
        "        return df\n",
        "    \n",
        "    \n",
        "    def getimage(self,imgpath):\n",
        "        img = cv2.imread(imgpath)\n",
        "        img = cv2.resize(img, dsize=(224, 224), interpolation=cv2.INTER_AREA)\n",
        "        img = img.astype(np.float32)/255  ##픽셀값을 0~1사이로 정규화\n",
        "        # img = np.transpose(img, (2,0,1))\n",
        "        return img\n",
        "    \n",
        "    def getlable(self,jsonpath):\n",
        "        with open(jsonpath, 'r') as f:\n",
        "            json_file = json.load(f)\n",
        "\n",
        "        crop = json_file['annotations']['crop']\n",
        "        disease = json_file['annotations']['disease']\n",
        "        risk = json_file['annotations']['risk']\n",
        "        label = f'{crop}_{disease}_{risk}'\n",
        "        return label\n",
        "    \n",
        "    def getdata(self,datapath,timenum,featnum):\n",
        "\n",
        "        csvarr = np.empty((0,timenum,featnum),float32)\n",
        "        imgarr = np.empty((0,224,224,3), float)\n",
        "        lablearr = np.array([])\n",
        "        \n",
        "        # predictor = np.append(predictor,ndf.reshape(-1,timenum,featnum),axis = 0)\n",
        "        \n",
        "        for ind,i in enumerate(datapath):\n",
        "            \n",
        "            if glob(i + '/*.csv') == []:  #10462폴더 비어있음. 다음에 확인해보기\n",
        "                pass\n",
        "            else:\n",
        "                csvpath = glob(i + '/*.csv')[0]\n",
        "                imgpath = glob(i + '/*.jpg')[0]\n",
        "                jsonpath = glob(i + '/*.json')[0]\n",
        "                # con = DataController()\n",
        "                df = self.road_csv(csvpath,timenum)\n",
        "                df2 = self.scaling(csv_feature_dict,df).to_numpy().reshape(-1,timenum,featnum)\n",
        "                imgdata = self.getimage(imgpath).reshape(-1,224,224,3)\n",
        "                label = label_encoder[self.getlable(jsonpath)]\n",
        "                # label = self.getlable(jsonpath)\n",
        "                \n",
        "                csvarr = np.append(csvarr,df2, axis = 0)\n",
        "                imgarr = np.append(imgarr,imgdata, axis = 0)\n",
        "                lablearr = np.append(lablearr,label)\n",
        "            \n",
        "        return [csvarr,imgarr],lablearr\n",
        "            \n",
        "        \n",
        "        \n",
        "        \n",
        "          "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mpN3MemEpwb7"
      },
      "outputs": [],
      "source": [
        "#프로젝트에 있는 모든 데이터폴터 불러오기\n",
        "data_files = glob(path + '/sample_data/sample_data/*')\n",
        "#셔플\n",
        "random.shuffle(data_files)\n",
        "#앞에서 300번째까지 트레인셋으로\n",
        "trainfiles = data_files[:400]\n",
        "#나머지는 테스트셋으로\n",
        "testfiles = data_files[400:]\n",
        "\n",
        "# 데이터 컨트롤러\n",
        "dacon = DataController(csv_features,csv_feature_dict)\n",
        "# 배치화된 데이터셋 만들기\n",
        "# test셋용으로는 train = False로 하여 배치안생성하게됨\n",
        "x_train,y_train = dacon.getdata(trainfiles,260,9)\n",
        "x_test,y_test = dacon.getdata(testfiles,260,9)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train[0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ehmsYjEzpwb7",
        "outputId": "d06c8dec-1606-431f-de15-63d179a1b759"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(399, 260, 9)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from tensorflow.keras.callbacks import Callback\n",
        "\n",
        "class LRFinder(Callback):\n",
        "    \"\"\"`Callback` that exponentially adjusts the learning rate after each training batch between `start_lr` and\n",
        "    `end_lr` for a maximum number of batches: `max_step`. The loss and learning rate are recorded at each step allowing\n",
        "    visually finding a good learning rate as per https://sgugger.github.io/how-do-you-find-a-good-learning-rate.html via\n",
        "    the `plot` method.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, start_lr: float = 1e-7, end_lr: float = 10, max_steps: int = 100, smoothing=0.9):\n",
        "        super(LRFinder, self).__init__()\n",
        "        self.start_lr, self.end_lr = start_lr, end_lr\n",
        "        self.max_steps = max_steps\n",
        "        self.smoothing = smoothing\n",
        "        self.step, self.best_loss, self.avg_loss, self.lr = 0, 0, 0, 0\n",
        "        self.lrs, self.losses = [], []\n",
        "\n",
        "    def on_train_begin(self, logs=None):\n",
        "        self.step, self.best_loss, self.avg_loss, self.lr = 0, 0, 0, 0\n",
        "        self.lrs, self.losses = [], []\n",
        "\n",
        "    def on_train_batch_begin(self, batch, logs=None):\n",
        "        self.lr = self.exp_annealing(self.step)\n",
        "        tf.keras.backend.set_value(self.model.optimizer.lr, self.lr)\n",
        "\n",
        "    def on_train_batch_end(self, batch, logs=None):\n",
        "        logs = logs or {}\n",
        "        loss = logs.get('loss')\n",
        "        step = self.step\n",
        "        if loss:\n",
        "            self.avg_loss = self.smoothing * self.avg_loss + (1 - self.smoothing) * loss\n",
        "            smooth_loss = self.avg_loss / (1 - self.smoothing ** (self.step + 1))\n",
        "            self.losses.append(smooth_loss)\n",
        "            self.lrs.append(self.lr)\n",
        "\n",
        "            if step == 0 or loss < self.best_loss:\n",
        "                self.best_loss = loss\n",
        "\n",
        "            if smooth_loss > 4 * self.best_loss or tf.math.is_nan(smooth_loss):\n",
        "                self.model.stop_training = True\n",
        "\n",
        "        if step == self.max_steps:\n",
        "            self.model.stop_training = True\n",
        "\n",
        "        self.step += 1\n",
        "\n",
        "    def exp_annealing(self, step):\n",
        "        return self.start_lr * (self.end_lr / self.start_lr) ** (step * 1. / self.max_steps)\n",
        "\n",
        "    def plot(self):\n",
        "        fig, ax = plt.subplots(1, 1)\n",
        "        ax.set_ylabel('Loss')\n",
        "        ax.set_xlabel('Learning Rate (log scale)')\n",
        "        ax.set_xscale('log')\n",
        "        ax.xaxis.set_major_formatter(plt.FormatStrFormatter('%.0e'))\n",
        "        ax.plot(self.lrs, self.losses)"
      ],
      "metadata": {
        "id": "2U9W-bx24R9S"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_resnet_model(categories=32):\n",
        "  def residual_block(X, kernels, stride):\n",
        "    out = keras.layers.Conv1D(kernels, stride, padding='same')(X)\n",
        "    out = keras.layers.ReLU()(out)\n",
        "    out = keras.layers.Conv1D(kernels, stride, padding='same')(out)\n",
        "    out = keras.layers.add([X, out])\n",
        "    out = keras.layers.ReLU()(out)\n",
        "    out = keras.layers.MaxPool1D(5, 2)(out)\n",
        "    return out\n",
        "\n",
        "  kernels = 32\n",
        "  stride = 5\n",
        "\n",
        "  inputs = keras.layers.Input([187,1])\n",
        "  X = keras.layers.Conv1D(kernels, stride)(inputs)\n",
        "  X = residual_block(X, kernels, stride)\n",
        "  X = residual_block(X, kernels, stride)\n",
        "  X = residual_block(X, kernels, stride)\n",
        "  X = residual_block(X, kernels, stride)\n",
        "  X = residual_block(X, kernels, stride)\n",
        "  X = keras.layers.Flatten()(X)\n",
        "  X = keras.layers.Dense(32, activation='relu')(X)\n",
        "  X = keras.layers.Dense(32, activation='relu')(X)\n",
        "  output = (keras.layers.Dense(1, activation='sigmoid')(X) if categories == 2 else keras.layers.Dense(5, activation='softmax')(X))\n",
        "\n",
        "  model = keras.Model(inputs=inputs, outputs=output)\n",
        "  return model"
      ],
      "metadata": {
        "id": "5HiCBp1830ML"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRMYUhWVaioD"
      },
      "source": [
        "optimizer = keras.optimizers.Adam(lr=0.001)\n",
        "model = get_resnet_model(32) \n",
        "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "# x_train,y_train\n",
        "lr_finder = LRFinder(start_lr=1e-7, end_lr= 1e-06, max_steps=50, smoothing=0.6)\n",
        "_ = model.fit(x_train[0][0], y_train[:260], batch_size=256, epochs=5, callbacks=[lr_finder], verbose=False)\n",
        "lr_finder.plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "sjVA_Ogr4z8R"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2i7Rr7NVai1m"
      },
      "source": [
        "lr_schedule = CyclicalLearningRate(1e-5, 1e-3, step_size=step_size, scale_fn=lambda x: tf.pow(0.95,x))\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
        "m_resnet_model = get_resnet_model(5)\n",
        "\n",
        "save_best_weights = ModelCheckpoint(filepath=\"weights.hdf5\", verbose=0, save_best_only=True)\n",
        "\n",
        "m_resnet_model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "history = m_resnet_model.fit(X_mitbihl_train, y_mitbihl_train, validation_data=(X_mitbihl_val, y_mitbihl_val), \n",
        "                             shuffle=True, batch_size=128, epochs=50, callbacks=[save_best_weights])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lViBLsYvhHpw"
      },
      "source": [
        "pretty_plot(history, 'loss', lambda x: np.argmin(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JH8sJwbahO24"
      },
      "source": [
        "pretty_plot(history, 'accuracy', lambda x: np.argmax(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BPgulOTpqJ3m",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "outputId": "ee58bf38-8de2-435e-f859-9fec37c72f28"
      },
      "source": [
        "y_pred = tf.argmax(m_resnet_model.predict(X_mitbihl_test), axis=-1)\n",
        "# Compute confusion matrix\n",
        "cnf_matrix = sklearn.metrics.confusion_matrix(y_mitbih_test, y_pred)\n",
        "np.set_printoptions(precision=2)\n",
        "\n",
        "# Plot non-normalized confusion matrix\n",
        "plt.figure()\n",
        "plot_confusion_matrix(cnf_matrix, classes=['N', 'S', 'V', 'F', 'Q'],\n",
        "                      title='Confusion matrix, without normalization')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Confusion matrix, without normalization\n",
            "[[18063    23    18     4    10]\n",
            " [  124   417    13     1     1]\n",
            " [   52     3  1374    15     4]\n",
            " [   30     0    16   116     0]\n",
            " [   23     2     0     0  1583]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "8865460eab3b8858828539624ef2f45d875655b94242e338bcb660acf08eeb38"
    },
    "kernelspec": {
      "display_name": "Python 3.8.10 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "orig_nbformat": 4,
    "colab": {
      "name": "model_001.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}