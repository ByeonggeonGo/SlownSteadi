{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "import os\n",
    "import json \n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 498/498 [00:02<00:00, 244.05it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'내부 온도 1 평균': [14.4, 47.3],\n",
       " '내부 온도 1 최고': [14.5, 47.6],\n",
       " '내부 온도 1 최저': [14.4, 47.0],\n",
       " '내부 습도 1 평균': [34.1, 100.0],\n",
       " '내부 습도 1 최고': [36.5, 100.0],\n",
       " '내부 습도 1 최저': [32.4, 100.0],\n",
       " '내부 이슬점 평균': [12.4, 29.9],\n",
       " '내부 이슬점 최고': [12.8, 31.9],\n",
       " '내부 이슬점 최저': [12.1, 29.1]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 분석에 사용할 feature 선택\n",
    "csv_features = ['내부 온도 1 평균', '내부 온도 1 최고', '내부 온도 1 최저', '내부 습도 1 평균', '내부 습도 1 최고', \n",
    "                '내부 습도 1 최저', '내부 이슬점 평균', '내부 이슬점 최고', '내부 이슬점 최저']\n",
    "\n",
    "# csv_files = sorted(glob('sample_data/*/*.csv'))\n",
    "csv_files = sorted(glob(path + '\\\\sample_data\\\\sample_data\\\\*\\\\*.csv'))\n",
    "\n",
    "temp_csv = pd.read_csv(csv_files[0])[csv_features]\n",
    "max_arr, min_arr = temp_csv.max().to_numpy(), temp_csv.min().to_numpy()\n",
    "\n",
    "# feature 별 최대값, 최솟값 계산\n",
    "for csv in tqdm(csv_files[1:]):\n",
    "    temp_csv = pd.read_csv(csv)[csv_features]\n",
    "    temp_max, temp_min = temp_csv.max().to_numpy(), temp_csv.min().to_numpy()\n",
    "    max_arr = np.max([max_arr,temp_max], axis=0)\n",
    "    min_arr = np.min([min_arr,temp_min], axis=0)\n",
    "\n",
    "# feature 별 최대값, 최솟값 dictionary 생성\n",
    "csv_feature_dict = {csv_features[i]:[min_arr[i], max_arr[i]] for i in range(len(csv_features))}\n",
    "csv_feature_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 제공된 sample data는 파프리카와 시설포도 2종류의 작물만 존재\n",
    "label_description = {\n",
    " '3_00_0': '파프리카_정상',\n",
    " '3_a9_1': '파프리카흰가루병_초기',\n",
    " '3_a9_2': '파프리카흰가루병_중기',\n",
    " '3_a9_3': '파프리카흰가루병_말기',\n",
    " '3_a10_1': '파프리카잘록병_초기',\n",
    " '3_a10_2': '파프리카잘록병_중기',\n",
    " '3_a10_3': '파프리카잘록병_말기',\n",
    " '3_b3_1': '칼슘결핍_초기',\n",
    " '3_b3_2': '칼슘결핍_중기',\n",
    " '3_b3_3': '칼슘결핍_말기',\n",
    " '3_b6_1': '다량원소결핍 (N)_초기',\n",
    " '3_b6_2': '다량원소결핍 (N)_중기',\n",
    " '3_b6_3': '다량원소결핍 (N)_말기',\n",
    " '3_b7_1': '다량원소결핍 (P)_초기',\n",
    " '3_b7_2': '다량원소결핍 (P)_중기',\n",
    " '3_b7_3': '다량원소결핍 (P)_말기',\n",
    " '3_b8_1': '다량원소결핍 (K)_초기',\n",
    " '3_b8_2': '다량원소결핍 (K)_중기',\n",
    " '3_b8_3': '다량원소결핍 (K)_말기',\n",
    " '6_00_0': '시설포도_정상',\n",
    " '6_a11_1': '시설포도탄저병_초기',\n",
    " '6_a11_2': '시설포도탄저병_중기',\n",
    " '6_a11_3': '시설포도탄저병_말기',\n",
    " '6_a12_1': '시설포도노균병_초기',\n",
    " '6_a12_2': '시설포도노균병_중기',\n",
    " '6_a12_3': '시설포도노균병_말기',\n",
    " '6_b4_1': '일소피해_초기',\n",
    " '6_b4_2': '일소피해_중기',\n",
    " '6_b4_3': '일소피해_말기',\n",
    " '6_b5_1': '축과병_초기',\n",
    " '6_b5_2': '축과병_중기',\n",
    " '6_b5_3': '축과병_말기',\n",
    "}\n",
    "\n",
    "label_encoder = {key:idx for idx, key in enumerate(label_description)}\n",
    "label_decoder = {val:key for key, val in label_encoder.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[291, 588, 294, 294, 294, 294, 588, 588, 588, 291, 294, 294, 294, 294, 294, 588, 294, 294, 588, 588, 294, 294, 588, 291, 588, 294, 588, 291, 294, 294, 588, 294, 294, 294, 588, 294, 294, 294, 294, 588, 291, 294, 294, 294, 294, 294, 294, 294, 294, 588, 588, 294, 294, 588, 294, 588, 588, 294, 294, 294, 588, 294, 294, 588, 294, 291, 588, 291, 294, 294, 588, 294, 588, 294, 294, 294, 291, 291, 588, 294, 294, 588, 262, 291, 291, 294, 294, 291, 291, 294, 294, 294, 294, 588, 294, 294, 588, 588, 294, 294, 293, 588, 291, 294, 588, 294, 294, 588, 294, 294, 291, 294, 294, 294, 294, 294, 588, 294, 294, 294, 294, 294, 588, 291, 294, 588, 294, 294, 588, 588, 294, 294, 588, 588, 294, 294, 291, 294, 588, 588, 291, 294, 294, 294, 294, 294, 294, 588, 294, 588, 588, 294, 294, 588, 294, 294, 294, 294, 294, 294, 291, 294, 294, 294, 294, 294, 291, 588, 294, 294, 588, 291, 294, 294, 294, 294, 294, 294, 294, 294, 588, 291, 291, 291, 294, 294, 291, 294, 294, 294, 294, 291, 588, 294, 294, 588, 588, 294, 294, 294, 294, 294, 294, 294, 294, 294, 294, 588, 291, 588, 294, 291, 294, 294, 294, 588, 588, 294, 294, 291, 588, 588, 294, 294, 291, 588, 262, 294, 294, 294, 294, 291, 588, 294, 294, 294, 588, 294, 291, 294, 291, 588, 294, 294, 588, 588, 294, 294, 588, 294, 294, 294, 294, 588, 294, 294, 294, 294, 294, 291, 294, 291, 588, 294, 294, 588, 294, 262, 294, 588, 294, 294, 294, 294, 291, 294, 294, 294, 588, 294, 291, 294, 294, 295, 294, 294, 295, 588, 294, 294, 294, 294, 294, 294, 294, 588, 294, 294, 294, 294, 263, 588, 291, 262, 588, 294, 291, 294, 291, 588, 294, 588, 291, 291, 294, 291, 294, 294, 291, 294, 294, 294, 294, 588, 294, 294, 294, 294, 294, 294, 588, 291, 294, 294, 294, 294, 588, 291, 588, 294, 294, 294, 294, 291, 294, 294, 294, 588, 294, 294, 294, 294, 294, 294, 588, 588, 588, 294, 588, 294, 588, 294, 294, 294, 294, 294, 588, 588, 588, 294, 588, 291, 294, 291, 294, 588, 295, 294, 588, 294, 293, 588, 295, 294, 294, 294, 291, 588, 294, 588, 294, 294, 294, 291, 294, 294, 588, 294, 294, 294, 264, 294, 294, 588, 294, 293, 294, 291, 294, 294, 294, 294, 294, 294, 294, 294, 294, 294, 294, 588, 294, 294, 294, 294, 294, 588, 588, 294, 588, 291, 294, 588, 294, 588, 294, 588, 294, 588, 588, 294, 294, 294, 588, 294, 588, 294, 294, 586, 294, 294, 588, 291, 294, 262, 295, 294, 588, 588, 588, 294, 294, 294, 588, 294, 294, 291, 294, 294, 291, 294, 588, 294, 291, 294, 588, 588, 588, 294, 294, 294, 588, 588, 294, 294, 294, 588, 294, 294, 294, 294, 294, 294, 294, 294, 294, 588, 588, 291, 294]\n"
     ]
    }
   ],
   "source": [
    "data_files = glob(path + '\\\\sample_data\\\\sample_data\\\\*')\n",
    "csvlen = []\n",
    "for i in data_files:\n",
    "    \n",
    "    if glob(i + '\\\\*.csv') == []:   #10462폴더 비어있음. 다음에 확인해보기\n",
    "        pass\n",
    "    else:\n",
    "        df = pd.read_csv(glob(i + '\\\\*.csv')[0])\n",
    "        csvlen.append(len(df))\n",
    "    \n",
    "print(csvlen)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "262"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(csvlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataController():\n",
    "    def __init__(self,csvfeatures,csvfeaturedict):\n",
    "        self.csv_features = csvfeatures\n",
    "        self.csv_feature_dict = csvfeaturedict\n",
    "    \n",
    "    def road_csv(self,foldnam,timenum):\n",
    "        df = pd.read_csv(foldnam)\n",
    "        return df.loc[:timenum-1,self.csv_features]      #csv파일 제일 짧은게 291개임 오류인지는 모르겠으나 일단 최소길이로 통일하여 처리\n",
    "    \n",
    "    def scaling(self,minmaxdic,df):\n",
    "        for col in minmaxdic.keys():\n",
    "            df.loc[:,col] = df.loc[:,col] - csv_feature_dict[col][0]\n",
    "            df.loc[:,col] = df.loc[:,col] / (csv_feature_dict[col][1]-csv_feature_dict[col][0])\n",
    "        return df\n",
    "    \n",
    "    \n",
    "    def getimage(self,imgpath):\n",
    "        img = cv2.imread(imgpath)\n",
    "        img = cv2.resize(img, dsize=(256, 256), interpolation=cv2.INTER_AREA)\n",
    "        img = img.astype(np.float32)/255  ##픽셀값을 0~1사이로 정규화\n",
    "        # img = np.transpose(img, (2,0,1))\n",
    "        return img\n",
    "    \n",
    "    def getlable(self,jsonpath):\n",
    "        with open(jsonpath, 'r') as f:\n",
    "            json_file = json.load(f)\n",
    "\n",
    "        crop = json_file['annotations']['crop']\n",
    "        disease = json_file['annotations']['disease']\n",
    "        risk = json_file['annotations']['risk']\n",
    "        label = f'{crop}_{disease}_{risk}'\n",
    "        return label\n",
    "    \n",
    "    def getdata(self,datapath,timenum,featnum):\n",
    "\n",
    "        csvarr = np.empty((0,timenum,featnum), float)\n",
    "        imgarr = np.empty((0,256,256,3), float)\n",
    "        lablearr = np.array([])\n",
    "        \n",
    "        # predictor = np.append(predictor,ndf.reshape(-1,timenum,featnum),axis = 0)\n",
    "        \n",
    "        for ind,i in enumerate(datapath):\n",
    "            \n",
    "            if glob(i + '\\\\*.csv') == []:  #10462폴더 비어있음. 다음에 확인해보기\n",
    "                pass\n",
    "            else:\n",
    "                csvpath = glob(i + '\\\\*.csv')[0]\n",
    "                imgpath = glob(i + '\\\\*.jpg')[0]\n",
    "                jsonpath = glob(i + '\\\\*.json')[0]\n",
    "                # con = DataController()\n",
    "                df = self.road_csv(csvpath,timenum)\n",
    "                df2 = self.scaling(csv_feature_dict,df).to_numpy().reshape(-1,timenum,featnum)\n",
    "                imgdata = self.getimage(imgpath).reshape(-1,256,256,3)\n",
    "                label = label_encoder[self.getlable(jsonpath)]\n",
    "                # label = self.getlable(jsonpath)\n",
    "                \n",
    "                csvarr = np.append(csvarr,df2, axis = 0)\n",
    "                imgarr = np.append(imgarr,imgdata, axis = 0)\n",
    "                lablearr = np.append(lablearr,label)\n",
    "            \n",
    "        return [csvarr,imgarr],lablearr\n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#프로젝트에 있는 모든 데이터폴터 불러오기\n",
    "data_files = glob(path + '\\\\sample_data\\\\sample_data\\\\*')\n",
    "#셔플\n",
    "random.shuffle(data_files)\n",
    "#앞에서 300번째까지 트레인셋으로\n",
    "trainfiles = data_files[:400]\n",
    "#나머지는 테스트셋으로\n",
    "testfiles = data_files[400:]\n",
    "\n",
    "# 데이터 컨트롤러\n",
    "dacon = DataController(csv_features,csv_feature_dict)\n",
    "# 배치화된 데이터셋 만들기\n",
    "# test셋용으로는 train = False로 하여 배치안생성하게됨\n",
    "x_train,y_train = dacon.getdata(trainfiles,260,9)\n",
    "x_test,y_test = dacon.getdata(testfiles,260,9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class paraBlock(keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(paraBlock, self).__init__()\n",
    "        self.block1_layer1 = tf.keras.layers.LSTM(128)\n",
    "        self.block1_layer2 = tf.keras.layers.Dense(64)\n",
    "        self.block1_layer3 = tf.keras.layers.Dense(32)\n",
    "        self.block1_layer4 = tf.keras.layers.Dense(16)\n",
    "        \n",
    "        self.block2_layer1 = tf.keras.layers.Conv2D( filters=16, kernel_size=10, activation='relu')\n",
    "        self.block2_layer2 = tf.keras.layers.Conv2D( filters=32, kernel_size=10, activation='relu')\n",
    "        self.block2_layer3 = tf.keras.layers.Conv2D( filters=64, kernel_size=10, activation='relu')\n",
    "        self.block2_layer4 = tf.keras.layers.Conv2D( filters=128, kernel_size=10, activation='relu')\n",
    "        \n",
    "        self.pooling_layer = tf.keras.layers.MaxPooling2D((2, 2))\n",
    "        self.flat_layer = tf.keras.layers.Flatten()\n",
    "        \n",
    "        self.out_layer1 = tf.keras.layers.Dense(64,activation='relu')\n",
    "        self.out_layer2 = tf.keras.layers.Dense(32,activation='softmax')\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        #LSTM파트\n",
    "        lstm_x = self.block1_layer1(inputs[0])\n",
    "        lstm_x = tf.nn.relu(lstm_x)\n",
    "        lstm_x = self.block1_layer2(lstm_x)\n",
    "        lstm_x = tf.nn.relu(lstm_x)\n",
    "        lstm_x = self.block1_layer3(lstm_x)\n",
    "        lstm_x = tf.nn.relu(lstm_x)\n",
    "        lstm_x = self.block1_layer4(lstm_x)\n",
    "        # x = tf.nn.relu(x)\n",
    "        # x = self.layer5(x)\n",
    "        \n",
    "        #CNN파트\n",
    "        cnn_x = self.block2_layer1(inputs[1])\n",
    "        cnn_x = self.pooling_layer(cnn_x)\n",
    "        \n",
    "        cnn_x = self.block2_layer2(cnn_x)\n",
    "        cnn_x = self.pooling_layer(cnn_x)\n",
    "        \n",
    "        cnn_x = self.block2_layer3(cnn_x)\n",
    "        cnn_x = self.pooling_layer(cnn_x)\n",
    "        \n",
    "        cnn_x = self.block2_layer4(cnn_x)\n",
    "        cnn_x = self.flat_layer(cnn_x)\n",
    "        \n",
    "        #합치기\n",
    "        out = tf.concat([lstm_x,cnn_x],axis=1)\n",
    "        out = self.out_layer1(out)\n",
    "        out = self.out_layer2(out)\n",
    "        \n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Paramodel(keras.Model):\n",
    "    def __init__(self, name = None):\n",
    "        super(Paramodel, self).__init__()\n",
    "    \n",
    "        self.module1 = paraBlock()\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x1 = self.module1(inputs)\n",
    "    \n",
    "        # x = tf.concat([x1,x2,x3],axis=1)\n",
    "        \n",
    "        \n",
    "        return(x1)\n",
    "    \n",
    "# inputs = tf.random.normal([3,2,10,8])\n",
    "# model1 = Paramodel(name='model_01')\n",
    "# print(model1(inputs))\n",
    "# model1.variables,model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0.03534087 0.03173486 0.02937816 ... 0.03081517 0.03039897 0.03134452]\n",
      " [0.03605588 0.0313326  0.02957969 ... 0.03046688 0.03148051 0.03194847]\n",
      " [0.03503105 0.03170215 0.03005319 ... 0.03042696 0.03162529 0.03117512]\n",
      " ...\n",
      " [0.03660838 0.03142332 0.02915426 ... 0.03074797 0.03039005 0.03124927]\n",
      " [0.03620866 0.03141771 0.02952239 ... 0.03054002 0.03093392 0.0314289 ]\n",
      " [0.03609478 0.03178464 0.02955336 ... 0.03031245 0.03136037 0.03164868]], shape=(400, 32), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "model1 = Paramodel(name='model_01')\n",
    "inputs = x_train\n",
    "print(model1(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate an optimizer.\n",
    "optimizer = keras.optimizers.Adam(learning_rate=1e-3)\n",
    "# Instantiate a loss function.\n",
    "loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start of epoch 0\n",
      "Training loss (for one batch) at step 0: 39.3254\n",
      "Seen so far: 1 samples\n",
      "\n",
      "Start of epoch 1\n",
      "Training loss (for one batch) at step 0: 2.9413\n",
      "Seen so far: 1 samples\n"
     ]
    }
   ],
   "source": [
    "epochs = 2\n",
    "for epoch in range(epochs):\n",
    "    print(\"\\nStart of epoch %d\" % (epoch,))\n",
    "\n",
    "    # Iterate over the batches of the dataset.\n",
    "    for step in range(1):\n",
    "\n",
    "        # Open a GradientTape to record the operations run\n",
    "        # during the forward pass, which enables auto-differentiation.\n",
    "        with tf.GradientTape() as tape:\n",
    "\n",
    "            # Run the forward pass of the layer.\n",
    "            # The operations that the layer applies\n",
    "            # to its inputs are going to be recorded\n",
    "            # on the GradientTape.\n",
    "            logits = model1(x_train, training=True)  # Logits for this minibatch\n",
    "\n",
    "            # Compute the loss value for this minibatch.\n",
    "            loss_value = loss_fn(y_train, logits)\n",
    "            #label_encoder\n",
    "        # Use the gradient tape to automatically retrieve\n",
    "        # the gradients of the trainable variables with respect to the loss.\n",
    "        grads = tape.gradient(loss_value, model1.trainable_weights)\n",
    "\n",
    "        # Run one step of gradient descent by updating\n",
    "        # the value of the variables to minimize the loss.\n",
    "        optimizer.apply_gradients(zip(grads, model1.trainable_weights))\n",
    "\n",
    "        # Log every 200 batches.\n",
    "        if step % 200 == 0:\n",
    "            print(\n",
    "                \"Training loss (for one batch) at step %d: %.4f\"\n",
    "                % (step, float(loss_value))\n",
    "            )\n",
    "            print(\"Seen so far: %s samples\" % ((step + 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델성능:  tf.Tensor(3.3808637, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "y_pred = model1(x_test)\n",
    "score = loss_fn(y_test,y_pred)\n",
    "print(\"모델성능: \",score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8865460eab3b8858828539624ef2f45d875655b94242e338bcb660acf08eeb38"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
