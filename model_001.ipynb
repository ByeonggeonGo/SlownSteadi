{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "import os\n",
    "import json \n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 498/498 [00:02<00:00, 238.83it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'내부 온도 1 평균': [14.4, 47.3],\n",
       " '내부 온도 1 최고': [14.5, 47.6],\n",
       " '내부 온도 1 최저': [14.4, 47.0],\n",
       " '내부 습도 1 평균': [34.1, 100.0],\n",
       " '내부 습도 1 최고': [36.5, 100.0],\n",
       " '내부 습도 1 최저': [32.4, 100.0],\n",
       " '내부 이슬점 평균': [12.4, 29.9],\n",
       " '내부 이슬점 최고': [12.8, 31.9],\n",
       " '내부 이슬점 최저': [12.1, 29.1]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 분석에 사용할 feature 선택\n",
    "csv_features = ['내부 온도 1 평균', '내부 온도 1 최고', '내부 온도 1 최저', '내부 습도 1 평균', '내부 습도 1 최고', \n",
    "                '내부 습도 1 최저', '내부 이슬점 평균', '내부 이슬점 최고', '내부 이슬점 최저']\n",
    "\n",
    "# csv_files = sorted(glob('sample_data/*/*.csv'))\n",
    "csv_files = sorted(glob(path + '\\\\sample_data\\\\sample_data\\\\*\\\\*.csv'))\n",
    "\n",
    "temp_csv = pd.read_csv(csv_files[0])[csv_features]\n",
    "max_arr, min_arr = temp_csv.max().to_numpy(), temp_csv.min().to_numpy()\n",
    "\n",
    "# feature 별 최대값, 최솟값 계산\n",
    "for csv in tqdm(csv_files[1:]):\n",
    "    temp_csv = pd.read_csv(csv)[csv_features]\n",
    "    temp_max, temp_min = temp_csv.max().to_numpy(), temp_csv.min().to_numpy()\n",
    "    max_arr = np.max([max_arr,temp_max], axis=0)\n",
    "    min_arr = np.min([min_arr,temp_min], axis=0)\n",
    "\n",
    "# feature 별 최대값, 최솟값 dictionary 생성\n",
    "csv_feature_dict = {csv_features[i]:[min_arr[i], max_arr[i]] for i in range(len(csv_features))}\n",
    "csv_feature_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 제공된 sample data는 파프리카와 시설포도 2종류의 작물만 존재\n",
    "label_description = {\n",
    " '3_00_0': '파프리카_정상',\n",
    " '3_a9_1': '파프리카흰가루병_초기',\n",
    " '3_a9_2': '파프리카흰가루병_중기',\n",
    " '3_a9_3': '파프리카흰가루병_말기',\n",
    " '3_a10_1': '파프리카잘록병_초기',\n",
    " '3_a10_2': '파프리카잘록병_중기',\n",
    " '3_a10_3': '파프리카잘록병_말기',\n",
    " '3_b3_1': '칼슘결핍_초기',\n",
    " '3_b3_2': '칼슘결핍_중기',\n",
    " '3_b3_3': '칼슘결핍_말기',\n",
    " '3_b6_1': '다량원소결핍 (N)_초기',\n",
    " '3_b6_2': '다량원소결핍 (N)_중기',\n",
    " '3_b6_3': '다량원소결핍 (N)_말기',\n",
    " '3_b7_1': '다량원소결핍 (P)_초기',\n",
    " '3_b7_2': '다량원소결핍 (P)_중기',\n",
    " '3_b7_3': '다량원소결핍 (P)_말기',\n",
    " '3_b8_1': '다량원소결핍 (K)_초기',\n",
    " '3_b8_2': '다량원소결핍 (K)_중기',\n",
    " '3_b8_3': '다량원소결핍 (K)_말기',\n",
    " '6_00_0': '시설포도_정상',\n",
    " '6_a11_1': '시설포도탄저병_초기',\n",
    " '6_a11_2': '시설포도탄저병_중기',\n",
    " '6_a11_3': '시설포도탄저병_말기',\n",
    " '6_a12_1': '시설포도노균병_초기',\n",
    " '6_a12_2': '시설포도노균병_중기',\n",
    " '6_a12_3': '시설포도노균병_말기',\n",
    " '6_b4_1': '일소피해_초기',\n",
    " '6_b4_2': '일소피해_중기',\n",
    " '6_b4_3': '일소피해_말기',\n",
    " '6_b5_1': '축과병_초기',\n",
    " '6_b5_2': '축과병_중기',\n",
    " '6_b5_3': '축과병_말기',\n",
    "}\n",
    "\n",
    "label_encoder = {key:idx for idx, key in enumerate(label_description)}\n",
    "label_decoder = {val:key for key, val in label_encoder.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[291, 588, 294, 294, 294, 294, 588, 588, 588, 291, 294, 294, 294, 294, 294, 588, 294, 294, 588, 588, 294, 294, 588, 291, 588, 294, 588, 291, 294, 294, 588, 294, 294, 294, 588, 294, 294, 294, 294, 588, 291, 294, 294, 294, 294, 294, 294, 294, 294, 588, 588, 294, 294, 588, 294, 588, 588, 294, 294, 294, 588, 294, 294, 588, 294, 291, 588, 291, 294, 294, 588, 294, 588, 294, 294, 294, 291, 291, 588, 294, 294, 588, 262, 291, 291, 294, 294, 291, 291, 294, 294, 294, 294, 588, 294, 294, 588, 588, 294, 294, 293, 588, 291, 294, 588, 294, 294, 588, 294, 294, 291, 294, 294, 294, 294, 294, 588, 294, 294, 294, 294, 294, 588, 291, 294, 588, 294, 294, 588, 588, 294, 294, 588, 588, 294, 294, 291, 294, 588, 588, 291, 294, 294, 294, 294, 294, 294, 588, 294, 588, 588, 294, 294, 588, 294, 294, 294, 294, 294, 294, 291, 294, 294, 294, 294, 294, 291, 588, 294, 294, 588, 291, 294, 294, 294, 294, 294, 294, 294, 294, 588, 291, 291, 291, 294, 294, 291, 294, 294, 294, 294, 291, 588, 294, 294, 588, 588, 294, 294, 294, 294, 294, 294, 294, 294, 294, 294, 588, 291, 588, 294, 291, 294, 294, 294, 588, 588, 294, 294, 291, 588, 588, 294, 294, 291, 588, 262, 294, 294, 294, 294, 291, 588, 294, 294, 294, 588, 294, 291, 294, 291, 588, 294, 294, 588, 588, 294, 294, 588, 294, 294, 294, 294, 588, 294, 294, 294, 294, 294, 291, 294, 291, 588, 294, 294, 588, 294, 262, 294, 588, 294, 294, 294, 294, 291, 294, 294, 294, 588, 294, 291, 294, 294, 295, 294, 294, 295, 588, 294, 294, 294, 294, 294, 294, 294, 588, 294, 294, 294, 294, 263, 588, 291, 262, 588, 294, 291, 294, 291, 588, 294, 588, 291, 291, 294, 291, 294, 294, 291, 294, 294, 294, 294, 588, 294, 294, 294, 294, 294, 294, 588, 291, 294, 294, 294, 294, 588, 291, 588, 294, 294, 294, 294, 291, 294, 294, 294, 588, 294, 294, 294, 294, 294, 294, 588, 588, 588, 294, 588, 294, 588, 294, 294, 294, 294, 294, 588, 588, 588, 294, 588, 291, 294, 291, 294, 588, 295, 294, 588, 294, 293, 588, 295, 294, 294, 294, 291, 588, 294, 588, 294, 294, 294, 291, 294, 294, 588, 294, 294, 294, 264, 294, 294, 588, 294, 293, 294, 291, 294, 294, 294, 294, 294, 294, 294, 294, 294, 294, 294, 588, 294, 294, 294, 294, 294, 588, 588, 294, 588, 291, 294, 588, 294, 588, 294, 588, 294, 588, 588, 294, 294, 294, 588, 294, 588, 294, 294, 586, 294, 294, 588, 291, 294, 262, 295, 294, 588, 588, 588, 294, 294, 294, 588, 294, 294, 291, 294, 294, 291, 294, 588, 294, 291, 294, 588, 588, 588, 294, 294, 294, 588, 588, 294, 294, 294, 588, 294, 294, 294, 294, 294, 294, 294, 294, 294, 588, 588, 291, 294]\n"
     ]
    }
   ],
   "source": [
    "data_files = glob(path + '\\\\sample_data\\\\sample_data\\\\*')\n",
    "csvlen = []\n",
    "for i in data_files:\n",
    "    \n",
    "    if glob(i + '\\\\*.csv') == []:   #10462폴더 비어있음. 다음에 확인해보기\n",
    "        pass\n",
    "    else:\n",
    "        df = pd.read_csv(glob(i + '\\\\*.csv')[0])\n",
    "        csvlen.append(len(df))\n",
    "    \n",
    "print(csvlen)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "262"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(csvlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataController():\n",
    "    def __init__(self,csvfeatures,csvfeaturedict):\n",
    "        self.csv_features = csvfeatures\n",
    "        self.csv_feature_dict = csvfeaturedict\n",
    "    \n",
    "    def road_csv(self,foldnam,timenum):\n",
    "        df = pd.read_csv(foldnam)\n",
    "        return df.loc[:timenum-1,self.csv_features]      #csv파일 제일 짧은게 291개임 오류인지는 모르겠으나 일단 최소길이로 통일하여 처리\n",
    "    \n",
    "    def scaling(self,minmaxdic,df):\n",
    "        for col in minmaxdic.keys():\n",
    "            df.loc[:,col] = df.loc[:,col] - csv_feature_dict[col][0]\n",
    "            df.loc[:,col] = df.loc[:,col] / (csv_feature_dict[col][1]-csv_feature_dict[col][0])\n",
    "        return df\n",
    "    \n",
    "    \n",
    "    def getimage(self,imgpath):\n",
    "        img = cv2.imread(imgpath)\n",
    "        img = cv2.resize(img, dsize=(256, 256), interpolation=cv2.INTER_AREA)\n",
    "        img = img.astype(np.float32)/255  ##픽셀값을 0~1사이로 정규화\n",
    "        # img = np.transpose(img, (2,0,1))\n",
    "        return img\n",
    "    \n",
    "    def getlable(self,jsonpath):\n",
    "        with open(jsonpath, 'r') as f:\n",
    "            json_file = json.load(f)\n",
    "\n",
    "        crop = json_file['annotations']['crop']\n",
    "        disease = json_file['annotations']['disease']\n",
    "        risk = json_file['annotations']['risk']\n",
    "        label = f'{crop}_{disease}_{risk}'\n",
    "        return label\n",
    "    \n",
    "    def getdata(self,datapath,timenum,featnum):\n",
    "\n",
    "        csvarr = np.empty((0,timenum,featnum), float)\n",
    "        imgarr = np.empty((0,256,256,3), float)\n",
    "        lablearr = np.array([])\n",
    "        \n",
    "        # predictor = np.append(predictor,ndf.reshape(-1,timenum,featnum),axis = 0)\n",
    "        \n",
    "        for ind,i in enumerate(datapath):\n",
    "            \n",
    "            if glob(i + '\\\\*.csv') == []:  #10462폴더 비어있음. 다음에 확인해보기\n",
    "                pass\n",
    "            else:\n",
    "                csvpath = glob(i + '\\\\*.csv')[0]\n",
    "                imgpath = glob(i + '\\\\*.jpg')[0]\n",
    "                jsonpath = glob(i + '\\\\*.json')[0]\n",
    "                # con = DataController()\n",
    "                df = self.road_csv(csvpath,timenum)\n",
    "                df2 = self.scaling(csv_feature_dict,df).to_numpy().reshape(-1,timenum,featnum)\n",
    "                imgdata = self.getimage(imgpath).reshape(-1,256,256,3)\n",
    "                label = label_encoder[self.getlable(jsonpath)]\n",
    "                # label = self.getlable(jsonpath)\n",
    "                \n",
    "                csvarr = np.append(csvarr,df2, axis = 0)\n",
    "                imgarr = np.append(imgarr,imgdata, axis = 0)\n",
    "                lablearr = np.append(lablearr,label)\n",
    "            \n",
    "        return [csvarr,imgarr],lablearr\n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#프로젝트에 있는 모든 데이터폴터 불러오기\n",
    "data_files = glob(path + '\\\\sample_data\\\\sample_data\\\\*')\n",
    "#셔플\n",
    "random.shuffle(data_files)\n",
    "#앞에서 300번째까지 트레인셋으로\n",
    "trainfiles = data_files[:400]\n",
    "#나머지는 테스트셋으로\n",
    "testfiles = data_files[400:]\n",
    "\n",
    "# 데이터 컨트롤러\n",
    "dacon = DataController(csv_features,csv_feature_dict)\n",
    "# 배치화된 데이터셋 만들기\n",
    "# test셋용으로는 train = False로 하여 배치안생성하게됨\n",
    "x_train,y_train = dacon.getdata(trainfiles,260,9)\n",
    "x_test,y_test = dacon.getdata(testfiles,260,9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class paraBlock(keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(paraBlock, self).__init__()\n",
    "        self.block1_layer1 = tf.keras.layers.LSTM(128)\n",
    "        self.block1_layer2 = tf.keras.layers.Dense(64)\n",
    "        self.block1_layer3 = tf.keras.layers.Dense(32)\n",
    "        self.block1_layer4 = tf.keras.layers.Dense(16)\n",
    "        \n",
    "        self.block2_layer1 = tf.keras.layers.Conv2D( filters=16, kernel_size=10, activation='relu')\n",
    "        self.block2_layer2 = tf.keras.layers.Conv2D( filters=32, kernel_size=10, activation='relu')\n",
    "        self.block2_layer3 = tf.keras.layers.Conv2D( filters=64, kernel_size=10, activation='relu')\n",
    "        self.block2_layer4 = tf.keras.layers.Conv2D( filters=128, kernel_size=10, activation='relu')\n",
    "        \n",
    "        self.pooling_layer = tf.keras.layers.MaxPooling2D((2, 2))\n",
    "        self.flat_layer = tf.keras.layers.Flatten()\n",
    "        \n",
    "        self.out_layer1 = tf.keras.layers.Dense(64,activation='relu')\n",
    "        self.out_layer2 = tf.keras.layers.Dense(32,activation='softmax')\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        #LSTM파트\n",
    "        lstm_x = self.block1_layer1(inputs[0])\n",
    "        lstm_x = tf.nn.relu(lstm_x)\n",
    "        lstm_x = self.block1_layer2(lstm_x)\n",
    "        lstm_x = tf.nn.relu(lstm_x)\n",
    "        lstm_x = self.block1_layer3(lstm_x)\n",
    "        lstm_x = tf.nn.relu(lstm_x)\n",
    "        lstm_x = self.block1_layer4(lstm_x)\n",
    "        # x = tf.nn.relu(x)\n",
    "        # x = self.layer5(x)\n",
    "        \n",
    "        #CNN파트\n",
    "        cnn_x = self.block2_layer1(inputs[1])\n",
    "        cnn_x = self.pooling_layer(cnn_x)\n",
    "        \n",
    "        cnn_x = self.block2_layer2(cnn_x)\n",
    "        cnn_x = self.pooling_layer(cnn_x)\n",
    "        \n",
    "        cnn_x = self.block2_layer3(cnn_x)\n",
    "        cnn_x = self.pooling_layer(cnn_x)\n",
    "        \n",
    "        cnn_x = self.block2_layer4(cnn_x)\n",
    "        cnn_x = self.flat_layer(cnn_x)\n",
    "        \n",
    "        #합치기\n",
    "        out = tf.concat([lstm_x,cnn_x],axis=1)\n",
    "        out = self.out_layer1(out)\n",
    "        out = self.out_layer2(out)\n",
    "        \n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Paramodel(keras.Model):\n",
    "    def __init__(self, name = None):\n",
    "        super(Paramodel, self).__init__()\n",
    "    \n",
    "        self.module1 = paraBlock()\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x1 = self.module1(inputs)\n",
    "    \n",
    "        # x = tf.concat([x1,x2,x3],axis=1)\n",
    "        \n",
    "        \n",
    "        return(x1)\n",
    "    \n",
    "# inputs = tf.random.normal([3,2,10,8])\n",
    "# model1 = Paramodel(name='model_01')\n",
    "# print(model1(inputs))\n",
    "# model1.variables,model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0.03205568 0.03262511 0.0325114  ... 0.03049173 0.0311074  0.0291132 ]\n",
      " [0.03210901 0.03192621 0.03190545 ... 0.03176542 0.03091844 0.02995418]\n",
      " [0.03171767 0.03203866 0.03224945 ... 0.03156668 0.03095929 0.0295018 ]\n",
      " ...\n",
      " [0.03192802 0.03189824 0.0316848  ... 0.03154081 0.03108016 0.03016209]\n",
      " [0.03244521 0.03235402 0.03152893 ... 0.03139766 0.03135813 0.02974537]\n",
      " [0.03240421 0.03274881 0.03285348 ... 0.03085973 0.03163281 0.02943368]], shape=(399, 32), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "model1 = Paramodel(name='model_01')\n",
    "inputs = x_train\n",
    "print(model1(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate an optimizer.\n",
    "optimizer = keras.optimizers.Adam(learning_rate=1e-3)\n",
    "# Instantiate a loss function.\n",
    "loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start of epoch 0\n",
      "Training loss (for one batch) at step 0: 1.6394\n",
      "Seen so far: 1 samples\n",
      "\n",
      "Start of epoch 1\n",
      "Training loss (for one batch) at step 0: 1.6249\n",
      "Seen so far: 1 samples\n",
      "\n",
      "Start of epoch 2\n",
      "Training loss (for one batch) at step 0: 1.5995\n",
      "Seen so far: 1 samples\n",
      "\n",
      "Start of epoch 3\n",
      "Training loss (for one batch) at step 0: 1.5569\n",
      "Seen so far: 1 samples\n",
      "\n",
      "Start of epoch 4\n",
      "Training loss (for one batch) at step 0: 1.5165\n",
      "Seen so far: 1 samples\n",
      "\n",
      "Start of epoch 5\n",
      "Training loss (for one batch) at step 0: 1.4911\n",
      "Seen so far: 1 samples\n",
      "\n",
      "Start of epoch 6\n",
      "Training loss (for one batch) at step 0: 1.4752\n",
      "Seen so far: 1 samples\n",
      "\n",
      "Start of epoch 7\n",
      "Training loss (for one batch) at step 0: 1.4441\n",
      "Seen so far: 1 samples\n",
      "\n",
      "Start of epoch 8\n",
      "Training loss (for one batch) at step 0: 1.4919\n",
      "Seen so far: 1 samples\n",
      "\n",
      "Start of epoch 9\n",
      "Training loss (for one batch) at step 0: 1.4690\n",
      "Seen so far: 1 samples\n",
      "\n",
      "Start of epoch 10\n",
      "Training loss (for one batch) at step 0: 1.4630\n",
      "Seen so far: 1 samples\n",
      "\n",
      "Start of epoch 11\n",
      "Training loss (for one batch) at step 0: 1.4258\n",
      "Seen so far: 1 samples\n",
      "\n",
      "Start of epoch 12\n",
      "Training loss (for one batch) at step 0: 1.3579\n",
      "Seen so far: 1 samples\n",
      "\n",
      "Start of epoch 13\n",
      "Training loss (for one batch) at step 0: 1.3546\n",
      "Seen so far: 1 samples\n",
      "\n",
      "Start of epoch 14\n",
      "Training loss (for one batch) at step 0: 1.3434\n",
      "Seen so far: 1 samples\n",
      "\n",
      "Start of epoch 15\n",
      "Training loss (for one batch) at step 0: 1.3342\n",
      "Seen so far: 1 samples\n",
      "\n",
      "Start of epoch 16\n",
      "Training loss (for one batch) at step 0: 1.2928\n",
      "Seen so far: 1 samples\n",
      "\n",
      "Start of epoch 17\n",
      "Training loss (for one batch) at step 0: 1.2731\n",
      "Seen so far: 1 samples\n",
      "\n",
      "Start of epoch 18\n",
      "Training loss (for one batch) at step 0: 1.2724\n",
      "Seen so far: 1 samples\n",
      "\n",
      "Start of epoch 19\n",
      "Training loss (for one batch) at step 0: 1.2938\n",
      "Seen so far: 1 samples\n",
      "\n",
      "Start of epoch 20\n",
      "Training loss (for one batch) at step 0: 1.2291\n",
      "Seen so far: 1 samples\n",
      "\n",
      "Start of epoch 21\n",
      "Training loss (for one batch) at step 0: 1.2397\n",
      "Seen so far: 1 samples\n",
      "\n",
      "Start of epoch 22\n",
      "Training loss (for one batch) at step 0: 1.2250\n",
      "Seen so far: 1 samples\n",
      "\n",
      "Start of epoch 23\n",
      "Training loss (for one batch) at step 0: 1.2325\n",
      "Seen so far: 1 samples\n",
      "\n",
      "Start of epoch 24\n",
      "Training loss (for one batch) at step 0: 1.2114\n",
      "Seen so far: 1 samples\n",
      "\n",
      "Start of epoch 25\n",
      "Training loss (for one batch) at step 0: 1.1999\n",
      "Seen so far: 1 samples\n",
      "\n",
      "Start of epoch 26\n",
      "Training loss (for one batch) at step 0: 1.1891\n",
      "Seen so far: 1 samples\n",
      "\n",
      "Start of epoch 27\n",
      "Training loss (for one batch) at step 0: 1.1712\n",
      "Seen so far: 1 samples\n",
      "\n",
      "Start of epoch 28\n",
      "Training loss (for one batch) at step 0: 1.1416\n",
      "Seen so far: 1 samples\n",
      "\n",
      "Start of epoch 29\n",
      "Training loss (for one batch) at step 0: 1.1459\n",
      "Seen so far: 1 samples\n",
      "\n",
      "Start of epoch 30\n",
      "Training loss (for one batch) at step 0: 1.1229\n",
      "Seen so far: 1 samples\n",
      "\n",
      "Start of epoch 31\n",
      "Training loss (for one batch) at step 0: 1.0956\n",
      "Seen so far: 1 samples\n",
      "\n",
      "Start of epoch 32\n",
      "Training loss (for one batch) at step 0: 1.0949\n",
      "Seen so far: 1 samples\n",
      "\n",
      "Start of epoch 33\n",
      "Training loss (for one batch) at step 0: 1.1695\n",
      "Seen so far: 1 samples\n",
      "\n",
      "Start of epoch 34\n",
      "Training loss (for one batch) at step 0: 1.1467\n",
      "Seen so far: 1 samples\n",
      "\n",
      "Start of epoch 35\n",
      "Training loss (for one batch) at step 0: 1.0811\n",
      "Seen so far: 1 samples\n",
      "\n",
      "Start of epoch 36\n",
      "Training loss (for one batch) at step 0: 1.0859\n",
      "Seen so far: 1 samples\n",
      "\n",
      "Start of epoch 37\n",
      "Training loss (for one batch) at step 0: 1.0818\n",
      "Seen so far: 1 samples\n",
      "\n",
      "Start of epoch 38\n",
      "Training loss (for one batch) at step 0: 1.0519\n",
      "Seen so far: 1 samples\n",
      "\n",
      "Start of epoch 39\n",
      "Training loss (for one batch) at step 0: 1.0767\n",
      "Seen so far: 1 samples\n",
      "\n",
      "Start of epoch 40\n",
      "Training loss (for one batch) at step 0: 1.0202\n",
      "Seen so far: 1 samples\n",
      "\n",
      "Start of epoch 41\n",
      "Training loss (for one batch) at step 0: 1.0304\n",
      "Seen so far: 1 samples\n",
      "\n",
      "Start of epoch 42\n",
      "Training loss (for one batch) at step 0: 1.0238\n",
      "Seen so far: 1 samples\n",
      "\n",
      "Start of epoch 43\n",
      "Training loss (for one batch) at step 0: 0.9885\n",
      "Seen so far: 1 samples\n",
      "\n",
      "Start of epoch 44\n",
      "Training loss (for one batch) at step 0: 1.0012\n",
      "Seen so far: 1 samples\n",
      "\n",
      "Start of epoch 45\n",
      "Training loss (for one batch) at step 0: 0.9583\n",
      "Seen so far: 1 samples\n",
      "\n",
      "Start of epoch 46\n",
      "Training loss (for one batch) at step 0: 0.9691\n",
      "Seen so far: 1 samples\n",
      "\n",
      "Start of epoch 47\n",
      "Training loss (for one batch) at step 0: 0.9386\n",
      "Seen so far: 1 samples\n",
      "\n",
      "Start of epoch 48\n",
      "Training loss (for one batch) at step 0: 0.9254\n",
      "Seen so far: 1 samples\n",
      "\n",
      "Start of epoch 49\n",
      "Training loss (for one batch) at step 0: 0.9033\n",
      "Seen so far: 1 samples\n",
      "\n",
      "Start of epoch 50\n",
      "Training loss (for one batch) at step 0: 0.8801\n",
      "Seen so far: 1 samples\n",
      "\n",
      "Start of epoch 51\n",
      "Training loss (for one batch) at step 0: 0.8564\n",
      "Seen so far: 1 samples\n",
      "\n",
      "Start of epoch 52\n",
      "Training loss (for one batch) at step 0: 0.8328\n",
      "Seen so far: 1 samples\n",
      "\n",
      "Start of epoch 53\n",
      "Training loss (for one batch) at step 0: 0.8103\n",
      "Seen so far: 1 samples\n",
      "\n",
      "Start of epoch 54\n",
      "Training loss (for one batch) at step 0: 0.7995\n",
      "Seen so far: 1 samples\n",
      "\n",
      "Start of epoch 55\n",
      "Training loss (for one batch) at step 0: 0.7655\n",
      "Seen so far: 1 samples\n",
      "\n",
      "Start of epoch 56\n",
      "Training loss (for one batch) at step 0: 0.7317\n",
      "Seen so far: 1 samples\n",
      "\n",
      "Start of epoch 57\n",
      "Training loss (for one batch) at step 0: 0.7208\n",
      "Seen so far: 1 samples\n",
      "\n",
      "Start of epoch 58\n",
      "Training loss (for one batch) at step 0: 0.7543\n",
      "Seen so far: 1 samples\n",
      "\n",
      "Start of epoch 59\n",
      "Training loss (for one batch) at step 0: 0.6856\n",
      "Seen so far: 1 samples\n",
      "\n",
      "Start of epoch 60\n",
      "Training loss (for one batch) at step 0: 0.6524\n",
      "Seen so far: 1 samples\n",
      "\n",
      "Start of epoch 61\n",
      "Training loss (for one batch) at step 0: 0.6335\n",
      "Seen so far: 1 samples\n",
      "\n",
      "Start of epoch 62\n",
      "Training loss (for one batch) at step 0: 0.6195\n",
      "Seen so far: 1 samples\n",
      "\n",
      "Start of epoch 63\n",
      "Training loss (for one batch) at step 0: 0.5752\n",
      "Seen so far: 1 samples\n",
      "\n",
      "Start of epoch 64\n",
      "Training loss (for one batch) at step 0: 0.5546\n",
      "Seen so far: 1 samples\n",
      "\n",
      "Start of epoch 65\n",
      "Training loss (for one batch) at step 0: 0.5356\n",
      "Seen so far: 1 samples\n",
      "\n",
      "Start of epoch 66\n",
      "Training loss (for one batch) at step 0: 0.5107\n",
      "Seen so far: 1 samples\n",
      "\n",
      "Start of epoch 67\n",
      "Training loss (for one batch) at step 0: 0.5329\n",
      "Seen so far: 1 samples\n",
      "\n",
      "Start of epoch 68\n",
      "Training loss (for one batch) at step 0: 0.5137\n",
      "Seen so far: 1 samples\n",
      "\n",
      "Start of epoch 69\n",
      "Training loss (for one batch) at step 0: 0.4839\n",
      "Seen so far: 1 samples\n",
      "\n",
      "Start of epoch 70\n",
      "Training loss (for one batch) at step 0: 0.4476\n",
      "Seen so far: 1 samples\n",
      "\n",
      "Start of epoch 71\n",
      "Training loss (for one batch) at step 0: 0.4095\n",
      "Seen so far: 1 samples\n",
      "\n",
      "Start of epoch 72\n",
      "Training loss (for one batch) at step 0: 0.3824\n",
      "Seen so far: 1 samples\n",
      "\n",
      "Start of epoch 73\n",
      "Training loss (for one batch) at step 0: 0.3605\n",
      "Seen so far: 1 samples\n",
      "\n",
      "Start of epoch 74\n",
      "Training loss (for one batch) at step 0: 0.3361\n",
      "Seen so far: 1 samples\n",
      "\n",
      "Start of epoch 75\n",
      "Training loss (for one batch) at step 0: 0.3230\n",
      "Seen so far: 1 samples\n",
      "\n",
      "Start of epoch 76\n",
      "Training loss (for one batch) at step 0: 0.3017\n",
      "Seen so far: 1 samples\n",
      "\n",
      "Start of epoch 77\n",
      "Training loss (for one batch) at step 0: 0.2755\n",
      "Seen so far: 1 samples\n",
      "\n",
      "Start of epoch 78\n",
      "Training loss (for one batch) at step 0: 0.2522\n",
      "Seen so far: 1 samples\n",
      "\n",
      "Start of epoch 79\n",
      "Training loss (for one batch) at step 0: 0.2377\n",
      "Seen so far: 1 samples\n",
      "\n",
      "Start of epoch 80\n",
      "Training loss (for one batch) at step 0: 0.2097\n",
      "Seen so far: 1 samples\n",
      "\n",
      "Start of epoch 81\n",
      "Training loss (for one batch) at step 0: 0.1928\n",
      "Seen so far: 1 samples\n",
      "\n",
      "Start of epoch 82\n",
      "Training loss (for one batch) at step 0: 0.1778\n",
      "Seen so far: 1 samples\n",
      "\n",
      "Start of epoch 83\n",
      "Training loss (for one batch) at step 0: 0.1599\n",
      "Seen so far: 1 samples\n",
      "\n",
      "Start of epoch 84\n",
      "Training loss (for one batch) at step 0: 0.1492\n",
      "Seen so far: 1 samples\n",
      "\n",
      "Start of epoch 85\n",
      "Training loss (for one batch) at step 0: 0.1532\n",
      "Seen so far: 1 samples\n",
      "\n",
      "Start of epoch 86\n",
      "Training loss (for one batch) at step 0: 0.2606\n",
      "Seen so far: 1 samples\n",
      "\n",
      "Start of epoch 87\n",
      "Training loss (for one batch) at step 0: 0.4500\n",
      "Seen so far: 1 samples\n",
      "\n",
      "Start of epoch 88\n",
      "Training loss (for one batch) at step 0: 0.2749\n",
      "Seen so far: 1 samples\n",
      "\n",
      "Start of epoch 89\n",
      "Training loss (for one batch) at step 0: 0.2793\n",
      "Seen so far: 1 samples\n",
      "\n",
      "Start of epoch 90\n",
      "Training loss (for one batch) at step 0: 0.2739\n",
      "Seen so far: 1 samples\n",
      "\n",
      "Start of epoch 91\n",
      "Training loss (for one batch) at step 0: 0.2103\n",
      "Seen so far: 1 samples\n",
      "\n",
      "Start of epoch 92\n",
      "Training loss (for one batch) at step 0: 0.1991\n",
      "Seen so far: 1 samples\n",
      "\n",
      "Start of epoch 93\n",
      "Training loss (for one batch) at step 0: 0.2212\n",
      "Seen so far: 1 samples\n",
      "\n",
      "Start of epoch 94\n",
      "Training loss (for one batch) at step 0: 0.1929\n",
      "Seen so far: 1 samples\n",
      "\n",
      "Start of epoch 95\n",
      "Training loss (for one batch) at step 0: 0.1991\n",
      "Seen so far: 1 samples\n",
      "\n",
      "Start of epoch 96\n",
      "Training loss (for one batch) at step 0: 0.1328\n",
      "Seen so far: 1 samples\n",
      "\n",
      "Start of epoch 97\n",
      "Training loss (for one batch) at step 0: 0.1270\n",
      "Seen so far: 1 samples\n",
      "\n",
      "Start of epoch 98\n",
      "Training loss (for one batch) at step 0: 0.1468\n",
      "Seen so far: 1 samples\n",
      "\n",
      "Start of epoch 99\n",
      "Training loss (for one batch) at step 0: 0.0997\n",
      "Seen so far: 1 samples\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    print(\"\\nStart of epoch %d\" % (epoch,))\n",
    "\n",
    "    # Iterate over the batches of the dataset.\n",
    "    for step in range(1):\n",
    "\n",
    "        # Open a GradientTape to record the operations run\n",
    "        # during the forward pass, which enables auto-differentiation.\n",
    "        with tf.GradientTape() as tape:\n",
    "\n",
    "            # Run the forward pass of the layer.\n",
    "            # The operations that the layer applies\n",
    "            # to its inputs are going to be recorded\n",
    "            # on the GradientTape.\n",
    "            logits = model1(x_train, training=True)  # Logits for this minibatch\n",
    "\n",
    "            # Compute the loss value for this minibatch.\n",
    "            loss_value = loss_fn(y_train, logits)\n",
    "            #label_encoder\n",
    "        # Use the gradient tape to automatically retrieve\n",
    "        # the gradients of the trainable variables with respect to the loss.\n",
    "        grads = tape.gradient(loss_value, model1.trainable_weights)\n",
    "\n",
    "        # Run one step of gradient descent by updating\n",
    "        # the value of the variables to minimize the loss.\n",
    "        optimizer.apply_gradients(zip(grads, model1.trainable_weights))\n",
    "\n",
    "        # Log every 200 batches.\n",
    "        if step % 200 == 0:\n",
    "            print(\n",
    "                \"Training loss (for one batch) at step %d: %.4f\"\n",
    "                % (step, float(loss_value))\n",
    "            )\n",
    "            print(\"Seen so far: %s samples\" % ((step + 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델성능:  tf.Tensor(0.90654695, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "y_pred = model1(x_test)\n",
    "score = loss_fn(y_test,y_pred)\n",
    "print(\"모델성능: \",score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>preds</th>\n",
       "      <th>시설포도_정상</th>\n",
       "      <th>시설포도노균병_중기</th>\n",
       "      <th>시설포도탄저병_초기</th>\n",
       "      <th>일소피해_말기</th>\n",
       "      <th>파프리카_정상</th>\n",
       "      <th>파프리카흰가루병_중기</th>\n",
       "      <th>파프리카흰가루병_초기</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>answer</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>시설포도_정상</th>\n",
       "      <td>43</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>시설포도노균병_중기</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>시설포도탄저병_초기</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>일소피해_말기</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>일소피해_초기</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>파프리카_정상</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>파프리카흰가루병_말기</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>파프리카흰가루병_중기</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>파프리카흰가루병_초기</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "preds        시설포도_정상  시설포도노균병_중기  시설포도탄저병_초기  일소피해_말기  파프리카_정상  파프리카흰가루병_중기  \\\n",
       "answer                                                                        \n",
       "시설포도_정상           43           0           0        0        0            1   \n",
       "시설포도노균병_중기         1           1           0        0        0            0   \n",
       "시설포도탄저병_초기         0           0           3        0        0            0   \n",
       "일소피해_말기            0           0           0        1        0            0   \n",
       "일소피해_초기            1           0           0        0        0            0   \n",
       "파프리카_정상            0           0           0        0       29            1   \n",
       "파프리카흰가루병_말기        0           0           0        0        0            2   \n",
       "파프리카흰가루병_중기        0           0           0        0        0            4   \n",
       "파프리카흰가루병_초기        0           0           0        0        3            2   \n",
       "\n",
       "preds        파프리카흰가루병_초기  \n",
       "answer                    \n",
       "시설포도_정상                0  \n",
       "시설포도노균병_중기             0  \n",
       "시설포도탄저병_초기             0  \n",
       "일소피해_말기                0  \n",
       "일소피해_초기                1  \n",
       "파프리카_정상                1  \n",
       "파프리카흰가루병_말기            0  \n",
       "파프리카흰가루병_중기            1  \n",
       "파프리카흰가루병_초기            5  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# y_pred\n",
    "pred = np.array([])\n",
    "for i in y_pred:\n",
    "  maxi = max(i)\n",
    "  for ind,j in enumerate(i):\n",
    "    if j ==maxi:\n",
    "      pred = np.append(pred,ind)\n",
    "\n",
    "answer = np.array([label_description[label_decoder[int(val)]] for val in y_test])\n",
    "predss = np.array([label_description[label_decoder[int(val)]] for val in pred])\n",
    "\n",
    "new_crosstab = pd.crosstab(answer, predss, rownames=['answer'], colnames=['preds'])\n",
    "new_crosstab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 17325318423252082272\n",
      "xla_global_id: -1\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8865460eab3b8858828539624ef2f45d875655b94242e338bcb660acf08eeb38"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
